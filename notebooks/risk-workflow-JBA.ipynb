{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2674e289-cc8d-4452-8581-dc1ae337b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np \n",
    "import shapely \n",
    "import csv\n",
    "import ast\n",
    "\n",
    "import osm_flex.download as dl\n",
    "import osm_flex.extract as ex\n",
    "from osm_flex.simplify import remove_contained_points,remove_exact_duplicates,remove_contained_polys\n",
    "from osm_flex.config import OSM_DATA_DIR,DICT_GEOFABRIK\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lonboard import viz\n",
    "from lonboard.colormap import apply_continuous_cmap\n",
    "from palettable.colorbrewer.sequential import Blues_9\n",
    "\n",
    "from pathlib import Path\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4793cc25-f186-4a54-acfb-43b675fc17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define paths\n",
    "p = Path('..')\n",
    "data_path = Path(pathlib.Path.home().parts[0]) / 'Projects' / 'gmhcira' / 'data' #should contain folder 'Vulnerability' with vulnerability data\n",
    "#flood_data_path = Path(pathlib.Path('Z:') / 'eks510' / 'fathom-global') # Flood data\n",
    "flood_data_path = Path(pathlib.Path('C:/') / 'Users' / 'snn490' / 'OneDrive - Vrije Universiteit Amsterdam' / 'ADB' / 'Data') # Flood data\n",
    "eq_data_path = Path(pathlib.Path('Z:') / 'data_catalogue' / 'open_street_map' / 'global_hazards' / 'earthquakes') # Earthquake data\n",
    "landslide_data_path = Path(pathlib.Path('Z:') / 'data_catalogue' / 'open_street_map' / 'global_hazards' / 'landslides') # Landslide data\n",
    "cyclone_data_path = Path(pathlib.Path('Z:') / 'data_catalogue' / 'open_street_map' / 'global_hazards' / 'tropical_cyclones') # Cyclone data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92eb7504-af16-4a75-8e4a-563d67992199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import geopandas as gpd\n",
    "from osgeo import ogr, gdal\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import shapely\n",
    "from tqdm import tqdm\n",
    "\n",
    "from osm_flex.config import DICT_CIS_OSM, OSM_CONFIG_FILE\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "DATA_DIR = '' #TODO: dito, where & how to define\n",
    "gdal.SetConfigOption(\"OSM_CONFIG_FILE\", str(OSM_CONFIG_FILE))\n",
    "\n",
    "\n",
    "def _query_builder(geo_type, constraint_dict):\n",
    "    \"\"\"\n",
    "    This function builds an SQL query from the values passed to the extract()\n",
    "    function.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    geo_type : str\n",
    "        Type of geometry to extract. One of [points, lines, multipolygons]\n",
    "    constraint_dict :  dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    query : str\n",
    "        an SQL query string.\n",
    "    \"\"\"\n",
    "    # columns which to report in output\n",
    "    query =  \"SELECT osm_id\"\n",
    "    for key in constraint_dict['osm_keys']:\n",
    "        query+= \",\"+ key\n",
    "    # filter condition(s)\n",
    "    if constraint_dict['osm_query'] is not None:\n",
    "        query+= \" FROM \" + geo_type + \" WHERE \" + constraint_dict['osm_query']\n",
    "    else:\n",
    "        query += \" FROM \" + geo_type + f\" WHERE {constraint_dict['osm_keys'][0]} IS NOT NULL\"\n",
    "    return query\n",
    "\n",
    "def extract(osm_path, geo_type, osm_keys, osm_query=None):\n",
    "    \"\"\"\n",
    "    Function to extract geometries and tag info for entires in the OSM file\n",
    "    matching certain OSM keys, or key-value constraints.\n",
    "    from an OpenStreetMap osm.pbf file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    osm_path : str or Path\n",
    "        location of osm.pbf file from which to parse\n",
    "    geo_type : str\n",
    "        Type of geometry to extract. One of [points, lines, multipolygons]\n",
    "    osm_keys : list\n",
    "        a list with all the osm keys that should be reported as columns in\n",
    "        the output gdf.\n",
    "    osm_query : str\n",
    "        optional. query string of the syntax\n",
    "        \"key='value' (and/or further queries)\". If left empty, all objects\n",
    "        for which the first entry of osm_keys is not Null will be parsed.\n",
    "        See examples in DICT_CIS_OSM in case of doubt.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gpd.GeoDataFrame\n",
    "        A gdf with all results from the osm.pbf file matching the\n",
    "        specified constraints.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    1) The keys that are searchable are specified in the osmconf.ini file.\n",
    "    Make sure that they exist in the attributes=... paragraph under the\n",
    "    respective geometry section.\n",
    "    For example, to extract multipolygons with building='yes',\n",
    "    building must be in the attributes under\n",
    "    the [multipolygons] section of the file. You can find it in the same\n",
    "    folder as the osm_dataloader.py module is located.\n",
    "    2) OSM keys that have : in their name must be changed to _ in the\n",
    "    search dict, but not in the osmconf.ini\n",
    "    E.g. tower:type is called tower_type, since it would interfere with the\n",
    "    SQL syntax otherwise, but still tower:type in the osmconf.ini\n",
    "    3) If the osm_query is left empty (None), then all objects will be parsed\n",
    "    for which the first entry of osm_keys is not Null. E.g. if osm_keys =\n",
    "    ['building', 'name'] and osm_query = None, then all items matching\n",
    "    building=* will be parsed.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    https://taginfo.openstreetmap.org/ to check what keys and key/value\n",
    "    pairs are valid.\n",
    "    https://overpass-turbo.eu/ for a direct visual output of the query,\n",
    "    and to quickly check the validity. The wizard can help you find the\n",
    "    correct keys / values you are looking for.\n",
    "    \"\"\"\n",
    "    if not Path(osm_path).is_file():\n",
    "        raise ValueError(f\"the given path is not a file: {osm_path}\")\n",
    "\n",
    "    osm_path = str(osm_path)\n",
    "    constraint_dict = {\n",
    "        'osm_keys' : osm_keys,\n",
    "        'osm_query' : osm_query}\n",
    "\n",
    "    driver = ogr.GetDriverByName('OSM')\n",
    "    data = driver.Open(osm_path)\n",
    "    query = _query_builder(geo_type, constraint_dict)\n",
    "    LOGGER.debug(\"query: %s\", query)\n",
    "    sql_lyr = data.ExecuteSQL(query)\n",
    "    features = []\n",
    "    geometry = []\n",
    "    if data is not None:\n",
    "        LOGGER.info('query is finished, lets start the loop')\n",
    "        for feature in tqdm(sql_lyr, desc=f'extract {geo_type}'):\n",
    "            try:\n",
    "                wkb = feature.geometry().ExportToWkb()\n",
    "                geom = shapely.wkb.loads(bytes(wkb))\n",
    "                if geom is None:\n",
    "                    continue\n",
    "                geometry.append(geom)\n",
    "                fields = [\n",
    "                    feature.GetField(key)\n",
    "                    for key in [\"osm_id\", *constraint_dict[\"osm_keys\"]]\n",
    "                ]\n",
    "                features.append(fields)\n",
    "            except Exception as exc:\n",
    "                LOGGER.info('%s - %s', exc.__class__, exc)\n",
    "                LOGGER.warning(\"skipped OSM feature\")\n",
    "    else:\n",
    "        LOGGER.error(\"\"\"Nonetype error when requesting SQL. Check the\n",
    "                     query and the OSM config file under the respective\n",
    "                     geometry - perhaps key is unknown.\"\"\")\n",
    "\n",
    "    return gpd.GeoDataFrame(\n",
    "        features,\n",
    "        columns=[\"osm_id\", *constraint_dict['osm_keys']],\n",
    "        geometry=geometry,\n",
    "        crs=\"epsg:4326\"\n",
    "    )\n",
    "\n",
    "# TODO: decide on name of wrapper, which categories included & what components fall under it.\n",
    "def extract_cis(osm_path, ci_type):\n",
    "    \"\"\"\n",
    "    A wrapper around extract() to conveniently extract map info for a\n",
    "    selection of  critical infrastructure types from the given osm.pbf file.\n",
    "    No need to search for osm key/value tags and relevant geometry types.\n",
    "    Parameters\n",
    "    ----------\n",
    "    osm_path : str or Path\n",
    "        location of osm.pbf file from which to parse\n",
    "    ci_type : str\n",
    "        one of DICT_CIS_OSM.keys(), i.e. 'education', 'healthcare',\n",
    "        'water', 'telecom', 'road', 'rail', 'air', 'gas', 'oil', 'power',\n",
    "        'wastewater', 'food'\n",
    "    See also\n",
    "    -------\n",
    "    DICT_CIS_OSM for the keys and key/value tags queried for the respective\n",
    "    CIs. Modify if desired.\n",
    "    \"\"\"\n",
    "    # features consisting in points and multipolygon results:\n",
    "    if ci_type in ['healthcare','education','food','buildings']:\n",
    "        gdf = pd.concat([\n",
    "            extract(osm_path, 'points', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                    DICT_CIS_OSM[ci_type]['osm_query']),\n",
    "            extract(osm_path, 'multipolygons', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                    DICT_CIS_OSM[ci_type]['osm_query'])\n",
    "            ])\n",
    "\n",
    "    # features consisting in points, multipolygons and lines:\n",
    "    elif ci_type in ['gas','oil', 'water','power']:\n",
    "        gdf =  pd.concat([\n",
    "            extract(osm_path, 'points', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                    DICT_CIS_OSM[ci_type]['osm_query']),\n",
    "            extract(osm_path, 'multipolygons', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                             DICT_CIS_OSM[ci_type]['osm_query']),\n",
    "            extract(osm_path, 'lines', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                             DICT_CIS_OSM[ci_type]['osm_query'])\n",
    "            ])\n",
    "\n",
    "    # features consisting in multipolygons and lines:\n",
    "    elif ci_type in ['air']:\n",
    "        gdf =  pd.concat([\n",
    "            extract(osm_path, 'multipolygons', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                             DICT_CIS_OSM[ci_type]['osm_query']),\n",
    "            extract(osm_path, 'lines', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                             DICT_CIS_OSM[ci_type]['osm_query'])\n",
    "            ])\n",
    "    \n",
    "    # features consisting in multiple datattypes, but only lines needed:\n",
    "    elif ci_type in ['rail','road', 'main_road']:\n",
    "        gdf =  pd.concat([\n",
    "            extract(osm_path, 'lines', \n",
    "                    DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                    DICT_CIS_OSM[ci_type]['osm_query'])\n",
    "            ])\n",
    "\n",
    "\n",
    "    # features consisting in all data types, but only points and multipolygon needed:\n",
    "    elif ci_type in ['telecom','wastewater','waste_solid','waste_water','water_supply']:\n",
    "        gdf = pd.concat([\n",
    "            extract(osm_path, 'points', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                    DICT_CIS_OSM[ci_type]['osm_query']),\n",
    "            extract(osm_path, 'multipolygons', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                    DICT_CIS_OSM[ci_type]['osm_query'])\n",
    "            ])\n",
    "        \n",
    "    else:\n",
    "        LOGGER.warning('feature not in DICT_CIS_OSM. Returning empty gdf')\n",
    "        gdf = gpd.GeoDataFrame()\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2402921-488f-4a41-97bd-0eeb6597d93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_CIS_OSM =  {\n",
    "        'power' : {\n",
    "              'osm_keys' : ['power','voltage','name'],\n",
    "              'osm_query' : \"\"\"power='line' or power='cable' or\n",
    "                               power='minor_line' or power='minor_cable' or\n",
    "                               power='plant' or power='generator' or\n",
    "                               power='substation' or power='tower' or\n",
    "                               power='pole' or power='portal'\"\"\"},\n",
    "        'road' :  {\n",
    "            'osm_keys' : ['highway','name','maxspeed','lanes','surface'],\n",
    "            'osm_query' : \"\"\"highway in ('motorway', 'motorway_link', 'motorway_junction', 'trunk', 'trunk_link',\n",
    "                            'primary', 'primary_link', 'secondary', 'secondary_link', 'tertiary', 'tertiary_link', \n",
    "                            'residential', 'road', 'unclassified', 'living_street', 'pedestrian', 'bus_guideway', 'escape', 'raceway', \n",
    "                            'cycleway', 'construction', 'bus_stop', 'crossing', 'mini_roundabout', 'passing_place', 'rest_area', \n",
    "                            'turning_circle', 'traffic_island', 'yes', 'emergency_bay', 'service')\"\"\"},\n",
    "        'rail' : {\n",
    "            'osm_keys' : ['railway','name','gauge','electrified','voltage'],\n",
    "            'osm_query' : \"\"\"railway='rail' or railway='narrow_gauge'\"\"\"},\n",
    "         'air' : {\n",
    "             'osm_keys' : ['aeroway','name'],\n",
    "             'osm_query' : \"\"\"aeroway='aerodrome' or aeroway='terminal' or aeroway='runway'\"\"\"}, \n",
    "        'telecom' : {\n",
    "            'osm_keys' : ['man_made','tower_type','name'],\n",
    "            'osm_query' : \"\"\"tower_type='communication' or man_made='mast' or man_made='communications_tower'\"\"\"},\n",
    "        'water_supply' : {\n",
    "            'osm_keys' : ['man_made','name'],\n",
    "            'osm_query' : \"\"\"man_made='water_well' or man_made='water_works' or\n",
    "                             man_made='water_tower' or\n",
    "                             man_made='reservoir_covered' or\n",
    "                             (man_made='storage_tank' and content='water')\"\"\"},\n",
    "        'waste_solid' : {\n",
    "              'osm_keys' : ['amenity','name'],\n",
    "              'osm_query' : \"\"\"amenity='waste_transfer_station'\"\"\"},\n",
    "        'waste_water' : {\n",
    "              'osm_keys' : ['man_made','name'],\n",
    "              'osm_query' : \"\"\"man_made='wastewater_plant'\"\"\"},\n",
    "        'education' : {\n",
    "            'osm_keys' : ['amenity','building','name'],\n",
    "            'osm_query' : \"\"\"building='school' or amenity='school' or\n",
    "                             building='kindergarten' or \n",
    "                             amenity='kindergarten' or\n",
    "                             building='college' or amenity='college' or\n",
    "                             building='university' or amenity='university' or\n",
    "                             building='library' or amenity='library'\"\"\"},\n",
    "        'healthcare' : {\n",
    "            'osm_keys' : ['amenity','building','healthcare','name'],\n",
    "            'osm_query' : \"\"\"amenity='hospital' or healthcare='hospital' or\n",
    "                             building='hospital' or building='clinic' or\n",
    "                             amenity='clinic' or healthcare='clinic' or \n",
    "                             amenity='doctors' or healthcare='doctors' or\n",
    "                             amenity='dentist' or amenity='pharmacy' or \n",
    "                             healthcare='pharmacy' or healthcare='dentist' or\n",
    "                             healthcare='physiotherapist' or healthcare='alternative' or \n",
    "                             healthcare='laboratory' or healthcare='optometrist' or \n",
    "                             healthcare='rehabilitation' or healthcare='blood_donation' or\n",
    "                             healthcare='birthing_center'\n",
    "                             \"\"\"},\n",
    "        'power_original' : {\n",
    "              'osm_keys' : ['power','voltage','utility','name'],\n",
    "              'osm_query' : \"\"\"power='line' or power='cable' or\n",
    "                               power='minor_line' or power='plant' or\n",
    "                               power='generator' or power='substation' or\n",
    "                               power='transformer' or\n",
    "                               power='pole' or power='portal' or \n",
    "                               power='tower' or power='terminal' or \n",
    "                               power='switch' or power='catenary_mast' or\n",
    "                               utility='power'\"\"\"},\n",
    "         'gas' : {\n",
    "             'osm_keys' : ['man_made','pipeline', 'utility','name'],\n",
    "             'osm_query' : \"\"\"(man_made='pipeline' and substance='gas') or\n",
    "                              (pipeline='substation' and substance='gas') or\n",
    "                              (man_made='storage_tank' and content='gas') or\n",
    "                              utility='gas'\"\"\"},\n",
    "        'oil' : {\n",
    "             'osm_keys' : ['pipeline','man_made','amenity','name'],\n",
    "             'osm_query' : \"\"\"(pipeline='substation' and substance='oil') or\n",
    "                              (man_made='pipeline' and substance='oil') or\n",
    "                              man_made='petroleum_well' or \n",
    "                              man_made='oil_refinery' or\n",
    "                              amenity='fuel'\"\"\"},\n",
    "        'main_road' :  {\n",
    "            'osm_keys' : ['highway','name','maxspeed','lanes','surface'],\n",
    "            'osm_query' : \"\"\"highway in ('primary', 'primary_link', 'secondary',\n",
    "                             'secondary_link', 'tertiary', 'tertiary_link', 'trunk', 'trunk_link', \n",
    "                             'motorway', 'motorway_link')\n",
    "                            \"\"\"},\n",
    "        'wastewater' : {\n",
    "              'osm_keys' : ['man_made','amenity',\n",
    "                            'name'],\n",
    "              'osm_query' : \"\"\"amenity='waste_transfer_station' or man_made='wastewater_plant'\"\"\"},\n",
    "         'food' : {\n",
    "             'osm_keys' : ['shop','name'],\n",
    "             'osm_query' : \"\"\"shop='supermarket' or shop='greengrocer' or\n",
    "                              shop='grocery' or shop='general' or \n",
    "                              shop='bakery'\"\"\"},             \n",
    "        'buildings' : {\n",
    "            'osm_keys' : ['building','amenity','name'],\n",
    "            'osm_query' : \"\"\"building='yes' or building='house' or \n",
    "                            building='residential' or building='detached' or \n",
    "                            building='hut' or building='industrial' or \n",
    "                            building='shed' or building='apartments'\"\"\"}\n",
    "                              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "794dd7c4-3c68-44eb-b6ab-db5627bab45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_download(iso3):\n",
    "    \"\"\"\n",
    "    Download OpenStreetMap data for a specific country.\n",
    "    Arguments:\n",
    "        *iso3* (str): ISO 3166-1 alpha-3 country code.\n",
    "    Returns:\n",
    "        *Path*: The file path of the downloaded OpenStreetMap data file.\n",
    "    \"\"\"\n",
    "    \n",
    "    dl.get_country_geofabrik(iso3) # Use the download library to get the geofabrik data for the specified country\n",
    "    data_loc = OSM_DATA_DIR.joinpath(f'{DICT_GEOFABRIK[iso3][1]}-latest.osm.pbf') # Specify the location of the OpenStreetMap (OSM) data file\n",
    "    return data_loc\n",
    "\n",
    "def overlay_hazard_assets(df_ds,assets):\n",
    "    \"\"\"\n",
    "    Overlay hazard assets on a dataframe of spatial geometries.\n",
    "    Arguments:\n",
    "        *df_ds*: GeoDataFrame containing the spatial geometries of the hazard data. \n",
    "        *assets*: GeoDataFrame containing the infrastructure assets.\n",
    "    Returns:\n",
    "        *geopandas.GeoSeries*: A GeoSeries containing the spatial geometries of df_ds that intersect with the infrastructure assets.\n",
    "    \"\"\"\n",
    "    \n",
    "    #overlay \n",
    "    hazard_tree = shapely.STRtree(df_ds.geometry.values)\n",
    "    if (shapely.get_type_id(assets.iloc[0].geometry) == 3) | (shapely.get_type_id(assets.iloc[0].geometry) == 6): # id types 3 and 6 stand for polygon and multipolygon\n",
    "        return  hazard_tree.query(assets.geometry,predicate='intersects')    \n",
    "    else:\n",
    "        return  hazard_tree.query(assets.buffered,predicate='intersects')\n",
    "\n",
    "def buffer_assets(assets,buffer_size=0.00083):\n",
    "    \"\"\"\n",
    "    Buffer spatial assets in a GeoDataFrame.\n",
    "    Arguments:\n",
    "        *assets*: GeoDataFrame containing spatial geometries to be buffered.\n",
    "        *buffer_size* (float, optional): The distance by which to buffer the geometries. Default is 0.00083.\n",
    "    Returns:\n",
    "        *GeoDataFrame*: A new GeoDataFrame with an additional 'buffered' column containing the buffered geometries.\n",
    "    \"\"\"\n",
    "    assets['buffered'] = shapely.buffer(assets.geometry.values,distance=buffer_size)\n",
    "    return assets\n",
    "\n",
    "def get_damage_per_asset(asset,hazard_numpified,asset_geom,hazard_intensity,fragility_values,maxdam_asset,unit_maxdam):\n",
    "    \"\"\"\n",
    "    Calculate damage for a given asset based on hazard information.\n",
    "    Arguments:\n",
    "        *asset*: Tuple containing information about the asset. It includes:\n",
    "            - Index or identifier of the asset (asset[0]).\n",
    "            - The specific hazard points in which asset is exposed (asset[1]['hazard_point']).\n",
    "        *hazard_numpified*: NumPy array representing hazard information.\n",
    "        *asset_geom*: Shapely geometry representing the spatial coordinates of the asset.\n",
    "        *hazard_intensity*: NumPy array representing the hazard intensities of the curve for the asset type.\n",
    "        *fragility_values*: NumPy array representing the damage factors of the curve for the asset type.\n",
    "        *maxdam_asset*: Maximum damage value for asset.\n",
    "        *unit_maxdam*: The unit of maximum damage value for asset.\n",
    "    Returns:\n",
    "        *float*: The calculated damage for the specific asset.\n",
    "    \"\"\"\n",
    "     \n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = hazard_numpified[asset[1]['hazard_point'].values] \n",
    "    get_hazard_points[shapely.intersects(get_hazard_points[:,1],asset_geom)]\n",
    "\n",
    "    # estimate damage\n",
    "    if len(get_hazard_points) == 0: # no overlay of asset with hazard\n",
    "        return 0\n",
    "    \n",
    "    else:\n",
    "        if asset_geom.geom_type == 'LineString':\n",
    "            overlay_meters = shapely.length(shapely.intersection(get_hazard_points[:,1],asset_geom)) # get the length of exposed meters per hazard cell\n",
    "            return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_meters*maxdam_asset) #return asset number, total damage for asset number (damage factor * meters * max. damage)\n",
    "        elif asset_geom.geom_type in ['MultiPolygon','Polygon']:\n",
    "            overlay_m2 = shapely.area(shapely.intersection(get_hazard_points[:,1],asset_geom))\n",
    "            if '/unit' in unit_maxdam:\n",
    "                converted_maxdam = maxdam_asset / shapely.area(asset_geom) #convert to maxdam/m2\n",
    "                return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_m2*converted_maxdam)\n",
    "            else:\n",
    "                return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_m2*maxdam_asset) #return asset number, total damage for asset number (damage factor * meters * max. damage)\n",
    "        elif asset_geom.geom_type == 'Point':\n",
    "            return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*maxdam_asset)\n",
    "\n",
    "def get_damage_per_asset_og(asset,hazard_numpified,asset_geom,hazard_intensity,fragility_values,maxdam_asset):\n",
    "    \"\"\"\n",
    "    Calculate damage for a given asset based on hazard information.\n",
    "    Arguments:\n",
    "        *asset*: Tuple containing information about the asset. It includes:\n",
    "            - Index or identifier of the asset (asset[0]).\n",
    "            - The specific hazard points in which asset is exposed (asset[1]['hazard_point']).\n",
    "        *hazard_numpified*: NumPy array representing hazard information.\n",
    "        *asset_geom*: Shapely geometry representing the spatial coordinates of the asset.\n",
    "        *hazard_intensity*: NumPy array representing the hazard intensities of the curve for the asset type.\n",
    "        *fragility_values*: NumPy array representing the damage factors of the curve for the asset type.\n",
    "        *maxdam_asset*: Maximum damage value for asset.\n",
    "    Returns:\n",
    "        *float*: The calculated damage for the specific asset.\n",
    "    \"\"\"\n",
    "     \n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = hazard_numpified[asset[1]['hazard_point'].values] \n",
    "    get_hazard_points[shapely.intersects(get_hazard_points[:,1],asset_geom)]\n",
    "\n",
    "    # estimate damage\n",
    "    if len(get_hazard_points) == 0: # no overlay of asset with hazard\n",
    "        return 0\n",
    "    \n",
    "    else:\n",
    "        if asset_geom.geom_type == 'LineString':\n",
    "            overlay_meters = shapely.length(shapely.intersection(get_hazard_points[:,1],asset_geom)) # get the length of exposed meters per hazard cell\n",
    "            return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_meters*maxdam_asset) #return asset number, total damage for asset number (damage factor * meters * max. damage)\n",
    "        elif asset_geom.geom_type in ['MultiPolygon','Polygon']:\n",
    "            overlay_m2 = shapely.area(shapely.intersection(get_hazard_points[:,1],asset_geom))\n",
    "            return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_m2*maxdam_asset) #return asset number, total damage for asset number (damage factor * meters * max. damage)\n",
    "        elif asset_geom.geom_type == 'Point':\n",
    "            return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*maxdam_asset)\n",
    "\n",
    "def create_pathway_dict(data_path, flood_data_path, eq_data_path, landslide_data_path, cyclone_data_path): \n",
    "\n",
    "    \"\"\"\n",
    "    Create a dictionary containing paths to various hazard datasets.\n",
    "    Arguments:\n",
    "        *data_path* (Path): Base directory path for general data.\n",
    "        *flood_data_path* (Path): Path to flood hazard data.\n",
    "        *eq_data_path* (Path): Path to earthquake hazard data.\n",
    "        *landslide_data_path* (Path): Path to landslide hazard data.\n",
    "        *cyclone_data_path* (Path): Path to tropical cyclone hazard data.\n",
    "    Returns:\n",
    "        *dict*: A dictionary where keys represent a general pathway and different hazard types and values are corresponding paths.\n",
    "    \"\"\"\n",
    "\n",
    "    #create a dictionary\n",
    "    pathway_dict = {'data_path': data_path, \n",
    "                    'jba_rb_rf': flood_data_path, \n",
    "                    'jba_rb_sw': flood_data_path, \n",
    "                    'jba_rd_rf': flood_data_path, \n",
    "                    'jba_rd_sw': flood_data_path, \n",
    "                    'jba_re_rf': flood_data_path, \n",
    "                    'jba_re_sw': flood_data_path, \n",
    "                    'windstorm': cyclone_data_path, \n",
    "                    'earthquake': eq_data_path, \n",
    "                    'landslides': landslide_data_path,}\n",
    "\n",
    "    return pathway_dict\n",
    "\n",
    "def read_hazard_data(hazard_data_path,data_path,hazard_type,ISO3):\n",
    "    \"\"\"\n",
    "    Read hazard data files for a specific hazard type.\n",
    "    Arguments:\n",
    "        *hazard_data_path* (Path): Base directory path where hazard data is stored.\n",
    "        *hazard_type* (str): Type of hazard for which data needs to be read ('fluvial', 'pluvial', 'windstorm', 'earthquake', 'landslides').\n",
    "    \n",
    "    Returns:\n",
    "        *list*: A list of Path objects representing individual hazard data files for the specified hazard type.\n",
    "    \"\"\"  \n",
    "\n",
    "    jba_dict = {'PAK' : 'pakistan-30m',\n",
    "                'PNG' : 'papua-new-guinea-30m',\n",
    "                'TJK' : 'tajikistan-30m',\n",
    "               }\n",
    "\n",
    "    if hazard_type == 'jba_rb_rf':\n",
    "        if ISO3 == 'PAK':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '82_PK_30m_4326' / 'PK_202105_RB_30m_4326' / 'FLRF_U'\n",
    "        elif ISO3 == 'PNG':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '102_PG_30m_4326' / 'PG_202105_RB_30m_4326' / 'FLRF_U'\n",
    "        elif ISO3 == 'TJK':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '288_TJ_30m_4326' / 'TJ_202203_RB_30m_4326' / 'FLRF_U'\n",
    "        \n",
    "        return [file for file in hazard_data.iterdir() if file.suffix == '.tif']\n",
    "\n",
    "    elif hazard_type == 'jba_rb_sw':\n",
    "        if ISO3 == 'PAK':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '82_PK_30m_4326' / 'PK_202105_RB_30m_4326' / 'FLSW_U'\n",
    "        elif ISO3 == 'PNG':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '102_PG_30m_4326' / 'PG_202105_RB_30m_4326' / 'FLSW_U'\n",
    "        elif ISO3 == 'TJK':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '288_TJ_30m_4326' / 'TJ_202203_RB_30m_4326' / 'FLSW_U'\n",
    "        \n",
    "        return [file for file in hazard_data.iterdir() if file.suffix == '.tif']\n",
    "\n",
    "    elif hazard_type == 'jba_rd_rf':\n",
    "        if ISO3 == 'PAK':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '82_PK_30m_4326' / 'PK_202105_RD_30m_4326' / 'FLRF_U'\n",
    "        elif ISO3 == 'PNG':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '102_PG_30m_4326' / 'PG_202105_RD_30m_4326' / 'FLRF_U'\n",
    "        elif ISO3 == 'TJK':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '288_TJ_30m_4326' / 'TJ_202203_RD_30m_4326' / 'FLRF_U'\n",
    "        \n",
    "        return [file for file in hazard_data.iterdir() if file.suffix == '.tif']\n",
    "\n",
    "    elif hazard_type == 'jba_rd_sw':\n",
    "        if ISO3 == 'PAK':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '82_PK_30m_4326' / 'PK_202105_RD_30m_4326' / 'FLSW_U'\n",
    "        elif ISO3 == 'PNG':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '102_PG_30m_4326' / 'PG_202105_RD_30m_4326' / 'FLSW_U'\n",
    "        elif ISO3 == 'TJK':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '288_TJ_30m_4326' / 'TJ_202203_RD_30m_4326' / 'FLSW_U'\n",
    "        \n",
    "        return [file for file in hazard_data.iterdir() if file.suffix == '.tif']\n",
    "\n",
    "    elif hazard_type == 'jba_re_rf':\n",
    "        if ISO3 == 'PAK':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '82_PK_30m_4326' / 'PK_202105_RE_30m_4326' / 'FLRF_U'\n",
    "        elif ISO3 == 'PNG':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '102_PG_30m_4326' / 'PG_202105_RE_30m_4326' / 'FLRF_U'\n",
    "        elif ISO3 == 'TJK':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '288_TJ_30m_4326' / 'TJ_202203_RE_30m_4326' / 'FLRF_U'\n",
    "        \n",
    "        return [file for file in hazard_data.iterdir() if file.suffix == '.tif']\n",
    "\n",
    "    elif hazard_type == 'jba_re_sw':\n",
    "        if ISO3 == 'PAK':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '82_PK_30m_4326' / 'PK_202105_RE_30m_4326' / 'FLSW_U'\n",
    "        elif ISO3 == 'PNG':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '102_PG_30m_4326' / 'PG_202105_RE_30m_4326' / 'FLSW_U'\n",
    "        elif ISO3 == 'TJK':\n",
    "            hazard_data = hazard_data_path / jba_dict[ISO3] / '288_TJ_30m_4326' / 'TJ_202203_RE_30m_4326' / 'FLSW_U'\n",
    "\n",
    "        return [file for file in hazard_data.iterdir() if file.suffix == '.tif']\n",
    "    \n",
    "    elif hazard_type == 'windstorm':\n",
    "        hazard_data = hazard_data_path \n",
    "        return list(hazard_data.iterdir())\n",
    "\n",
    "    elif hazard_type == 'earthquake':\n",
    "        hazard_data = hazard_data_path\n",
    "        return list(hazard_data.iterdir())\n",
    "\n",
    "    elif hazard_type == 'landslides':\n",
    "        hazard_data = hazard_data_path \n",
    "        return list(hazard_data.iterdir())\n",
    "\n",
    "def read_vul_maxdam_orginal(data_path,hazard_type,infra_type):\n",
    "    \"\"\"\n",
    "    Read vulnerability curves and maximum damage data for a specific hazard and infrastructure type.\n",
    "    Arguments:\n",
    "        *data_path*: The base directory path where vulnerability and maximum damage data files are stored.\n",
    "        *hazard_type*: The type of hazard in string format, such as 'pluvial', 'fluvial', or 'windstorm'.\n",
    "        *infra_type*: The type of infrastructure in string format for which vulnerability curves and maximum damage data are needed.\n",
    "    \n",
    "    Returns:\n",
    "        *tuple*: A tuple containing two DataFrames:\n",
    "            - The first DataFrame contains vulnerability curves specific to the given hazard and infrastructure type.\n",
    "            - The second DataFrame contains maximum damage data for the specified infrastructure type.\n",
    "    \"\"\"\n",
    "\n",
    "    vul_data = data_path / 'Vulnerability'\n",
    "    \n",
    "    # Load assumptions file containing curve - maxdam combinations per infrastructure type\n",
    "    assumptions = pd.read_excel(vul_data / 'S1_Assumptions_Test.xlsx',sheet_name = 'Flooding assumptions',header=[1])\n",
    "    assumptions['Infrastructure type'] = assumptions['Infrastructure type'].str.lower()\n",
    "    if \"_\" in infra_type: infra_type = infra_type.replace('_', ' ')\n",
    "    assump_infra_type = assumptions[assumptions['Infrastructure type'] == infra_type]\n",
    "    assump_curves = ast.literal_eval(assump_infra_type['Vulnerability ID number'].item())\n",
    "    assump_maxdams = ast.literal_eval(assump_infra_type['Maximum damage ID number'].item())\n",
    "    \n",
    "    # Get curves\n",
    "    if hazard_type in ['pluvial','fluvial']:  \n",
    "        curves = pd.read_excel(vul_data / 'Table_D2_Hazard_Fragility_and_Vulnerability_Curves_V1.1.0_converted.xlsx',sheet_name = 'F_Vuln_Depth',index_col=[0],header=[0,1,2,3,4])\n",
    "    elif hazard_type == 'windstorm':\n",
    "        curves = pd.read_excel(vul_data / 'Table_D2_Hazard_Fragility_and_Vulnerability_Curves_V1.1.0_converted.xlsx',sheet_name = 'W_Vuln_V10m',index_col=[0],header=[0,1,2,3,4])\n",
    "    \n",
    "    infra_curves =  curves[assump_curves]\n",
    "    \n",
    "    # get maxdam\n",
    "    maxdam = pd.read_excel(vul_data / 'Table_D3_Costs_V1.1.0_converted.xlsx', sheet_name='Cost_Database',index_col=[0])\n",
    "    infra_maxdam = maxdam[maxdam.index.isin(assump_maxdams)]['Amount'].dropna()\n",
    "    infra_maxdam = infra_maxdam[pd.to_numeric(infra_maxdam, errors='coerce').notnull()]\n",
    "\n",
    "    return infra_curves,infra_maxdam\n",
    "\n",
    "def read_vul_maxdam(data_path,hazard_type,infra_type):\n",
    "    \"\"\"\n",
    "    Read vulnerability curves and maximum damage data for a specific hazard and infrastructure type.\n",
    "    Arguments:\n",
    "        *data_path*: The base directory path where vulnerability and maximum damage data files are stored.\n",
    "        *hazard_type*: The type of hazard in string format, such as 'pluvial', 'fluvial', or 'windstorm'.\n",
    "        *infra_type*: The type of infrastructure in string format for which vulnerability curves and maximum damage data are needed.\n",
    "    \n",
    "    Returns:\n",
    "        *tuple*: A tuple containing two DataFrames:\n",
    "            - The first DataFrame contains vulnerability curves specific to the given hazard and infrastructure type.\n",
    "            - The second DataFrame contains maximum damage data for the specified infrastructure type.\n",
    "    \"\"\"\n",
    "\n",
    "    vul_data = data_path / 'Vulnerability'\n",
    "    \n",
    "    # Load assumptions file containing curve - maxdam combinations per infrastructure type\n",
    "    assumptions = pd.read_excel(vul_data / 'S1_Assumptions_Test.xlsx',sheet_name = 'Flooding assumptions',header=[1])\n",
    "    assumptions['Infrastructure type'] = assumptions['Infrastructure type'].str.lower()\n",
    "    if \"_\" in infra_type: infra_type = infra_type.replace('_', ' ')\n",
    "    assump_infra_type = assumptions[assumptions['Infrastructure type'] == infra_type]\n",
    "    assump_curves = ast.literal_eval(assump_infra_type['Vulnerability ID number'].item())\n",
    "    assump_maxdams = ast.literal_eval(assump_infra_type['Maximum damage ID number'].item())\n",
    "    \n",
    "    # Get curves\n",
    "    if hazard_type in ['pluvial','fluvial']:  \n",
    "        curves = pd.read_excel(vul_data / 'Table_D2_Hazard_Fragility_and_Vulnerability_Curves_V1.1.0.xlsx',sheet_name = 'F_Vuln_Depth',index_col=[0],header=[0,1,2,3,4])\n",
    "    elif hazard_type == 'windstorm':\n",
    "        curves = pd.read_excel(vul_data / 'Table_D2_Hazard_Fragility_and_Vulnerability_Curves_V1.1.0.xlsx',sheet_name = 'W_Vuln_V10m',index_col=[0],header=[0,1,2,3,4])\n",
    "    \n",
    "    infra_curves =  curves[assump_curves]\n",
    "    \n",
    "    # get maxdam\n",
    "    maxdam = pd.read_excel(vul_data / 'Table_D3_Costs_V1.1.0_converted.xlsx', sheet_name='Cost_Database',index_col=[0])\n",
    "    infra_costs = maxdam[maxdam.index.isin(assump_maxdams)][['Amount', 'Unit']].dropna(subset=['Amount'])\n",
    "    infra_maxdam = infra_costs['Amount'][pd.to_numeric(infra_costs['Amount'], errors='coerce').notnull()]\n",
    "    infra_units = infra_costs['Unit'].filter(items=list(infra_maxdam.index), axis=0)\n",
    "\n",
    "    return infra_curves,infra_maxdam,infra_units\n",
    "\n",
    "def read_flood_map(flood_map_path,diameter_distance=0.00083/2):\n",
    "    \"\"\"\n",
    "    Read flood map data from a NetCDF file and process it into a GeoDataFrame.\n",
    "    Arguments:\n",
    "        *flood_map_path* (Path): Path to the NetCDF file containing flood map data.\n",
    "        *diameter_distance* (float, optional): The diameter distance used for creating square geometries around data points. Default is 0.00083/2.\n",
    "    \n",
    "    Returns:\n",
    "        *geopandas.GeoDataFrame*: A GeoDataFrame representing the processed flood map data.\n",
    "    \"\"\"\n",
    "    \n",
    "    flood_map = xr.open_dataset(flood_map_path, engine=\"rasterio\")\n",
    "\n",
    "    flood_map_vector = flood_map['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "    \n",
    "    #remove data that will not be used\n",
    "    flood_map_vector = flood_map_vector.loc[(flood_map_vector.band_data > 0) & (flood_map_vector.band_data < 100)]\n",
    "    \n",
    "    # create geometry values and drop lat lon columns\n",
    "    flood_map_vector['geometry'] = [shapely.points(x) for x in list(zip(flood_map_vector['x'],flood_map_vector['y']))]\n",
    "    flood_map_vector = flood_map_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "    \n",
    "    # drop all non values to reduce size\n",
    "    flood_map_vector = flood_map_vector.loc[~flood_map_vector['band_data'].isna()].reset_index(drop=True)\n",
    "    \n",
    "    # and turn them into squares again:\n",
    "    flood_map_vector.geometry= shapely.buffer(flood_map_vector.geometry,distance=diameter_distance,cap_style='square').values \n",
    "\n",
    "    return flood_map_vector\n",
    "\n",
    "def read_flood_jba_map(flood_map_path,diameter_distance=0.0002777777999999998869/2):\n",
    "    \"\"\"\n",
    "    Read flood map data from a NetCDF file and process it into a GeoDataFrame.\n",
    "    Arguments:\n",
    "        *flood_map_path* (Path): Path to the NetCDF file containing flood map data.\n",
    "        *diameter_distance* (float, optional): The diameter distance used for creating square geometries around data points. Default is 0.00083/2.\n",
    "    \n",
    "    Returns:\n",
    "        *geopandas.GeoDataFrame*: A GeoDataFrame representing the processed flood map data.\n",
    "    \"\"\"\n",
    "    \n",
    "    flood_map = xr.open_dataset(flood_map_path, engine=\"rasterio\")\n",
    "\n",
    "    flood_map_vector = flood_map['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "    \n",
    "    #remove data that will not be used\n",
    "    flood_map_vector = flood_map_vector.loc[(flood_map_vector.band_data > 0) & (flood_map_vector.band_data < 100)]\n",
    "    \n",
    "    # create geometry values and drop lat lon columns\n",
    "    flood_map_vector['geometry'] = [shapely.points(x) for x in list(zip(flood_map_vector['x'],flood_map_vector['y']))]\n",
    "    flood_map_vector = flood_map_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "    \n",
    "    # drop all non values to reduce size\n",
    "    flood_map_vector = flood_map_vector.loc[~flood_map_vector['band_data'].isna()].reset_index(drop=True)\n",
    "    \n",
    "    # and turn them into squares again:\n",
    "    flood_map_vector.geometry= shapely.buffer(flood_map_vector.geometry,distance=diameter_distance,cap_style='square').values \n",
    "\n",
    "    return flood_map_vector\n",
    "\n",
    "def read_windstorm_map(windstorm_map_path,bbox):\n",
    "     \n",
    "    # load data from NetCDF file\n",
    "    with xr.open_dataset(flood_map_path) as ds:\n",
    "        \n",
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        ds = ds.rio.clip_box(minx=bbox[0], miny=bbox[1], maxx=bbox[2], maxy=bbox[3])\n",
    "        #ds['band_data'] = ds['band_data']/0.88*1.11 #convert 10-min sustained wind speed to 3-s gust wind speed\n",
    "    \n",
    "        ds_vector = ds['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "        \n",
    "        #remove data that will not be used\n",
    "        ds_vector = ds_vector.loc[(ds_vector.band_data > 0) & (ds_vector.band_data < 100)]\n",
    "        \n",
    "        # create geometry values and drop lat lon columns\n",
    "        ds_vector['geometry'] = [shapely.points(x) for x in list(zip(ds_vector['x'],ds_vector['y']))]\n",
    "        ds_vector = ds_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "        ds_vector['geometry'] = shapely.buffer(ds_vector.geometry, distance=0.1/2, cap_style='square').values\n",
    "    \n",
    "        return ds_vector\n",
    "\n",
    "def combine_columns(a, b):\n",
    "    \"\"\"\n",
    "    Combine values from two input arguments 'a' and 'b' into a single string.\n",
    "    Arguments:\n",
    "    - a (str or None): Value from column 'A'.\n",
    "    - b (str or None): Value from column 'B'.\n",
    "\n",
    "    Returns:\n",
    "    - str or None: A string of 'a', 'b' or combination. If both 'a' and 'b' are None, return None.\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.notna(a) and pd.notna(b) == False: #if only a contains a string\n",
    "        return f\"{a}\" \n",
    "    elif pd.notna(b) and pd.notna(a) == False: #if only b contains a string\n",
    "        return f\"{b}\"\n",
    "    elif pd.notna(a) and pd.notna(b):  #if both values contain a string\n",
    "        if a == b: \n",
    "            return f\"{a}\"\n",
    "        elif a == 'yes' or b == 'yes':\n",
    "            if a == 'yes':\n",
    "                return  f\"{b}\"\n",
    "            elif b == 'yes':\n",
    "                return  f\"{a}\"\n",
    "        else: \n",
    "            return f\"{a}\" #f\"{a}_{b}\" # assuming that value from column A contains the more detailed information\n",
    "    else: \n",
    "        None # Decision point: If nones are existent, decide on what to do with Nones. Are we sure that these are education facilities? Delete them? Provide another tag to them?\n",
    "\n",
    "def filter_dataframe(assets, column_names_lst):\n",
    "    \"\"\"\n",
    "    Filter a GeoDataFrame by combining information from two specified columns and removing selected columns.\n",
    "    Args:\n",
    "        assets (geopandas.GeoDataFrame): The input GeoDataFrame containing spatial geometries and columns to filter.\n",
    "        column_names_lst (list): A list of two column names whose information needs to be combined to create a new 'asset' column.\n",
    "\n",
    "    Returns:\n",
    "        geopandas.GeoDataFrame: A filtered GeoDataFrame with a new 'asset' column and selected columns dropped, and points converted to polygons.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(column_names_lst) == 2:        \n",
    "        assets['asset'] = assets.apply(lambda row: combine_columns(row[column_names_lst[0]], row[column_names_lst[1]]), axis=1) # create new column based on tag information provided in two columns\n",
    "    elif len(column_names_lst) == 3:\n",
    "        assets['asset_temp'] = assets.apply(lambda row: combine_columns(row[column_names_lst[0]], row[column_names_lst[1]]), axis=1) # create temp column based on tag information provided in two columns\n",
    "        assets['asset'] = assets.apply(lambda row: combine_columns(row['asset_temp'], row[column_names_lst[2]]), axis=1) # create new column based on tag information provided in two columns\n",
    "        column_names_lst.append('asset_temp')        \n",
    "    else:\n",
    "        print(\"Warning: column_names_lst should contain 2 or 3 items\")\n",
    "\n",
    "    assets = assets.drop(columns=column_names_lst, axis=1) # drop columns\n",
    "    assets = remove_contained_assets_and_convert(assets)\n",
    "    \n",
    "    return assets\n",
    "\n",
    "def delete_linestring_data(assets, infra_lst):\n",
    "    \"\"\"\n",
    "    Filter and update a GeoDataFrame by excluding rows with LineString geometries.\n",
    "\n",
    "    Parameters:\n",
    "    - assets (geopandas.GeoDataFrame): The original GeoDataFrame.\n",
    "    - infra_lst (lst): A list with the infrastructure typs to filter.\n",
    "\n",
    "    Returns:\n",
    "    - geopandas.GeoDataFrame: The updated GeoDataFrame with excluded LineString rows.\n",
    "    \"\"\"\n",
    "\n",
    "    for infra_type in infra_lst:\n",
    "        #create subset of data\n",
    "        condition = assets['asset'] == infra_type\n",
    "        subset = assets[condition]\n",
    "        \n",
    "        #delete line data if there is line data (assuming that this function is only for point and polygon data)\n",
    "        subset = subset[subset['geometry'].geom_type.isin(['Point', 'MultiPoint', 'Polygon', 'MultiPolygon'])]  # Keep (multi-) points and polygon geometries\n",
    "    \n",
    "        #update the original Dataframe by excluding rows in the subset\n",
    "        assets = assets[~condition | condition & subset['geometry'].notna()]\n",
    "\n",
    "    return assets\n",
    "\n",
    "def delete_point_and_polygons(assets, infra_lst):\n",
    "    \"\"\"\n",
    "    Filter and update a GeoDataFrame by excluding rows with points and (multi-)polygon geometries.\n",
    "\n",
    "    Parameters:\n",
    "    - assets (geopandas.GeoDataFrame): The original GeoDataFrame.\n",
    "    - infra_lst (lst): A list with the infrastructure typs to filter.\n",
    "\n",
    "    Returns:\n",
    "    - geopandas.GeoDataFrame: The updated GeoDataFrame with excluded points and (multi-)polygon rows.\n",
    "    \"\"\"\n",
    "\n",
    "    for infra_type in infra_lst:\n",
    "        #create subset of data\n",
    "        condition = assets['asset'] == infra_type\n",
    "        subset = assets[condition]\n",
    "        \n",
    "        #delete points and (multi-)polygon data if available\n",
    "        subset = subset[subset['geometry'].geom_type.isin(['LineString', 'MultiLineString'])]  # Keep only LineString geometries\n",
    "\n",
    "\n",
    "        #update the original Dataframe by excluding rows in the subset\n",
    "        assets = assets[~condition | condition & subset['geometry'].notna()]\n",
    "\n",
    "    return assets\n",
    "\n",
    "def remove_polygons_with_contained_points(gdf):\n",
    "    \"\"\"\n",
    "    Remove polygons in a GeoDataFrame if there is a point falling within them.\n",
    "    Arguments:\n",
    "        gdf : GeoDataFrame containing entries with point and (multi-)polygon geometry\n",
    "    Returns:\n",
    "    - geopandas.GeoDataFrame: GeoDataFrame containing entries with point and (multi-)polygon geometry, but without duplicates\n",
    "    \"\"\"\n",
    "    gdf = gdf.reset_index(drop=True)\n",
    "    \n",
    "    ind_poly_with_points = np.unique(gpd.sjoin(gdf[gdf.geometry.type == 'Point'],\n",
    "                                              gdf[gdf.geometry.type.isin(['MultiPolygon', 'Polygon'])],\n",
    "                                              predicate='within').index_right)\n",
    "    \n",
    "    return gdf.drop(index=ind_poly_with_points).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def remove_contained_assets_and_convert(assets):\n",
    "    \"\"\"\n",
    "    Process the geometry of assets, removing contained points and polygons, and converting points to polygons.\n",
    "    Args:\n",
    "        assets (geopandas.GeoDataFrame): Input GeoDataFrame containing asset geometries.\n",
    "\n",
    "    Returns:\n",
    "        geopandas.GeoDataFrame: Processed GeoDataFrame with updated asset geometries.\n",
    "    \"\"\"\n",
    "    \n",
    "    assets =  remove_contained_polys(remove_contained_points(assets)) #remove points and polygons within a (larger) polygon\n",
    "    \n",
    "    #convert points to polygons\n",
    "    if (assets.loc[assets.geom_type == 'MultiPolygon']).empty:\n",
    "        default_distance = 58.776\n",
    "        assets.loc[assets.geom_type == 'Point','geometry'] = assets.loc[assets.geom_type == 'Point'].buffer(distance=default_distance, cap_style='square')\n",
    "    else:    \n",
    "        assets.loc[assets.geom_type == 'Point','geometry'] = assets.loc[assets.geom_type == 'Point'].buffer(\n",
    "                                                                        distance=np.sqrt(assets.loc[assets.geom_type == 'MultiPolygon'].area.median())/2, cap_style='square')\n",
    "\n",
    "    return assets\n",
    "\n",
    "def create_point_from_polygon(gdf):\n",
    "    \"\"\"\n",
    "    Transforms polygons into points\n",
    "    Arguments:\n",
    "        gdf: A geodataframe containing a column geometry\n",
    "    Returns:\n",
    "    - geopandas.GeoDataFrame: The updated GeoDataFrame without polygons but with only point geometries\n",
    "    \"\"\"\n",
    "    gdf['geometry'] = gdf['geometry'].apply(lambda geom: MultiPolygon([geom]) if geom.geom_type == 'Polygon' else geom) #convert to multipolygons in case polygons are in the df\n",
    "    #gdf.loc[gdf.geom_type == 'MultiPolygon','geometry'] = gdf.loc[assets.geom_type == 'MultiPolygon'].centroid #convert polygon to point\n",
    "    gdf.loc[gdf.geom_type == 'MultiPolygon','geometry'] = gdf.loc[gdf.geom_type == 'MultiPolygon'].centroid #convert polygon to point\n",
    "    return gdf\n",
    "    \n",
    "def process_selected_assets(gdf, polygon_types, point_types):\n",
    "    \"\"\"\n",
    "    Process the geometry of selected assets, removing contained points and polygons, and converting non-contained points to polygons.\n",
    "    Args:\n",
    "        gdf (geopandas.GeoDataFrame): Input GeoDataFrame containing asset geometries.\n",
    "        selected_types (list): List of asset types to process.\n",
    "\n",
    "    Returns:\n",
    "        geopandas.GeoDataFrame: Processed GeoDataFrame with updated asset geometries.\n",
    "    \"\"\"\n",
    "    asset_temp = gdf['asset'].tolist()\n",
    "    gdf.insert(1, 'asset_temp', asset_temp) \n",
    "    \n",
    "    # For assets that we need as (multi-)polygons: group by asset type and apply the processing function\n",
    "    filtered_assets = gdf[gdf['asset'].isin(polygon_types)] # Filter only selected asset types\n",
    "    polygon_gdf = (filtered_assets.groupby('asset_temp').apply(remove_contained_assets_and_convert, include_groups=False)).reset_index(drop=True)\n",
    "\n",
    "    # For assets that we need as (multi-)points: group by asset type and apply the processing function\n",
    "    filtered_assets = gdf[gdf['asset'].isin(point_types)] # Filter only selected asset types\n",
    "    #point_gdf = (filtered_assets.groupby('asset').apply(create_point_from_polygon)).reset_index(drop=True)\n",
    "    point_gdf = (filtered_assets.groupby('asset_temp').apply(lambda group: create_point_from_polygon(remove_polygons_with_contained_points(group)), include_groups=False)).reset_index(drop=True)\n",
    "    \n",
    "    # Concatenate the two dataframes along rows\n",
    "    merged_gdf = pd.concat([polygon_gdf, point_gdf], ignore_index=True)\n",
    "    \n",
    "    return merged_gdf\n",
    "\n",
    "def create_damage_csv(damage_output, hazard_type, pathway_dict, country_code, sub_system):\n",
    "    \"\"\"\n",
    "    Create a CSV file containing damage information.\n",
    "    Arguments:\n",
    "        damage_output: A dictionary containing damage information.\n",
    "        hazard_type: The type of hazard (e.g., 'earthquake', 'flood').\n",
    "        pathway_dict: A dictionary containing file paths for different data.\n",
    "        country_code: A string containing information about the country code\n",
    "        sub_system: A string containing information about the subsystem considered\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "  \n",
    "    hazard_output_path = pathway_dict['data_path'] / 'damage' / country_code\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not hazard_output_path.exists():\n",
    "        # Create the directory\n",
    "        hazard_output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    csv_file_path = hazard_output_path / '{}_{}_{}.csv'.format(country_code, hazard_type, sub_system)\n",
    "    \n",
    "    with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        \n",
    "        # Write header\n",
    "        csv_writer.writerow(['Country', 'Return period', 'Subsystem', 'Infrastructure type', 'Curve ID number', 'Damage ID number', 'Damage', 'Exposed assets'])\n",
    "        \n",
    "        # Write data\n",
    "        for key, value in damage_output.items():\n",
    "            csv_writer.writerow(list(key) + list(value))\n",
    "    \n",
    "    print(f\"CSV file created at: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fec1c657-fa81-43b5-a278-02d3a42729a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_infrastructure_hazard(pathway_dict, country_code, sub_system, infra_type_lst, hazard_type):\n",
    "\n",
    "    # get country osm data\n",
    "    data_loc = country_download(country_code)\n",
    "    \n",
    "    # get infrastructure data:\n",
    "    print(f'Time to extract OSM data for {sub_system}')\n",
    "    assets = extract_cis(data_loc, sub_system)\n",
    "    \n",
    "    # convert assets to epsg3857 (system in meters)\n",
    "    assets = gpd.GeoDataFrame(assets).set_crs(4326).to_crs(3857)\n",
    "    \n",
    "    if sub_system == 'power':\n",
    "        assets = assets.rename(columns={'power' : 'asset'}).reset_index(drop=True)\n",
    "        \n",
    "        #reclassify assets \n",
    "        mapping_dict = {\n",
    "            \"cable\" : \"cable\", \n",
    "            \"minor_cable\" : \"cable\",\n",
    "            \"line\" : \"transmission_line\", \n",
    "            \"minor_line\" : \"distribution_line\", \n",
    "            \"plant\" : \"plant\", \n",
    "            \"generator\" : \"plant\", \n",
    "            \"substation\" : \"substation\", \n",
    "            \"tower\" : \"power_tower\",\n",
    "            \"pole\" : \"power_pole\",\n",
    "            \"portal\" : \"power_tower\",\n",
    "            \n",
    "        }\n",
    "        assets['asset'] = assets.asset.apply(lambda x : mapping_dict[x])  #reclassification\n",
    "\n",
    "        #filter dataframe\n",
    "        infra_lst = ['plant', 'substation','power_tower','power_pole']\n",
    "        assets = delete_linestring_data(assets, infra_lst) #check for linestring data for specific infrastructure types and delete\n",
    "        infra_lst = ['transmission_line', 'distribution_line', 'cable'] \n",
    "        assets = delete_point_and_polygons(assets, infra_lst) #check for (multi-)polygon and point data and delete\n",
    "\n",
    "        # process geometries according to infra type\n",
    "        polygon_types = ['plant', 'substation']\n",
    "        point_types = ['power_tower', 'power_pole']\n",
    "        assets = process_selected_assets(assets, polygon_types, point_types)\n",
    "    \n",
    "    elif sub_system == 'road':\n",
    "        assets = assets.rename(columns={'highway' : 'asset'})\n",
    "        \n",
    "        #reclassify assets \n",
    "        mapping_dict = {\n",
    "            \"motorway\" : \"motorway\", \n",
    "            \"motorway_link\" : \"motorway\", \n",
    "            \"motorway_junction\" : \"motorway\",\n",
    "            \"trunk\" : \"trunk\",\n",
    "            \"trunk_link\" : \"trunk\",\n",
    "            \"primary\" : \"primary\", \n",
    "            \"primary_link\" : \"primary\", \n",
    "            \"secondary\" : \"secondary\", \n",
    "            \"secondary_link\" : \"secondary\", \n",
    "            \"tertiary\" : \"tertiary\", \n",
    "            \"tertiary_link\" : \"tertiary\", \n",
    "            \"residential\" : \"other\",           \n",
    "            \"road\" : \"other\", \n",
    "            \"unclassified\" : \"other\",\n",
    "            \"living_street\" : \"other\", \n",
    "            \"pedestrian\" : \"other\", \n",
    "            \"bus_guideway\" : \"other\", \n",
    "            \"escape\" : \"other\", \n",
    "            \"raceway\" : \"other\", \n",
    "            \"cycleway\" : \"other\", \n",
    "            \"construction\" : \"other\", \n",
    "            \"bus_stop\" : \"other\", \n",
    "            \"crossing\" : \"other\", \n",
    "            \"mini_roundabout\" : \"other\", \n",
    "            \"passing_place\" : \"other\", \n",
    "            \"rest_area\" : \"other\", \n",
    "            \"turning_circle\" : \"other\",\n",
    "            \"traffic_island\" : \"other\",\n",
    "            \"yes\" : \"other\",\n",
    "            \"emergency_bay\" : \"other\",\n",
    "            \"service\" : \"other\"\n",
    "        }\n",
    "        assets['asset'] = assets.asset.apply(lambda x : mapping_dict[x])  #reclassification\n",
    "    \n",
    "    elif sub_system == 'rail':\n",
    "        assets = assets.rename(columns={'railway' : 'asset'})\n",
    "        \n",
    "        #reclassify assets \n",
    "        mapping_dict = {\n",
    "            \"rail\" : \"railway\", \n",
    "            \"narrow_gauge\" : \"railway\", \n",
    "        }\n",
    "        assets['asset'] = assets.asset.apply(lambda x : mapping_dict[x])  #reclassification\n",
    "    \n",
    "    elif sub_system == 'air':\n",
    "        assets = assets.rename(columns={'aeroway' : 'asset'})   \n",
    "\n",
    "        #reclassify assets \n",
    "        mapping_dict = {\n",
    "            \"aerodrome\" : \"airport\", \n",
    "            \"terminal\" : \"terminal\",\n",
    "            \"runway\" : \"runway\"\n",
    "            }\n",
    "        assets['asset'] = assets.asset.apply(lambda x : mapping_dict[x])  #reclassification\n",
    "\n",
    "    elif sub_system == 'telecom':\n",
    "        #filter dataframe based on conditions \n",
    "        assets = (assets[(assets['man_made'] == 'tower') & (assets['tower_type'] == 'communication') |\n",
    "                (assets['man_made'] == 'mast') & (assets['tower_type'] == 'communication') |\n",
    "                (assets['man_made'] == 'communications_tower')|\n",
    "                (assets['man_made'] == 'mast') & (assets['tower_type'].isna())]).reset_index(drop=True)\n",
    "        assets = assets.drop(['tower_type'], axis=1) #drop columns that are of no further use\n",
    "\n",
    "        #reclassify assets\n",
    "        assets = assets.rename(columns={'man_made' : 'asset'})\n",
    "        mapping_dict = {\n",
    "            \"tower\" : \"communication_tower\", \n",
    "            \"communications_tower\" : \"communication_tower\",\n",
    "            \"mast\" : \"mast\", \n",
    "        }\n",
    "        assets['asset'] = assets.asset.apply(lambda x : mapping_dict[x])  #reclassification\n",
    "        \n",
    "        assets = create_point_from_polygon(remove_polygons_with_contained_points(assets)) #remove duplicates and transform polygons into points\n",
    "\n",
    "    elif sub_system == 'water_supply':\n",
    "        assets = assets.reset_index(drop=True)\n",
    "        assets = assets.rename(columns={'man_made' : 'asset'})\n",
    "        mapping_dict = {\n",
    "            \"water_tower\" : \"water_tower\",\n",
    "            \"water_well\" : \"water_well\",\n",
    "            \"reservoir_covered\" : \"reservoir_covered\",\n",
    "            \"water_works\" : \"water_treatment_plant\",\n",
    "            \"storage_tank\" : \"water_storage_tank\"\n",
    "        }\n",
    "        assets['asset'] = assets.asset.apply(lambda x : mapping_dict[x])  #reclassification\n",
    "        \n",
    "        # process geometries according to infra type\n",
    "        polygon_types = ['reservoir_covered', 'water_treatment_plant']\n",
    "        point_types = ['water_tower', 'water_well', 'storage_tank']\n",
    "        assets = process_selected_assets(assets, polygon_types, point_types)\n",
    "\n",
    "    elif sub_system == 'water':\n",
    "        assets = assets.reset_index(drop=True)\n",
    "        assets = assets.drop(assets[assets['emergency'] == 'fire_hydrant'].index).reset_index(drop=True) #drop linestrings\n",
    "        assets = assets.rename(columns={'man_made' : 'asset'})   \n",
    "\n",
    "    elif sub_system == 'waste_solid':\n",
    "        assets = assets.rename(columns={'amenity' : 'asset'})\n",
    "        assets = remove_contained_assets_and_convert(assets)\n",
    "\n",
    "    elif sub_system == 'waste_water':\n",
    "        assets = assets.rename(columns={'man_made' : 'asset'})\n",
    "        mapping_dict = {\n",
    "            \"wastewater_plant\" : \"wastewater_treatment_plant\", \n",
    "        }\n",
    "        assets['asset'] = assets.asset.apply(lambda x : mapping_dict[x])  #reclassification\n",
    "        assets = remove_contained_assets_and_convert(assets)\n",
    "\n",
    "    elif sub_system == 'wastewater':\n",
    "        column_names_lst = ['man_made', 'amenity']\n",
    "        assets = filter_dataframe(assets, column_names_lst)\n",
    "    \n",
    "    elif sub_system == 'healthcare':\n",
    "        column_names_lst = ['amenity' , 'building', 'healthcare']\n",
    "        assets = filter_dataframe(assets, column_names_lst)\n",
    "        list_of_assets_to_keep = [\"clinic\", \"doctors\", \"hospital\", \"dentist\", \"pharmacy\", \n",
    "                        \"physiotherapist\", \"alternative\", \"laboratory\", \"optometrist\", \"rehabilitation\", \n",
    "                        \"blood_donation\", \"birthing_center\"]\n",
    "        assets = assets.loc[assets.asset.isin(list_of_assets_to_keep)].reset_index(drop=True)\n",
    "    \n",
    "    elif sub_system == 'education':\n",
    "        column_names_lst = ['amenity' , 'building']\n",
    "        assets = filter_dataframe(assets, column_names_lst)\n",
    "        list_of_assets_to_keep =[\"college\", \"kindergarten\", \"library\", \"school\", \"university\"]\n",
    "        assets = assets.loc[assets.asset.isin(list_of_assets_to_keep)].reset_index(drop=True)\n",
    "    \n",
    "    # read hazard data\n",
    "    hazard_data_path = pathway_dict[hazard_type]\n",
    "    data_path = pathway_dict['data_path']\n",
    "    hazard_data_list = read_hazard_data(hazard_data_path,data_path,hazard_type,country_code)\n",
    "\n",
    "    # start analysis \n",
    "    print(f'{country_code} runs for {sub_system} for {hazard_type} for {len(hazard_data_list)} maps')\n",
    "\n",
    "    if hazard_type in ['windstorm','earthquake','landslide']:\n",
    "        # load country geometry file and create geometry to clip\n",
    "        ne_countries = gpd.read_file(data_path / \"natural_earth\" / \"ne_10m_admin_0_countries.shp\") #https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-0-countries/\n",
    "        bbox = ne_countries.loc[ne_countries['ISO_A3']==country_code].geometry.envelope.values[0].bounds\n",
    "        \n",
    "    collect_output = {}\n",
    "    for single_footprint in hazard_data_list: #tqdm(hazard_data_list,total=len(hazard_data_list)):\n",
    "    \n",
    "        hazard_name = single_footprint.parts[-1].split('.')[0]\n",
    "        \n",
    "        # load hazard map\n",
    "        if hazard_type in ['pluvial','fluvial']:\n",
    "            hazard_map = read_flood_map(single_footprint)\n",
    "        elif hazard_type in ['windstorm']:\n",
    "             hazard_map = read_windstorm_map(single_footprint,bbox)\n",
    "        elif hazard_type in ['earthquake']:\n",
    "             hazard_map = read_earthquake_map(single_footprint)\n",
    "        elif hazard_type in ['landslide']:\n",
    "             hazard_map = read_landslide_map(single_footprint)\n",
    "         \n",
    "        # convert hazard data to epsg 3857\n",
    "        hazard_map = gpd.GeoDataFrame(hazard_map).set_crs(4326).to_crs(3857)\n",
    "\n",
    "        # Loop through unique infrastructure types within the subsystem\n",
    "        for infra_type in infra_type_lst: \n",
    "            assets_infra_type = assets[assets['asset'] == infra_type].copy().reset_index(drop=True)\n",
    "        \n",
    "            # create dicts for quicker lookup\n",
    "            geom_dict = assets_infra_type['geometry'].to_dict()\n",
    "            type_dict = assets_infra_type['asset'].to_dict()\n",
    "\n",
    "            # read vulnerability and maxdam data:\n",
    "            infra_curves,maxdams,infra_units = read_vul_maxdam(data_path,hazard_type, infra_type)\n",
    "\n",
    "            # start analysis \n",
    "            print(f'{country_code} runs for {infra_type} for {hazard_type} for {hazard_name} map for {len(infra_curves.T)*len(maxdams)} combinations')\n",
    "    \n",
    "            if not assets_infra_type.empty:\n",
    "                # overlay assets\n",
    "                overlay_assets = pd.DataFrame(overlay_hazard_assets(hazard_map,buffer_assets(assets_infra_type)).T,columns=['asset','hazard_point'])\n",
    "            else: \n",
    "                overlay_assets = pd.DataFrame(columns=['asset','hazard_point']) #empty dataframe\n",
    "    \n",
    "            # convert dataframe to numpy array\n",
    "            hazard_numpified = hazard_map.to_numpy() \n",
    "\n",
    "            for infra_curve in infra_curves:\n",
    "                # get curves\n",
    "                curve = infra_curves[infra_curve[0]]\n",
    "                hazard_intensity = curve.index.values\n",
    "                fragility_values = (np.nan_to_num(curve.values,nan=(np.nanmax(curve.values)))).flatten()\n",
    "\n",
    "                for maxdam in maxdams:\n",
    "                    collect_inb = []\n",
    "                    collect_geom = []\n",
    "                    unit_maxdam = infra_units[maxdams[maxdams == maxdam].index[0]] #get unit maxdam\n",
    "                    \n",
    "                    for asset in tqdm(overlay_assets.groupby('asset'),total=len(overlay_assets.asset.unique())): #group asset items for different hazard points per asset and get total number of unique assets\n",
    "                        asset_geom = geom_dict[asset[0]]\n",
    "                        collect_geom.append(asset_geom.wkt)\n",
    "                        if np.max(fragility_values) == 0: #if exposure does not lead to damage\n",
    "                            collect_inb.append(0)  \n",
    "                        else:\n",
    "                            #collect_inb.append(get_damage_per_asset_og(asset,hazard_numpified,asset_geom,hazard_intensity,fragility_values,maxdam))\n",
    "                            collect_inb.append(get_damage_per_asset(asset,hazard_numpified,asset_geom,hazard_intensity,fragility_values,maxdam,unit_maxdam)) #get list of damages for specific asset\n",
    "                    collect_output[country_code, hazard_name, sub_system, infra_type, infra_curve[0], ((maxdams[maxdams == maxdam]).index)[0]] = np.sum(collect_inb), collect_geom # dictionary to store results for various combinations of hazard maps, infrastructure curves, and maximum damage values.\n",
    "        break #delete after testing, otherwise damage will only be assessed for first hazard map\n",
    "    return collect_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4a399442-b267-48e2-9f40-437d7a228259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of critical infrastructure systems to process\n",
    "cis_dict = {\n",
    "    \"energy\": {\"power\": [\"transmission_line\",\"distribution_line\",\"cable\",\"plant\",\"substation\",\n",
    "                        \"power_tower\",\"power_pole\"]},\n",
    "    \"transportation\": {\"road\":  [\"motorway\", \"trunk\", \"primary\", \"secondary\", \"tertiary\", \"other\"], \n",
    "                        \"air\": [\"airport\", \"runway\", \"terminal\"],\n",
    "                        \"rail\": [\"railway\"]},\n",
    "    \"water\": {\"water_supply\": [\"water_tower\", \"water_well\", \"reservoir_covered\",\n",
    "                                \"water_treatment_plant\", \"water_storage_tank\"]},\n",
    "    \"waste\": {\"waste_solid\": [\"waste_transfer_station\"],\n",
    "            \"waste_water\": [\"wastewater_treatment_plant\"]},\n",
    "    \"telecommunication\": {\"telecom\": [\"communication_tower\", \"mast\"]},\n",
    "    \"healthcare\": {\"healthcare\": [\"clinic\", \"doctors\", \"hospital\", \"dentist\", \"pharmacy\", \n",
    "                        \"physiotherapist\", \"alternative\", \"laboratory\", \"optometrist\", \"rehabilitation\", \n",
    "                        \"blood_donation\", \"birthing_center\"]},\n",
    "    \"education\": {\"education\": [\"college\", \"kindergarten\", \"library\", \"school\", \"university\"]}\n",
    "}\n",
    "\n",
    "#cis_dict = {\n",
    "#    \"waste\": {\n",
    "#            \"waste_water\": [\"wastewater_treatment_plant\"]},\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da673ad9-b62f-4b19-898e-e6428afb89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_type='jba_rb_rf'\n",
    "country_codes=['PAK'] #PNG, TAJ, PAK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15de89-7df7-4861-8315-03b04dd3bc81",
   "metadata": {},
   "source": [
    "# Run analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59a52342-2f5d-4a8e-98c4-b3ced83afd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to extract OSM data for power\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snn490\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\osgeo\\ogr.py:593: FutureWarning: Neither ogr.UseExceptions() nor ogr.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n",
      "extract points: 100%|| 2176/2176 [00:01<00:00, 1518.72it/s]\n",
      "extract multipolygons: 100%|| 59/59 [00:32<00:00,  1.83it/s]\n",
      "extract lines: 100%|| 48/48 [00:04<00:00, 10.55it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sub_system \u001b[38;5;129;01min\u001b[39;00m cis_dict[ci_system]:\n\u001b[0;32m      5\u001b[0m     infra_type_lst \u001b[38;5;241m=\u001b[39m cis_dict[ci_system][sub_system]\n\u001b[1;32m----> 6\u001b[0m     test \u001b[38;5;241m=\u001b[39m \u001b[43mcountry_infrastructure_hazard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathway_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcountry_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_system\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfra_type_lst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhazard_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     create_damage_csv(test, hazard_type, pathway_dict, country_code, sub_system)\n",
      "Cell \u001b[1;32mIn[12], line 180\u001b[0m, in \u001b[0;36mcountry_infrastructure_hazard\u001b[1;34m(pathway_dict, country_code, sub_system, infra_type_lst, hazard_type)\u001b[0m\n\u001b[0;32m    177\u001b[0m hazard_data_list \u001b[38;5;241m=\u001b[39m read_hazard_data(hazard_data_path,data_path,hazard_type,country_code)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# start analysis \u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m runs for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msub_system\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhazard_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhazard_data_list\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m maps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hazard_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindstorm\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearthquake\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlandslide\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# load country geometry file and create geometry to clip\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     ne_countries \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mread_file(data_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnatural_earth\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mne_10m_admin_0_countries.shp\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m#https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-0-countries/\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "pathway_dict = create_pathway_dict(data_path, flood_data_path, eq_data_path, landslide_data_path, cyclone_data_path)\n",
    "for country_code in country_codes: \n",
    "    for ci_system in cis_dict: \n",
    "        for sub_system in cis_dict[ci_system]:\n",
    "            infra_type_lst = cis_dict[ci_system][sub_system]\n",
    "            test = country_infrastructure_hazard(pathway_dict, country_code, sub_system, infra_type_lst, hazard_type)\n",
    "            create_damage_csv(test, hazard_type, pathway_dict, country_code, sub_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1643cd45-e4d5-4e08-8c9d-8fb4bdec2bd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('JAM', 'P_1in10', 'education', 'college', 'F21.6', 8): 6633.918668619526,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.6', 9): 94397.99580727497,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.6', 91): 109008.6931806091,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.6', 92): 25678.500818421366,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.7', 8): 1025.9792635472677,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.7', 9): 14599.272474776959,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.7', 91): 16858.91316074345,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.7', 92): 3971.349465483304,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.8', 8): 4103.917054189071,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.8', 9): 58397.089899107836,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.8', 91): 67435.6526429738,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.8', 92): 15885.397861933216,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.10', 8): 512.9896317736338,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.10', 9): 7299.6362373884795,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.10', 91): 8429.456580371725,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.10', 92): 1985.674732741652,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.11', 8): 4616.9066859627055,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.11', 9): 65696.72613649632,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.11', 91): 75865.10922334554,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.11', 92): 17871.07259467487,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.13', 8): 4918.063864467837,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.13', 9): 69982.07172089946,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.13', 91): 80813.73040950719,\n",
       " ('JAM', 'P_1in10', 'education', 'college', 'F21.13', 92): 19036.7885524689,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.6',\n",
       "  8): 76580.35873861582,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.6',\n",
       "  9): 1089707.7194091952,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.6',\n",
       "  91): 1258370.0895953095,\n",
       " ('JAM', 'P_1in10', 'education', 'kindergarten', 'F21.6', 92): 296426.42648704,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.7',\n",
       "  8): 16112.910827935102,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.7',\n",
       "  9): 229280.24366251822,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.7',\n",
       "  91): 264767.6946956597,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.7',\n",
       "  92): 62369.681413109894,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.8',\n",
       "  8): 48610.12175225258,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.8',\n",
       "  9): 691702.4911785875,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.8',\n",
       "  91): 798762.5583395981,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.8',\n",
       "  92): 188159.53489199607,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.10',\n",
       "  8): 13480.42358153668,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.10',\n",
       "  9): 191821.0084108544,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.10',\n",
       "  91): 221510.60806571043,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.10',\n",
       "  92): 52179.8781779743,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.11',\n",
       "  8): 52976.33137931657,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.11',\n",
       "  9): 753831.8989476217,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.11',\n",
       "  91): 870508.2081393544,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.11',\n",
       "  92): 205060.21201550585,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.13',\n",
       "  8): 74679.56929213888,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.13',\n",
       "  9): 1062660.2497065892,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.13',\n",
       "  91): 1227136.2768336192,\n",
       " ('JAM',\n",
       "  'P_1in10',\n",
       "  'education',\n",
       "  'kindergarten',\n",
       "  'F21.13',\n",
       "  92): 289068.8712025762,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.6', 8): 1695.4817705945313,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.6', 9): 24126.02400885314,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.6', 91): 27860.192648776534,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.6', 92): 6562.852547435591,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.7', 8): 262.2174351610464,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.7', 9): 3731.248690463332,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.7', 91): 4308.762492263852,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.7', 92): 1014.988419324177,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.8', 8): 1048.8697406441856,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.8', 9): 14924.994761853328,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.8', 91): 17235.04996905541,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.8', 92): 4059.953677296708,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.10', 8): 131.10871758052318,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.10', 9): 1865.6243452316655,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.10', 91): 2154.3812461319258,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.10', 92): 507.4942096620884,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.11', 8): 1179.9784582247084,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.11', 9): 16790.61910708499,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.11', 91): 19389.43121518733,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.11', 92): 4567.447886958796,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.13', 8): 1872.9816797217595,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.13', 9): 26651.77636045236,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.13', 91): 30776.874944741794,\n",
       " ('JAM', 'P_1in10', 'education', 'library', 'F21.13', 92): 7249.917280886976,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.6', 8): 4636744.523647439,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.6', 9): 65979010.59190381,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.6', 91): 76191084.99567014,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.6', 92): 17947860.682782244,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.7', 8): 978638.8543647315,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.7', 9): 13925637.483038707,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.7', 91): 16081014.546454672,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.7', 92): 3788104.7203090126,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.8', 8): 2961423.279117365,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.8', 9): 42139862.76438032,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.8', 91): 48662170.54156243,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.8', 92): 11463045.282153238,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.10', 8): 643149.7413797333,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.10', 9): 9151762.272486202,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.10', 91): 10568250.280017475,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.10', 92): 2489497.0808895463,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.11', 8): 3299675.378728626,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.11', 9): 46953054.163899,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.11', 91): 54220336.263225354,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.11', 92): 12772347.860399645,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.13', 8): 4310445.94272442,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.13', 9): 61335912.95798293,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.13', 91): 70829339.74827063,\n",
       " ('JAM', 'P_1in10', 'education', 'school', 'F21.13', 92): 16684827.655724492,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.6', 8): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.6', 9): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.6', 91): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.6', 92): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.7', 8): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.7', 9): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.7', 91): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.7', 92): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.8', 8): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.8', 9): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.8', 91): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.8', 92): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.10', 8): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.10', 9): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.10', 91): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.10', 92): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.11', 8): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.11', 9): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.11', 91): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.11', 92): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.13', 8): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.13', 9): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.13', 91): 0.0,\n",
       " ('JAM', 'P_1in10', 'education', 'university', 'F21.13', 92): 0.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f44ffd-3f59-421e-956b-8ce7a928f8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a1bce41-236d-43ea-82ae-361dc5a5af11",
   "metadata": {},
   "source": [
    "## Code to extract infra - testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5da7e385-708d-4b92-b31e-6e14b6035f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_system = 'waste_water'\n",
    "country_code = 'PAK'\n",
    "iso3 = 'PAK'\n",
    "ISO3 = 'PAK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "926c9c10-f707-4b77-afd0-1856d3b4a36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAK runs for waste_water for jba_rb_rf for 6 maps\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.15 GiB for an array with shape (1, 70914, 93186) and data type bool",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m     hazard_map \u001b[38;5;241m=\u001b[39m read_flood_map(single_footprint)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hazard_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjba_rb_rf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjba_rb_sw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjba_rd_rf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjba_rd_sw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjba_re_rf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjba_re_sw\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 24\u001b[0m     hazard_map \u001b[38;5;241m=\u001b[39m \u001b[43mread_flood_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_footprint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hazard_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindstorm\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     26\u001b[0m      hazard_map \u001b[38;5;241m=\u001b[39m read_windstorm_map(single_footprint,bbox)\n",
      "Cell \u001b[1;32mIn[53], line 330\u001b[0m, in \u001b[0;36mread_flood_map\u001b[1;34m(flood_map_path, diameter_distance)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03mRead flood map data from a NetCDF file and process it into a GeoDataFrame.\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m    *geopandas.GeoDataFrame*: A GeoDataFrame representing the processed flood map data.\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    328\u001b[0m flood_map \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(flood_map_path, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrasterio\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 330\u001b[0m flood_map_vector \u001b[38;5;241m=\u001b[39m \u001b[43mflood_map\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mband_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index() \u001b[38;5;66;03m#transform to dataframe\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m#remove data that will not be used\u001b[39;00m\n\u001b[0;32m    333\u001b[0m flood_map_vector \u001b[38;5;241m=\u001b[39m flood_map_vector\u001b[38;5;241m.\u001b[39mloc[(flood_map_vector\u001b[38;5;241m.\u001b[39mband_data \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (flood_map_vector\u001b[38;5;241m.\u001b[39mband_data \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100\u001b[39m)]\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\dataarray.py:3877\u001b[0m, in \u001b[0;36mDataArray.to_dataframe\u001b[1;34m(self, name, dim_order)\u001b[0m\n\u001b[0;32m   3874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3875\u001b[0m     ordered_dims \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39m_normalize_dim_order(dim_order\u001b[38;5;241m=\u001b[39mdim_order)\n\u001b[1;32m-> 3877\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordered_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3878\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [name \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;241m==\u001b[39m unique_name \u001b[38;5;28;01melse\u001b[39;00m c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m   3879\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\dataset.py:7161\u001b[0m, in \u001b[0;36mDataset._to_dataframe\u001b[1;34m(self, ordered_dims)\u001b[0m\n\u001b[0;32m   7159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_dataframe\u001b[39m(\u001b[38;5;28mself\u001b[39m, ordered_dims: Mapping[Any, \u001b[38;5;28mint\u001b[39m]):\n\u001b[0;32m   7160\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims]\n\u001b[1;32m-> 7161\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m   7162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variables\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordered_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   7163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\n\u001b[0;32m   7164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   7165\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoords\u001b[38;5;241m.\u001b[39mto_index([\u001b[38;5;241m*\u001b[39mordered_dims])\n\u001b[0;32m   7166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(columns, data)), index\u001b[38;5;241m=\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\dataset.py:7162\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   7159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_dataframe\u001b[39m(\u001b[38;5;28mself\u001b[39m, ordered_dims: Mapping[Any, \u001b[38;5;28mint\u001b[39m]):\n\u001b[0;32m   7160\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims]\n\u001b[0;32m   7161\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m-> 7162\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variables\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordered_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   7163\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m columns\n\u001b[0;32m   7164\u001b[0m     ]\n\u001b[0;32m   7165\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoords\u001b[38;5;241m.\u001b[39mto_index([\u001b[38;5;241m*\u001b[39mordered_dims])\n\u001b[0;32m   7166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(columns, data)), index\u001b[38;5;241m=\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\variable.py:1359\u001b[0m, in \u001b[0;36mVariable.set_dims\u001b[1;34m(self, dims, shape)\u001b[0m\n\u001b[0;32m   1354\u001b[0m expanded_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dims \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m self_dims) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;241m==\u001b[39m expanded_dims:\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;66;03m# don't use broadcast_to unless necessary so the result remains\u001b[39;00m\n\u001b[0;32m   1358\u001b[0m     \u001b[38;5;66;03m# writeable if possible\u001b[39;00m\n\u001b[1;32m-> 1359\u001b[0m     expanded_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1361\u001b[0m     dims_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(dims, shape))\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\variable.py:433\u001b[0m, in \u001b[0;36mVariable.data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, indexing\u001b[38;5;241m.\u001b[39mExplicitlyIndexed):\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\indexing.py:809\u001b[0m, in \u001b[0;36mMemoryCachedArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 809\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mget_duck_array()\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\indexing.py:803\u001b[0m, in \u001b[0;36mMemoryCachedArray._ensure_cached\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_cached\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m as_indexable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\indexing.py:760\u001b[0m, in \u001b[0;36mCopyOnWriteArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\indexing.py:760\u001b[0m, in \u001b[0;36mCopyOnWriteArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\indexing.py:623\u001b[0m, in \u001b[0;36mLazilyIndexedArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    619\u001b[0m     array \u001b[38;5;241m=\u001b[39m apply_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey)\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;66;03m# If the array is not an ExplicitlyIndexedNDArrayMixin,\u001b[39;00m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;66;03m# it may wrap a BackendArray so use its __getitem__\u001b[39;00m\n\u001b[1;32m--> 623\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, ExplicitlyIndexed):\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\rioxarray\\_io.py:453\u001b[0m, in \u001b[0;36mRasterioArrayWrapper.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplicit_indexing_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexingSupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOUTER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\indexing.py:987\u001b[0m, in \u001b[0;36mexplicit_indexing_adapter\u001b[1;34m(key, shape, indexing_support, raw_indexing_method)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Support explicit indexing by delegating to a raw indexing method.\u001b[39;00m\n\u001b[0;32m    966\u001b[0m \n\u001b[0;32m    967\u001b[0m \u001b[38;5;124;03mOuter and/or vectorized indexers are supported by indexing a second time\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;124;03mIndexing result, in the form of a duck numpy-array.\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    986\u001b[0m raw_key, numpy_indices \u001b[38;5;241m=\u001b[39m decompose_indexer(key, shape, indexing_support)\n\u001b[1;32m--> 987\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mraw_indexing_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_key\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_indices\u001b[38;5;241m.\u001b[39mtuple:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;66;03m# index the loaded np.ndarray\u001b[39;00m\n\u001b[0;32m    990\u001b[0m     indexable \u001b[38;5;241m=\u001b[39m NumpyIndexingAdapter(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\rioxarray\\_io.py:430\u001b[0m, in \u001b[0;36mRasterioArrayWrapper._getitem\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock:\n\u001b[0;32m    427\u001b[0m     riods \u001b[38;5;241m=\u001b[39m _ensure_warped_vrt(\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanager\u001b[38;5;241m.\u001b[39macquire(needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvrt_params\n\u001b[0;32m    429\u001b[0m     )\n\u001b[1;32m--> 430\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mriods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mband_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unsigned_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    432\u001b[0m         out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unsigned_dtype)\n",
      "File \u001b[1;32mrasterio\\\\_io.pyx:662\u001b[0m, in \u001b[0;36mrasterio._io.DatasetReaderBase.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 6.15 GiB for an array with shape (1, 70914, 93186) and data type bool"
     ]
    }
   ],
   "source": [
    "    pathway_dict = create_pathway_dict(data_path, flood_data_path, eq_data_path, landslide_data_path, cyclone_data_path)\n",
    "    # read hazard data\n",
    "    hazard_data_path = pathway_dict[hazard_type]\n",
    "    data_path = pathway_dict['data_path']\n",
    "    hazard_data_list = read_hazard_data(hazard_data_path,data_path,hazard_type,country_code)\n",
    "\n",
    "    # start analysis \n",
    "    print(f'{country_code} runs for {sub_system} for {hazard_type} for {len(hazard_data_list)} maps')\n",
    "\n",
    "    if hazard_type in ['windstorm','earthquake','landslide']:\n",
    "        # load country geometry file and create geometry to clip\n",
    "        ne_countries = gpd.read_file(data_path / \"natural_earth\" / \"ne_10m_admin_0_countries.shp\") #https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-0-countries/\n",
    "        bbox = ne_countries.loc[ne_countries['ISO_A3']==country_code].geometry.envelope.values[0].bounds\n",
    "        \n",
    "    collect_output = {}\n",
    "    for single_footprint in hazard_data_list: #tqdm(hazard_data_list,total=len(hazard_data_list)):\n",
    "    \n",
    "        hazard_name = single_footprint.parts[-1].split('.')[0]\n",
    "        \n",
    "        # load hazard map\n",
    "        if hazard_type in ['pluvial','fluvial']:\n",
    "            hazard_map = read_flood_map(single_footprint)\n",
    "        elif hazard_type in ['jba_rb_rf', 'jba_rb_sw', 'jba_rd_rf', 'jba_rd_sw', 'jba_re_rf', 'jba_re_sw']:\n",
    "            hazard_map = read_flood_jba_map(single_footprint)\n",
    "        elif hazard_type in ['windstorm']:\n",
    "             hazard_map = read_windstorm_map(single_footprint,bbox)\n",
    "        elif hazard_type in ['earthquake']:\n",
    "             hazard_map = read_earthquake_map(single_footprint)\n",
    "        elif hazard_type in ['landslide']:\n",
    "             hazard_map = read_landslide_map(single_footprint)\n",
    "         \n",
    "        # convert hazard data to epsg 3857\n",
    "        hazard_map = gpd.GeoDataFrame(hazard_map).set_crs(4326).to_crs(3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69e8df-5808-4cf2-9c26-97a5059a82ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_flood_jba_map(flood_map_path,diameter_distance=0.0002777777999999998869/2):\n",
    "    \"\"\"\n",
    "    Read flood map data from a NetCDF file and process it into a GeoDataFrame.\n",
    "    Arguments:\n",
    "        *flood_map_path* (Path): Path to the NetCDF file containing flood map data.\n",
    "        *diameter_distance* (float, optional): The diameter distance used for creating square geometries around data points. Default is 0.00083/2.\n",
    "    \n",
    "    Returns:\n",
    "        *geopandas.GeoDataFrame*: A GeoDataFrame representing the processed flood map data.\n",
    "    \"\"\"\n",
    "    \n",
    "    flood_map = xr.open_dataset(flood_map_path, engine=\"rasterio\")\n",
    "\n",
    "    flood_map_vector = flood_map['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "    \n",
    "    #remove data that will not be used\n",
    "    flood_map_vector = flood_map_vector.loc[(flood_map_vector.band_data > 0) & (flood_map_vector.band_data < 100)]\n",
    "    \n",
    "    # create geometry values and drop lat lon columns\n",
    "    flood_map_vector['geometry'] = [shapely.points(x) for x in list(zip(flood_map_vector['x'],flood_map_vector['y']))]\n",
    "    flood_map_vector = flood_map_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "    \n",
    "    # drop all non values to reduce size\n",
    "    flood_map_vector = flood_map_vector.loc[~flood_map_vector['band_data'].isna()].reset_index(drop=True)\n",
    "    \n",
    "    # and turn them into squares again:\n",
    "    flood_map_vector.geometry= shapely.buffer(flood_map_vector.geometry,distance=diameter_distance,cap_style='square').values \n",
    "\n",
    "    return flood_map_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bfd75a24-671c-4ec4-b438-38705620dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_map = xr.open_dataset(single_footprint, engine=\"rasterio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "24d72846-5e61-4b99-9e65-7af8ebbc5e4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.15 GiB for an array with shape (1, 70914, 93186) and data type bool",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m flood_map_vector \u001b[38;5;241m=\u001b[39m \u001b[43mflood_map\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mband_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index() \u001b[38;5;66;03m#transform to dataframe\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\dataarray.py:3877\u001b[0m, in \u001b[0;36mDataArray.to_dataframe\u001b[1;34m(self, name, dim_order)\u001b[0m\n\u001b[0;32m   3874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3875\u001b[0m     ordered_dims \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39m_normalize_dim_order(dim_order\u001b[38;5;241m=\u001b[39mdim_order)\n\u001b[1;32m-> 3877\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordered_dims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3878\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [name \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;241m==\u001b[39m unique_name \u001b[38;5;28;01melse\u001b[39;00m c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m   3879\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\dataset.py:7161\u001b[0m, in \u001b[0;36mDataset._to_dataframe\u001b[1;34m(self, ordered_dims)\u001b[0m\n\u001b[0;32m   7159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_dataframe\u001b[39m(\u001b[38;5;28mself\u001b[39m, ordered_dims: Mapping[Any, \u001b[38;5;28mint\u001b[39m]):\n\u001b[0;32m   7160\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims]\n\u001b[1;32m-> 7161\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m   7162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variables\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordered_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   7163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\n\u001b[0;32m   7164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   7165\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoords\u001b[38;5;241m.\u001b[39mto_index([\u001b[38;5;241m*\u001b[39mordered_dims])\n\u001b[0;32m   7166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(columns, data)), index\u001b[38;5;241m=\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\dataset.py:7162\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   7159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_dataframe\u001b[39m(\u001b[38;5;28mself\u001b[39m, ordered_dims: Mapping[Any, \u001b[38;5;28mint\u001b[39m]):\n\u001b[0;32m   7160\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims]\n\u001b[0;32m   7161\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m-> 7162\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variables\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mordered_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   7163\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m columns\n\u001b[0;32m   7164\u001b[0m     ]\n\u001b[0;32m   7165\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoords\u001b[38;5;241m.\u001b[39mto_index([\u001b[38;5;241m*\u001b[39mordered_dims])\n\u001b[0;32m   7166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(columns, data)), index\u001b[38;5;241m=\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\variable.py:1359\u001b[0m, in \u001b[0;36mVariable.set_dims\u001b[1;34m(self, dims, shape)\u001b[0m\n\u001b[0;32m   1354\u001b[0m expanded_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dims \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m self_dims) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;241m==\u001b[39m expanded_dims:\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;66;03m# don't use broadcast_to unless necessary so the result remains\u001b[39;00m\n\u001b[0;32m   1358\u001b[0m     \u001b[38;5;66;03m# writeable if possible\u001b[39;00m\n\u001b[1;32m-> 1359\u001b[0m     expanded_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1361\u001b[0m     dims_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(dims, shape))\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\variable.py:433\u001b[0m, in \u001b[0;36mVariable.data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, indexing\u001b[38;5;241m.\u001b[39mExplicitlyIndexed):\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\indexing.py:809\u001b[0m, in \u001b[0;36mMemoryCachedArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 809\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mget_duck_array()\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\indexing.py:803\u001b[0m, in \u001b[0;36mMemoryCachedArray._ensure_cached\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_cached\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m as_indexable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\indexing.py:760\u001b[0m, in \u001b[0;36mCopyOnWriteArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\indexing.py:760\u001b[0m, in \u001b[0;36mCopyOnWriteArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\indexing.py:623\u001b[0m, in \u001b[0;36mLazilyIndexedArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    619\u001b[0m     array \u001b[38;5;241m=\u001b[39m apply_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey)\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;66;03m# If the array is not an ExplicitlyIndexedNDArrayMixin,\u001b[39;00m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;66;03m# it may wrap a BackendArray so use its __getitem__\u001b[39;00m\n\u001b[1;32m--> 623\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, ExplicitlyIndexed):\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\rioxarray\\_io.py:453\u001b[0m, in \u001b[0;36mRasterioArrayWrapper.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplicit_indexing_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexingSupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOUTER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\core\\indexing.py:987\u001b[0m, in \u001b[0;36mexplicit_indexing_adapter\u001b[1;34m(key, shape, indexing_support, raw_indexing_method)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Support explicit indexing by delegating to a raw indexing method.\u001b[39;00m\n\u001b[0;32m    966\u001b[0m \n\u001b[0;32m    967\u001b[0m \u001b[38;5;124;03mOuter and/or vectorized indexers are supported by indexing a second time\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;124;03mIndexing result, in the form of a duck numpy-array.\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    986\u001b[0m raw_key, numpy_indices \u001b[38;5;241m=\u001b[39m decompose_indexer(key, shape, indexing_support)\n\u001b[1;32m--> 987\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mraw_indexing_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_key\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_indices\u001b[38;5;241m.\u001b[39mtuple:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;66;03m# index the loaded np.ndarray\u001b[39;00m\n\u001b[0;32m    990\u001b[0m     indexable \u001b[38;5;241m=\u001b[39m NumpyIndexingAdapter(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\rioxarray\\_io.py:430\u001b[0m, in \u001b[0;36mRasterioArrayWrapper._getitem\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock:\n\u001b[0;32m    427\u001b[0m     riods \u001b[38;5;241m=\u001b[39m _ensure_warped_vrt(\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanager\u001b[38;5;241m.\u001b[39macquire(needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvrt_params\n\u001b[0;32m    429\u001b[0m     )\n\u001b[1;32m--> 430\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mriods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mband_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unsigned_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    432\u001b[0m         out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unsigned_dtype)\n",
      "File \u001b[1;32mrasterio\\\\_io.pyx:662\u001b[0m, in \u001b[0;36mrasterio._io.DatasetReaderBase.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 6.15 GiB for an array with shape (1, 70914, 93186) and data type bool"
     ]
    }
   ],
   "source": [
    "flood_map_vector = flood_map['band_data'].to_dataframe().reset_index() #transform to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c36e416e-f176-4c84-be9c-b0dc76f8fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "    pathway_dict = create_pathway_dict(data_path, flood_data_path, eq_data_path, landslide_data_path, cyclone_data_path)\n",
    "    # read hazard data\n",
    "    hazard_data_path = pathway_dict[hazard_type]\n",
    "    data_path = pathway_dict['data_path']\n",
    "    hazard_data_list = read_hazard_data(hazard_data_path,data_path,hazard_type,country_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "24308525-88b2-4994-b889-53fc2e4544a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP100_RB_30m_4326.tif'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP100_RB_30m_4326.tif.aux.xml'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP100_RB_30m_4326.tif.ovr'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP100_RB_30m_4326.tif.vat.dbf'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP1500_RB_30m_4326.tif'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP1500_RB_30m_4326.tif.aux.xml'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP1500_RB_30m_4326.tif.ovr'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP1500_RB_30m_4326.tif.vat.dbf'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP200_RB_30m_4326.tif'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP200_RB_30m_4326.tif.aux.xml'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP200_RB_30m_4326.tif.ovr'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP200_RB_30m_4326.tif.vat.dbf'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP20_RB_30m_4326.tif'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP20_RB_30m_4326.tif.aux.xml'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP20_RB_30m_4326.tif.ovr'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP20_RB_30m_4326.tif.vat.dbf'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP500_RB_30m_4326.tif'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP500_RB_30m_4326.tif.aux.xml'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP500_RB_30m_4326.tif.ovr'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP500_RB_30m_4326.tif.vat.dbf'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP50_RB_30m_4326.tif'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP50_RB_30m_4326.tif.aux.xml'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP50_RB_30m_4326.tif.ovr'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP50_RB_30m_4326.tif.vat.dbf'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_30m_4326_FLRF_U_RB_depth_check'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_FLRF_U_RB_30m_4326.vrt')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hazard_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "201a6752-af73-47af-b788-9e95ecf11354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "880ee391-5a07-412d-b623-11a52a8435a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP100_RB_30m_4326.tif'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP1500_RB_30m_4326.tif'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP200_RB_30m_4326.tif'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP20_RB_30m_4326.tif'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP500_RB_30m_4326.tif'),\n",
       " WindowsPath('C:/Users/snn490/OneDrive - Vrije Universiteit Amsterdam/ADB/Data/pakistan-30m/82_PK_30m_4326/PK_202105_RB_30m_4326/FLRF_U/PK_202105_FLRF_U_RP50_RB_30m_4326.tif')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tif_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d760647b-5c3a-4821-ac8a-b7cf6b93345a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP100_RB_30m_4326.tif\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP100_RB_30m_4326.tif.aux.xml\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP100_RB_30m_4326.tif.ovr\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP100_RB_30m_4326.tif.vat.dbf\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP1500_RB_30m_4326.tif\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP1500_RB_30m_4326.tif.aux.xml\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP1500_RB_30m_4326.tif.ovr\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP1500_RB_30m_4326.tif.vat.dbf\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP200_RB_30m_4326.tif\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP200_RB_30m_4326.tif.aux.xml\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP200_RB_30m_4326.tif.ovr\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP200_RB_30m_4326.tif.vat.dbf\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP20_RB_30m_4326.tif\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP20_RB_30m_4326.tif.aux.xml\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP20_RB_30m_4326.tif.ovr\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP20_RB_30m_4326.tif.vat.dbf\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP500_RB_30m_4326.tif\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP500_RB_30m_4326.tif.aux.xml\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP500_RB_30m_4326.tif.ovr\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP500_RB_30m_4326.tif.vat.dbf\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP50_RB_30m_4326.tif\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP50_RB_30m_4326.tif.aux.xml\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP50_RB_30m_4326.tif.ovr\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_202105_FLRF_U_RP50_RB_30m_4326.tif.vat.dbf\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_30m_4326_FLRF_U_RB_depth_check\n",
      "C:\\Users\\snn490\\OneDrive - Vrije Universiteit Amsterdam\\ADB\\Data\\pakistan-30m\\82_PK_30m_4326\\PK_202105_RB_30m_4326\\FLRF_U\\PK_FLRF_U_RB_30m_4326.vrt\n"
     ]
    }
   ],
   "source": [
    "for file_path in hazard_data_list:\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d08597-857c-4ddf-81b1-e3d4c4f7fd8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
