{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a5263e9-087d-4863-a4e5-839dc33017bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np \n",
    "import shapely \n",
    "import csv\n",
    "import ast\n",
    "\n",
    "import osm_flex.download as dl\n",
    "import osm_flex.extract as ex\n",
    "from osm_flex.simplify import remove_contained_points,remove_exact_duplicates,remove_contained_polys\n",
    "from osm_flex.config import OSM_DATA_DIR,DICT_GEOFABRIK\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lonboard import viz\n",
    "from lonboard.colormap import apply_continuous_cmap\n",
    "from palettable.colorbrewer.sequential import Blues_9\n",
    "\n",
    "from pathlib import Path\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bf45fe-9093-4812-9712-bf2982dfb542",
   "metadata": {},
   "source": [
    "## Set pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e31f3d3d-60ce-4348-9742-427aafa4f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change paths to make it work on your own machine\n",
    "\n",
    "# Set path to inputdata\n",
    "hazard_data_path = os.path.abspath(r'\\\\labsdfs.labs.vu.nl\\labsdfs\\BETA-IVM-BAZIS\\data_catalogue\\open_street_map\\global_hazards')\n",
    "fathom_data_path = os.path.abspath(r'\\\\labsdfs.labs.vu.nl\\labsdfs\\BETA-IVM-BAZIS\\eks510\\fathom-global')\n",
    "shapes_file = 'global_countries_advanced.geofeather'\n",
    "country_shapefile_path = os.path.join(os.path.abspath(r'\\\\labsdfs.labs.vu.nl\\labsdfs\\BETA-IVM-BAZIS\\snn490\\Datasets\\Administrative_boundaries\\global_countries_buffer'), shapes_file)\n",
    "#country_shapefile_path = os.path.abspath(os.path.join(local_path,'Datasets','Administrative_boundaries', 'global_countries_buffer', shapes_file)) #shapefiles with buffer around country\n",
    "\n",
    "# path to our python scripts\n",
    "sys.path.append(os.path.join('..','scripts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2516f2aa-1985-4461-8a2f-b39f38f59470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define paths\n",
    "p = Path('..')\n",
    "data_path = Path(pathlib.Path.home().parts[0]) / 'Projects' / 'gmhcira' / 'data' #should contain folder 'Vulnerability' with vulnerability data\n",
    "flood_data_path = Path(pathlib.Path('Z:') / 'eks510' / 'fathom-global') # Flood data\n",
    "eq_data_path = Path(pathlib.Path('Z:') / 'data_catalogue' / 'open_street_map' / 'global_hazards' / 'earthquakes') # Earthquake data\n",
    "landslide_data_path = Path(pathlib.Path('Z:') / 'data_catalogue' / 'open_street_map' / 'global_hazards' / 'landslides') # Landslide data\n",
    "landslide_data_path = Path(pathlib.Path.home().parts[0]) / 'Projects' / 'gmhcira' / 'data' # Landslide data\n",
    "cyclone_data_path = Path(pathlib.Path('Z:') / 'data_catalogue' / 'open_street_map' / 'global_hazards' / 'tropical_cyclones') # Cyclone data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e079f44f-c211-4f5e-95c3-67534d36fbbd",
   "metadata": {},
   "source": [
    "## Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75678664-7845-4ecf-b2b5-dab7ab8d9a5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'from_geofeather' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m hazard_dct \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoastal_flooding\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m1000\u001b[39m], \n\u001b[0;32m      2\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfluvial_flooding\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m75\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m1000\u001b[39m], \n\u001b[0;32m      3\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearthquakes\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m475\u001b[39m, \u001b[38;5;241m975\u001b[39m, \u001b[38;5;241m1500\u001b[39m, \u001b[38;5;241m2475\u001b[39m], \n\u001b[0;32m      4\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtropical_cyclones\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m , \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m2000\u001b[39m, \u001b[38;5;241m5000\u001b[39m, \u001b[38;5;241m10000\u001b[39m], \n\u001b[0;32m      5\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlandslides\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m]} \u001b[38;5;66;03m#keys are hazards, lists are return periods associated with hazard\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#and open country geometry file\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m shape_countries \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_geofeather\u001b[49m(country_shapefile_path) \u001b[38;5;66;03m#open as geofeather\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'from_geofeather' is not defined"
     ]
    }
   ],
   "source": [
    "hazard_dct = {'coastal_flooding': [2, 5, 10, 25, 50, 100, 250, 500, 1000], \n",
    "              'fluvial_flooding': [5, 10, 20, 50, 75, 100, 200, 250, 500, 1000], \n",
    "              'earthquakes': [250, 475, 975, 1500, 2475], \n",
    "              'tropical_cyclones': [10, 20 , 50, 100, 200, 1000, 2000, 5000, 10000], \n",
    "              'landslides': [None]} #keys are hazards, lists are return periods associated with hazard\n",
    "\n",
    "#and open country geometry file\n",
    "shape_countries = from_geofeather(country_shapefile_path) #open as geofeather\n",
    "\n",
    "def soft_overlay():\n",
    "    shape_countries = from_geofeather(country_shapefile_path) #open as geofeather\n",
    "    country_shape = shape_countries[shape_countries['ISO_3digit'] == country]\n",
    "    if country_shape.empty == False: #if ISO_3digit in shape_countries\n",
    "        spat_tree = pygeos.STRtree(grid_data.geometry)\n",
    "        grid_data_area = (grid_data.loc[spat_tree.query(country_shape.geometry.iloc[0],predicate='intersects').tolist()]).sort_index(ascending=True) #get grids that overlap with cover_box\n",
    "        grid_data_area = grid_data_area.reset_index().rename(columns = {'index':'grid_number'}) #get index as column and name column grid_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a955c-e9fb-448a-924b-4bcf6f9dc79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hazard_data(hazard_data_path,hazard_type):\n",
    "    \"\"\"\n",
    "    Read hazard data files for a specific hazard type.\n",
    "    Arguments:\n",
    "        *hazard_data_path* (Path): Base directory path where hazard data is stored.\n",
    "        *hazard_type* (str): Type of hazard for which data needs to be read ('fluvial', 'pluvial', 'windstorm', 'earthquake', 'landslides').\n",
    "    \n",
    "    Returns:\n",
    "        *list*: A list of Path objects representing individual hazard data files for the specified hazard type.\n",
    "    \"\"\"  \n",
    "\n",
    "    if hazard_type == 'fluvial':\n",
    "        hazard_data = hazard_data_path / 'Jamaica' / 'fluvial_undefended' # need to make country an input\n",
    "        return list(hazard_data.iterdir())\n",
    "\n",
    "    elif hazard_type == 'pluvial':\n",
    "        hazard_data = hazard_data_path / 'Jamaica' / 'pluvial' # need to make country an input\n",
    "        return list(hazard_data.iterdir())\n",
    "\n",
    "    elif hazard_type == 'windstorm':\n",
    "        hazard_data = hazard_data_path \n",
    "        return list(hazard_data.iterdir())\n",
    "\n",
    "    elif hazard_type == 'earthquake':\n",
    "        hazard_data = hazard_data_path\n",
    "        return list(hazard_data.iterdir())\n",
    "\n",
    "    elif hazard_type == 'landslides':\n",
    "        hazard_data = hazard_data_path \n",
    "        return list(hazard_data.iterdir())\n",
    "\n",
    "\n",
    "def read_flood_map(flood_map_path,diameter_distance=0.00083/2):\n",
    "    \"\"\"\n",
    "    Read flood map data from a NetCDF file and process it into a GeoDataFrame.\n",
    "    Arguments:\n",
    "        *flood_map_path* (Path): Path to the NetCDF file containing flood map data.\n",
    "        *diameter_distance* (float, optional): The diameter distance used for creating square geometries around data points. Default is 0.00083/2.\n",
    "    \n",
    "    Returns:\n",
    "        *geopandas.GeoDataFrame*: A GeoDataFrame representing the processed flood map data.\n",
    "    \"\"\"\n",
    "    \n",
    "    flood_map = xr.open_dataset(flood_map_path, engine=\"rasterio\")\n",
    "\n",
    "    flood_map_vector = flood_map['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "    \n",
    "    #remove data that will not be used\n",
    "    flood_map_vector = flood_map_vector.loc[(flood_map_vector.band_data > 0) & (flood_map_vector.band_data < 100)]\n",
    "    \n",
    "    # create geometry values and drop lat lon columns\n",
    "    flood_map_vector['geometry'] = [shapely.points(x) for x in list(zip(flood_map_vector['x'],flood_map_vector['y']))]\n",
    "    flood_map_vector = flood_map_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "    \n",
    "    # drop all non values to reduce size\n",
    "    flood_map_vector = flood_map_vector.loc[~flood_map_vector['band_data'].isna()].reset_index(drop=True)\n",
    "    \n",
    "    # and turn them into squares again:\n",
    "    flood_map_vector.geometry= shapely.buffer(flood_map_vector.geometry,distance=diameter_distance,cap_style='square').values \n",
    "\n",
    "    return flood_map_vector\n",
    "\n",
    "def read_windstorm_map(windstorm_map_path,bbox):\n",
    "     \n",
    "    # load data from NetCDF file\n",
    "    with xr.open_dataset(flood_map_path) as ds:\n",
    "        \n",
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        ds = ds.rio.clip_box(minx=bbox[0], miny=bbox[1], maxx=bbox[2], maxy=bbox[3])\n",
    "        #ds['band_data'] = ds['band_data']/0.88*1.11 #convert 10-min sustained wind speed to 3-s gust wind speed\n",
    "    \n",
    "        ds_vector = ds['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "        \n",
    "        #remove data that will not be used\n",
    "        ds_vector = ds_vector.loc[(ds_vector.band_data > 0) & (ds_vector.band_data < 100)]\n",
    "        \n",
    "        # create geometry values and drop lat lon columns\n",
    "        ds_vector['geometry'] = [shapely.points(x) for x in list(zip(ds_vector['x'],ds_vector['y']))]\n",
    "        ds_vector = ds_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "        ds_vector['geometry'] = shapely.buffer(ds_vector.geometry, distance=0.1/2, cap_style='square').values\n",
    "    \n",
    "        return ds_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5ec4b-1300-444e-8900-5f025ebab92d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed628934-cd0f-4a46-a074-ba6261818181",
   "metadata": {},
   "source": [
    "## Flood data (coastal, fluvial and pluvial) - Fathom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6137d84-f279-4569-9b43-d2c0591311e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for country_full in country_lst:\n",
    "    file_path_lst = os.path.abspath(os.path.join(dathom_data_path, country_full, 'fluvial_undefended', 'FU_1in{}'.format(rp))) #pathway to file\n",
    "    hazard_country_df = transform_raster_to_vectorgrid.core(file_path, hazard_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c68ed3-b275-446a-97be-61d7d1fddbbd",
   "metadata": {},
   "source": [
    "## Tropical cyclone data - STORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f6c966-64f8-453a-b28f-23cb2d4c6f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = ['EP', 'NA', 'NI', 'SI', 'SP', 'WP']\n",
    "for basin in basins: \n",
    "    file_path = os.path.abspath(os.path.join(hazard_data_path, hazard_type, 'STORM_FIXED_RETURN_PERIODS_{}.nc'.format(basin))) #pathway to file\n",
    "    \n",
    "    #open file\n",
    "    #transform to vector files\n",
    "    #clip per country\n",
    "    #save as feather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1d11e8-2eb6-4dbf-8d36-e287c2916244",
   "metadata": {},
   "source": [
    "## Earthquakes - GEM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0346840-fcaf-4c09-9b17-937413b53d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_lst = [os.path.abspath(os.path.join(hazard_data_path, hazard_type, 'rp_'.format(rp), 'gar17pga{}.tif'.format(rp))) for rp in hazard_dct[hazard_type]] #pathway to file\n",
    "\n",
    "#transform to vector data\n",
    "for file_path in file_path_lst:\n",
    "    hazard_df = transform_raster_to_vectorgrid(file_path,hazard_type)\n",
    "    print('Hazard data for has been loaded in polygon format for the following hazard type: {}'.format(hazard_type))\n",
    "\n",
    "    #soft overlay of hazard data with countries\n",
    "    country_shape = shape_countries[shape_countries['ISO_3digit'] == country]\n",
    "    if country_shape.empty == False: #if ISO_3digit in shape_countries\n",
    "        print(\"Time to overlay and output '{}' hazard data for the following country: {}\".format(hazard_type, country))\n",
    "        spat_tree = pygeos.STRtree(hazard_df.geometry)\n",
    "        hazard_country_df = (hazard_df.loc[spat_tree.query(country_shape.geometry.iloc[0],predicate='intersects').tolist()]).sort_index(ascending=True) #get grids that overlap with country\n",
    "        hazard_country_df = hazard_country_df.reset_index() #.rename(columns = {'index':'grid_number'}) #get index as column and name column grid_number\n",
    "\n",
    "        #save data\n",
    "        rp = str(hazard_dct[hazard_type][file_path_lst.index(file_path)]) #get return period using index\n",
    "        Path(os.path.abspath(os.path.join(hazard_data_path, hazard_type, rp))).mkdir(parents=True, exist_ok=True) \n",
    "        to_geofeather(hazard_country_df, os.path.join(hazard_data_path, hazard_type, rp, '{}_{}_{}.feather'.format(hazard_type, rp, country)), crs=\"EPSG:4326\") #save as geofeather #save file for each country\n",
    "        #to_geofeather(hazard_country_df, os.path.join(hazard_data_path, hazard_type, rp, '{}_{}_{}.feather'.format(hazard_type, rp, country)), crs=\"EPSG:4326\") #save as geofeather #save file for each country\n",
    "        temp_df = functions.transform_to_gpd(hazard_country_df) #transform df to gpd with shapely geometries\n",
    "        temp_df.to_file(os.path.join(hazard_data_path, hazard_type, rp, '{}_{}_{}.gpkg'.format(hazard_type, rp, country)), layer=' ', driver=\"GPKG\")              \n",
    "    else:\n",
    "        print(\"Country '{}' not specified in file containing shapefiles of countries with ISO_3digit codes. Please check inconsistency\".format(country))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d935587f-3af9-48e9-bc39-77349f8b9cf7",
   "metadata": {},
   "source": [
    "## Earthquakes - GEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f16a9b71-9f45-469c-a583-281ca055a310",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "did not find a match in any of xarray's currently installed IO backends ['scipy', 'rasterio']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load data from Excel file\u001b[39;00m\n\u001b[0;32m      2\u001b[0m landslide_map_path \u001b[38;5;241m=\u001b[39m Path(pathlib\u001b[38;5;241m.\u001b[39mPath\u001b[38;5;241m.\u001b[39mhome()\u001b[38;5;241m.\u001b[39mparts[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProjects\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgmhcira\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv2023_2_PGA_rock_475.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;66;03m# Landslide data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlandslide_map_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m ds:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\backends\\api.py:554\u001b[0m, in \u001b[0;36mopen_dataset\u001b[1;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(backend_kwargs)\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 554\u001b[0m     engine \u001b[38;5;241m=\u001b[39m \u001b[43mplugins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_array_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    557\u001b[0m     from_array_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniforge3\\envs\\py311\\Lib\\site-packages\\xarray\\backends\\plugins.py:197\u001b[0m, in \u001b[0;36mguess_engine\u001b[1;34m(store_spec)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound the following matches with the input file in xarray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms IO \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackends: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompatible_engines\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. But their dependencies may not be installed, see:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.xarray.dev/en/stable/user-guide/io.html \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m     )\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n",
      "\u001b[1;31mValueError\u001b[0m: did not find a match in any of xarray's currently installed IO backends ['scipy', 'rasterio']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html"
     ]
    }
   ],
   "source": [
    "    # load data from Excel file\n",
    "    landslide_map_path = Path(pathlib.Path.home().parts[0]) / 'Projects' / 'gmhcira' / 'data' / 'v2023_2_PGA_rock_475.csv'# Landslide data\n",
    "\n",
    "    with xr.open_dataset(landslide_map_path) as ds:\n",
    "        print(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5c06ddf-001b-4159-8651-71a3e04c1bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               lon       lat  PGA-0.002105\n",
      "0         16.76948 -28.98803       0.05761\n",
      "1         16.77544 -29.04607       0.05745\n",
      "2         16.82456 -28.95738       0.05772\n",
      "3         16.83054 -29.01545       0.05750\n",
      "4         16.83652 -29.07349       0.05743\n",
      "...            ...       ...           ...\n",
      "3613671  178.96487  51.58447       0.73558\n",
      "3613672  178.97079  51.52631       0.76061\n",
      "3613673  179.05708  51.49823       0.76937\n",
      "3613674  179.23502  51.38363       0.82476\n",
      "3613675  179.40123  51.38524       0.82278\n",
      "\n",
      "[3613676 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#https://docs.xarray.dev/en/latest/getting-started-guide/faq.html\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV file into pandas DataFrame using the \"c\" engine\n",
    "df = pd.read_csv(landslide_map_path)#, engine=\"c\")\n",
    "\n",
    "# Convert `:py:func:pandas` DataFrame to xarray.Dataset\n",
    "\n",
    "#ds = xr.Dataset.from_dataframe(df)\n",
    "\n",
    "# Prints the resulting xarray dataset\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a1ba8-e280-40ca-a62e-49c415999c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        ds = ds.rio.clip_box(minx=bbox[0], miny=bbox[1], maxx=bbox[2], maxy=bbox[3])\n",
    "        #ds['band_data'] = ds['band_data']/0.88*1.11 #convert 10-min sustained wind speed to 3-s gust wind speed\n",
    "    \n",
    "        ds_vector = ds['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "        \n",
    "        #remove data that will not be used\n",
    "        ds_vector = ds_vector.loc[(ds_vector.band_data > 0) & (ds_vector.band_data < 100)]\n",
    "        \n",
    "        # create geometry values and drop lat lon columns\n",
    "        ds_vector['geometry'] = [shapely.points(x) for x in list(zip(ds_vector['x'],ds_vector['y']))]\n",
    "        ds_vector = ds_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "        ds_vector['geometry'] = shapely.buffer(ds_vector.geometry, distance=0.1/2, cap_style='square').values\n",
    "    \n",
    "        return ds_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e53d4-3987-4547-9c85-72e269dc8395",
   "metadata": {},
   "source": [
    "## Landslides - ThinkHazard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd74560-aeb4-4fac-b941-550a804f1b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "landslide_methods = ['LS_RF_Mean','LS_RF_Median','LS_EQ'] #RF = rainfaill triggered, EQ = earthquake-triggered landslides\n",
    "methods = ['mean', 'median']\n",
    "for method in methods:\n",
    "    file_path = os.path.abspath(os.path.join(hazard_data_path, hazard_type, 'LS_RF_{}_1980-2018_COG.tif'.format(method))) #pathway to file\n",
    "    \n",
    "    #open file\n",
    "    #transform to vector files\n",
    "    #clip per country\n",
    "    #save as feather"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
