{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2674e289-cc8d-4452-8581-dc1ae337b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np \n",
    "import shapely \n",
    "import csv\n",
    "import ast\n",
    "import h3\n",
    "\n",
    "import osm_flex.download as dl\n",
    "import osm_flex.extract as ex\n",
    "from osm_flex.simplify import remove_contained_points,remove_exact_duplicates,remove_contained_polys\n",
    "from osm_flex.config import OSM_DATA_DIR,DICT_GEOFABRIK\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lonboard import viz\n",
    "from lonboard.colormap import apply_continuous_cmap\n",
    "from palettable.colorbrewer.sequential import Blues_9\n",
    "\n",
    "from pathlib import Path\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4793cc25-f186-4a54-acfb-43b675fc17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define paths\n",
    "p = Path('..')\n",
    "data_path = Path(pathlib.Path.home().parts[0]) / 'Projects' / 'gmhcira' / 'data' #should contain folder 'Vulnerability' with vulnerability data\n",
    "flood_data_path = Path(pathlib.Path('Z:') / 'eks510' / 'fathom-global') # Flood data\n",
    "#eq_data_path = Path(pathlib.Path('Z:') / 'data_catalogue' / 'open_street_map' / 'global_hazards' / 'earthquakes' / 'GEM') # Earthquake data\n",
    "#eq_data_path = Path(pathlib.Path('Z:') / 'data_catalogue' / 'open_street_map' / 'global_hazards' / 'earthquakes' / 'GAR' / 'raw') #eq data GAR\n",
    "eq_data_path = Path(pathlib.Path.home().parts[0]) / 'Users' / 'snn490' / 'OneDrive - Vrije Universiteit Amsterdam' / 'ADB' / 'Data' / 'Earthquake_data' #eq data provided by ADB\n",
    "landslide_data_path = Path(pathlib.Path('Z:') / 'data_catalogue' / 'open_street_map' / 'global_hazards' / 'landslides') # Landslide data\n",
    "cyclone_data_path = Path(pathlib.Path('Z:') / 'data_catalogue' / 'open_street_map' / 'global_hazards' / 'tropical_cyclones') # Cyclone data\n",
    "liquefaction_data_path = Path(pathlib.Path('Z:') / 'data_catalogue' / 'open_street_map' / 'global_hazards' / 'liquefaction') # Cyclone data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92eb7504-af16-4a75-8e4a-563d67992199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import geopandas as gpd\n",
    "from osgeo import ogr, gdal\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import shapely\n",
    "from tqdm import tqdm\n",
    "\n",
    "from osm_flex.config import DICT_CIS_OSM, OSM_CONFIG_FILE\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "DATA_DIR = '' #TODO: dito, where & how to define\n",
    "gdal.SetConfigOption(\"OSM_CONFIG_FILE\", str(OSM_CONFIG_FILE))\n",
    "\n",
    "\n",
    "def _query_builder(geo_type, constraint_dict):\n",
    "    \"\"\"\n",
    "    This function builds an SQL query from the values passed to the extract()\n",
    "    function.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    geo_type : str\n",
    "        Type of geometry to extract. One of [points, lines, multipolygons]\n",
    "    constraint_dict :  dict\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    query : str\n",
    "        an SQL query string.\n",
    "    \"\"\"\n",
    "    # columns which to report in output\n",
    "    query =  \"SELECT osm_id\"\n",
    "    for key in constraint_dict['osm_keys']:\n",
    "        query+= \",\"+ key\n",
    "    # filter condition(s)\n",
    "    if constraint_dict['osm_query'] is not None:\n",
    "        query+= \" FROM \" + geo_type + \" WHERE \" + constraint_dict['osm_query']\n",
    "    else:\n",
    "        query += \" FROM \" + geo_type + f\" WHERE {constraint_dict['osm_keys'][0]} IS NOT NULL\"\n",
    "    return query\n",
    "\n",
    "def extract(osm_path, geo_type, osm_keys, osm_query=None):\n",
    "    \"\"\"\n",
    "    Function to extract geometries and tag info for entires in the OSM file\n",
    "    matching certain OSM keys, or key-value constraints.\n",
    "    from an OpenStreetMap osm.pbf file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    osm_path : str or Path\n",
    "        location of osm.pbf file from which to parse\n",
    "    geo_type : str\n",
    "        Type of geometry to extract. One of [points, lines, multipolygons]\n",
    "    osm_keys : list\n",
    "        a list with all the osm keys that should be reported as columns in\n",
    "        the output gdf.\n",
    "    osm_query : str\n",
    "        optional. query string of the syntax\n",
    "        \"key='value' (and/or further queries)\". If left empty, all objects\n",
    "        for which the first entry of osm_keys is not Null will be parsed.\n",
    "        See examples in DICT_CIS_OSM in case of doubt.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gpd.GeoDataFrame\n",
    "        A gdf with all results from the osm.pbf file matching the\n",
    "        specified constraints.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    1) The keys that are searchable are specified in the osmconf.ini file.\n",
    "    Make sure that they exist in the attributes=... paragraph under the\n",
    "    respective geometry section.\n",
    "    For example, to extract multipolygons with building='yes',\n",
    "    building must be in the attributes under\n",
    "    the [multipolygons] section of the file. You can find it in the same\n",
    "    folder as the osm_dataloader.py module is located.\n",
    "    2) OSM keys that have : in their name must be changed to _ in the\n",
    "    search dict, but not in the osmconf.ini\n",
    "    E.g. tower:type is called tower_type, since it would interfere with the\n",
    "    SQL syntax otherwise, but still tower:type in the osmconf.ini\n",
    "    3) If the osm_query is left empty (None), then all objects will be parsed\n",
    "    for which the first entry of osm_keys is not Null. E.g. if osm_keys =\n",
    "    ['building', 'name'] and osm_query = None, then all items matching\n",
    "    building=* will be parsed.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    https://taginfo.openstreetmap.org/ to check what keys and key/value\n",
    "    pairs are valid.\n",
    "    https://overpass-turbo.eu/ for a direct visual output of the query,\n",
    "    and to quickly check the validity. The wizard can help you find the\n",
    "    correct keys / values you are looking for.\n",
    "    \"\"\"\n",
    "    if not Path(osm_path).is_file():\n",
    "        raise ValueError(f\"the given path is not a file: {osm_path}\")\n",
    "\n",
    "    osm_path = str(osm_path)\n",
    "    constraint_dict = {\n",
    "        'osm_keys' : osm_keys,\n",
    "        'osm_query' : osm_query}\n",
    "\n",
    "    driver = ogr.GetDriverByName('OSM')\n",
    "    data = driver.Open(osm_path)\n",
    "    query = _query_builder(geo_type, constraint_dict)\n",
    "    LOGGER.debug(\"query: %s\", query)\n",
    "    sql_lyr = data.ExecuteSQL(query)\n",
    "    features = []\n",
    "    geometry = []\n",
    "    if data is not None:\n",
    "        LOGGER.info('query is finished, lets start the loop')\n",
    "        for feature in tqdm(sql_lyr, desc=f'extract {geo_type}'):\n",
    "            try:\n",
    "                wkb = feature.geometry().ExportToWkb()\n",
    "                geom = shapely.wkb.loads(bytes(wkb))\n",
    "                if geom is None:\n",
    "                    continue\n",
    "                geometry.append(geom)\n",
    "                fields = [\n",
    "                    feature.GetField(key)\n",
    "                    for key in [\"osm_id\", *constraint_dict[\"osm_keys\"]]\n",
    "                ]\n",
    "                features.append(fields)\n",
    "            except Exception as exc:\n",
    "                LOGGER.info('%s - %s', exc.__class__, exc)\n",
    "                LOGGER.warning(\"skipped OSM feature\")\n",
    "    else:\n",
    "        LOGGER.error(\"\"\"Nonetype error when requesting SQL. Check the\n",
    "                     query and the OSM config file under the respective\n",
    "                     geometry - perhaps key is unknown.\"\"\")\n",
    "\n",
    "    return gpd.GeoDataFrame(\n",
    "        features,\n",
    "        columns=[\"osm_id\", *constraint_dict['osm_keys']],\n",
    "        geometry=geometry,\n",
    "        crs=\"epsg:4326\"\n",
    "    )\n",
    "\n",
    "# TODO: decide on name of wrapper, which categories included & what components fall under it.\n",
    "def extract_cis(osm_path, ci_type):\n",
    "    \"\"\"\n",
    "    A wrapper around extract() to conveniently extract map info for a\n",
    "    selection of  critical infrastructure types from the given osm.pbf file.\n",
    "    No need to search for osm key/value tags and relevant geometry types.\n",
    "    Parameters\n",
    "    ----------\n",
    "    osm_path : str or Path\n",
    "        location of osm.pbf file from which to parse\n",
    "    ci_type : str\n",
    "        one of DICT_CIS_OSM.keys(), i.e. 'education', 'healthcare',\n",
    "        'water', 'telecom', 'road', 'rail', 'air', 'gas', 'oil', 'power',\n",
    "        'wastewater', 'food'\n",
    "    See also\n",
    "    -------\n",
    "    DICT_CIS_OSM for the keys and key/value tags queried for the respective\n",
    "    CIs. Modify if desired.\n",
    "    \"\"\"\n",
    "    # features consisting in points and multipolygon results:\n",
    "    if ci_type in ['healthcare','education','food','buildings']:\n",
    "        gdf = pd.concat([\n",
    "            extract(osm_path, 'points', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                    DICT_CIS_OSM[ci_type]['osm_query']),\n",
    "            extract(osm_path, 'multipolygons', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                    DICT_CIS_OSM[ci_type]['osm_query'])\n",
    "            ])\n",
    "\n",
    "    # features consisting in points, multipolygons and lines:\n",
    "    elif ci_type in ['gas','oil', 'water','power']:\n",
    "        gdf =  pd.concat([\n",
    "            extract(osm_path, 'points', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                    DICT_CIS_OSM[ci_type]['osm_query']),\n",
    "            extract(osm_path, 'multipolygons', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                             DICT_CIS_OSM[ci_type]['osm_query']),\n",
    "            extract(osm_path, 'lines', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                             DICT_CIS_OSM[ci_type]['osm_query'])\n",
    "            ])\n",
    "\n",
    "    # features consisting in multipolygons and lines:\n",
    "    elif ci_type in ['air']:\n",
    "        gdf =  pd.concat([\n",
    "            extract(osm_path, 'multipolygons', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                             DICT_CIS_OSM[ci_type]['osm_query']),\n",
    "            extract(osm_path, 'lines', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                             DICT_CIS_OSM[ci_type]['osm_query'])\n",
    "            ])\n",
    "    \n",
    "    # features consisting in multiple datattypes, but only lines needed:\n",
    "    elif ci_type in ['rail','road', 'main_road']:\n",
    "        gdf =  pd.concat([\n",
    "            extract(osm_path, 'lines', \n",
    "                    DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                    DICT_CIS_OSM[ci_type]['osm_query'])\n",
    "            ])\n",
    "\n",
    "\n",
    "    # features consisting in all data types, but only points and multipolygon needed:\n",
    "    elif ci_type in ['telecom','wastewater','waste_solid','waste_water','water_supply']:\n",
    "        gdf = pd.concat([\n",
    "            extract(osm_path, 'points', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                    DICT_CIS_OSM[ci_type]['osm_query']),\n",
    "            extract(osm_path, 'multipolygons', DICT_CIS_OSM[ci_type]['osm_keys'],\n",
    "                    DICT_CIS_OSM[ci_type]['osm_query'])\n",
    "            ])\n",
    "        \n",
    "    else:\n",
    "        LOGGER.warning('feature not in DICT_CIS_OSM. Returning empty gdf')\n",
    "        gdf = gpd.GeoDataFrame()\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2402921-488f-4a41-97bd-0eeb6597d93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_CIS_OSM =  {\n",
    "        'power' : {\n",
    "              'osm_keys' : ['power','voltage','name'],\n",
    "              'osm_query' : \"\"\"power='line' or power='cable' or\n",
    "                               power='minor_line' or power='minor_cable' or\n",
    "                               power='plant' or power='generator' or\n",
    "                               power='substation' or power='tower' or\n",
    "                               power='pole' or power='portal'\"\"\"},\n",
    "        'road_gmhcira' :  {\n",
    "            'osm_keys' : ['highway','name','maxspeed','lanes','surface'],\n",
    "            'osm_query' : \"\"\"highway in ('motorway', 'motorway_link', 'motorway_junction', 'trunk', 'trunk_link',\n",
    "                            'primary', 'primary_link', 'secondary', 'secondary_link', 'tertiary', 'tertiary_link', \n",
    "                            'residential', 'road', 'unclassified', 'living_street', 'pedestrian', 'bus_guideway', 'escape', 'raceway', \n",
    "                            'cycleway', 'construction', 'bus_stop', 'crossing', 'mini_roundabout', 'passing_place', 'rest_area', \n",
    "                            'turning_circle', 'traffic_island', 'yes', 'emergency_bay', 'service', 'track')\"\"\"},\n",
    "        'road' :  {\n",
    "            'osm_keys' : ['highway','name','maxspeed','lanes','surface'],\n",
    "            'osm_query' : \"\"\"highway in ('motorway', 'motorway_link', 'trunk', 'trunk_link',\n",
    "                            'primary', 'primary_link', 'secondary', 'secondary_link', 'tertiary', 'tertiary_link', \n",
    "                            'residential', 'road', 'unclassified', 'track')\"\"\"},\n",
    "        'rail' : {\n",
    "            'osm_keys' : ['railway','name','gauge','electrified','voltage'],\n",
    "            'osm_query' : \"\"\"railway='rail' or railway='narrow_gauge'\"\"\"},\n",
    "         'air' : {\n",
    "             'osm_keys' : ['aeroway','name'],\n",
    "             'osm_query' : \"\"\"aeroway='aerodrome' or aeroway='terminal' or aeroway='runway'\"\"\"}, \n",
    "        'telecom' : {\n",
    "            'osm_keys' : ['man_made','tower_type','name'],\n",
    "            'osm_query' : \"\"\"tower_type='communication' or man_made='mast' or man_made='communications_tower'\"\"\"},\n",
    "        'water_supply' : {\n",
    "            'osm_keys' : ['man_made','name'],\n",
    "            'osm_query' : \"\"\"man_made='water_well' or man_made='water_works' or\n",
    "                             man_made='water_tower' or\n",
    "                             man_made='reservoir_covered' or\n",
    "                             (man_made='storage_tank' and content='water')\"\"\"},\n",
    "        'waste_solid' : {\n",
    "              'osm_keys' : ['amenity','name'],\n",
    "              'osm_query' : \"\"\"amenity='waste_transfer_station'\"\"\"},\n",
    "        'waste_water' : {\n",
    "              'osm_keys' : ['man_made','name'],\n",
    "              'osm_query' : \"\"\"man_made='wastewater_plant'\"\"\"},\n",
    "        'education' : {\n",
    "            'osm_keys' : ['amenity','building','name'],\n",
    "            'osm_query' : \"\"\"building='school' or amenity='school' or\n",
    "                             building='kindergarten' or \n",
    "                             amenity='kindergarten' or\n",
    "                             building='college' or amenity='college' or\n",
    "                             building='university' or amenity='university' or\n",
    "                             building='library' or amenity='library'\"\"\"},\n",
    "        'healthcare' : {\n",
    "            'osm_keys' : ['amenity','building','healthcare','name'],\n",
    "            'osm_query' : \"\"\"amenity='hospital' or healthcare='hospital' or\n",
    "                             building='hospital' or building='clinic' or\n",
    "                             amenity='clinic' or healthcare='clinic' or \n",
    "                             amenity='doctors' or healthcare='doctors' or\n",
    "                             amenity='dentist' or amenity='pharmacy' or \n",
    "                             healthcare='pharmacy' or healthcare='dentist' or\n",
    "                             healthcare='physiotherapist' or healthcare='alternative' or \n",
    "                             healthcare='laboratory' or healthcare='optometrist' or \n",
    "                             healthcare='rehabilitation' or healthcare='blood_donation' or\n",
    "                             healthcare='birthing_center'\n",
    "                             \"\"\"},\n",
    "        'power_original' : {\n",
    "              'osm_keys' : ['power','voltage','utility','name'],\n",
    "              'osm_query' : \"\"\"power='line' or power='cable' or\n",
    "                               power='minor_line' or power='plant' or\n",
    "                               power='generator' or power='substation' or\n",
    "                               power='transformer' or\n",
    "                               power='pole' or power='portal' or \n",
    "                               power='tower' or power='terminal' or \n",
    "                               power='switch' or power='catenary_mast' or\n",
    "                               utility='power'\"\"\"},\n",
    "         'gas' : {\n",
    "             'osm_keys' : ['man_made','pipeline', 'utility','name'],\n",
    "             'osm_query' : \"\"\"(man_made='pipeline' and substance='gas') or\n",
    "                              (pipeline='substation' and substance='gas') or\n",
    "                              (man_made='storage_tank' and content='gas') or\n",
    "                              utility='gas'\"\"\"},\n",
    "        'oil' : {\n",
    "             'osm_keys' : ['pipeline','man_made','amenity','name'],\n",
    "             'osm_query' : \"\"\"(pipeline='substation' and substance='oil') or\n",
    "                              (man_made='pipeline' and substance='oil') or\n",
    "                              man_made='petroleum_well' or \n",
    "                              man_made='oil_refinery' or\n",
    "                              amenity='fuel'\"\"\"},\n",
    "        'main_road' :  {\n",
    "            'osm_keys' : ['highway','name','maxspeed','lanes','surface'],\n",
    "            'osm_query' : \"\"\"highway in ('primary', 'primary_link', 'secondary',\n",
    "                             'secondary_link', 'tertiary', 'tertiary_link', 'trunk', 'trunk_link', \n",
    "                             'motorway', 'motorway_link')\n",
    "                            \"\"\"},\n",
    "        'wastewater' : {\n",
    "              'osm_keys' : ['man_made','amenity',\n",
    "                            'name'],\n",
    "              'osm_query' : \"\"\"amenity='waste_transfer_station' or man_made='wastewater_plant'\"\"\"},\n",
    "         'food' : {\n",
    "             'osm_keys' : ['shop','name'],\n",
    "             'osm_query' : \"\"\"shop='supermarket' or shop='greengrocer' or\n",
    "                              shop='grocery' or shop='general' or \n",
    "                              shop='bakery'\"\"\"},             \n",
    "        'buildings' : {\n",
    "            'osm_keys' : ['building','amenity','name'],\n",
    "            'osm_query' : \"\"\"building='yes' or building='house' or \n",
    "                            building='residential' or building='detached' or \n",
    "                            building='hut' or building='industrial' or \n",
    "                            building='shed' or building='apartments'\"\"\"}\n",
    "                              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "794dd7c4-3c68-44eb-b6ab-db5627bab45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_download(iso3):\n",
    "    \"\"\"\n",
    "    Download OpenStreetMap data for a specific country.\n",
    "    Arguments:\n",
    "        *iso3* (str): ISO 3166-1 alpha-3 country code.\n",
    "    Returns:\n",
    "        *Path*: The file path of the downloaded OpenStreetMap data file.\n",
    "    \"\"\"\n",
    "    \n",
    "    dl.get_country_geofabrik(iso3) # Use the download library to get the geofabrik data for the specified country\n",
    "    data_loc = OSM_DATA_DIR.joinpath(f'{DICT_GEOFABRIK[iso3][1]}-latest.osm.pbf') # Specify the location of the OpenStreetMap (OSM) data file\n",
    "    return data_loc\n",
    "\n",
    "def overlay_hazard_assets(df_ds,assets):\n",
    "    \"\"\"\n",
    "    Overlay hazard assets on a dataframe of spatial geometries.\n",
    "    Arguments:\n",
    "        *df_ds*: GeoDataFrame containing the spatial geometries of the hazard data. \n",
    "        *assets*: GeoDataFrame containing the infrastructure assets.\n",
    "    Returns:\n",
    "        *geopandas.GeoSeries*: A GeoSeries containing the spatial geometries of df_ds that intersect with the infrastructure assets.\n",
    "    \"\"\"\n",
    "    \n",
    "    #overlay \n",
    "    hazard_tree = shapely.STRtree(df_ds.geometry.values)\n",
    "    if (shapely.get_type_id(assets.iloc[0].geometry) == 3) | (shapely.get_type_id(assets.iloc[0].geometry) == 6): # id types 3 and 6 stand for polygon and multipolygon\n",
    "        return  hazard_tree.query(assets.geometry,predicate='intersects')    \n",
    "    else:\n",
    "        return  hazard_tree.query(assets.buffered,predicate='intersects')\n",
    "\n",
    "def buffer_assets(assets,buffer_size=0.00083):\n",
    "    \"\"\"\n",
    "    Buffer spatial assets in a GeoDataFrame.\n",
    "    Arguments:\n",
    "        *assets*: GeoDataFrame containing spatial geometries to be buffered.\n",
    "        *buffer_size* (float, optional): The distance by which to buffer the geometries. Default is 0.00083.\n",
    "    Returns:\n",
    "        *GeoDataFrame*: A new GeoDataFrame with an additional 'buffered' column containing the buffered geometries.\n",
    "    \"\"\"\n",
    "    assets['buffered'] = shapely.buffer(assets.geometry.values,distance=buffer_size)\n",
    "    return assets\n",
    "\n",
    "def get_damage_per_asset(asset,hazard_numpified,asset_geom,hazard_intensity,fragility_values,maxdam_asset,unit_maxdam):\n",
    "    \"\"\"\n",
    "    Calculate damage for a given asset based on hazard information.\n",
    "    Arguments:\n",
    "        *asset*: Tuple containing information about the asset. It includes:\n",
    "            - Index or identifier of the asset (asset[0]).\n",
    "            - The specific hazard points in which asset is exposed (asset[1]['hazard_point']).\n",
    "        *hazard_numpified*: NumPy array representing hazard information.\n",
    "        *asset_geom*: Shapely geometry representing the spatial coordinates of the asset.\n",
    "        *hazard_intensity*: NumPy array representing the hazard intensities of the curve for the asset type.\n",
    "        *fragility_values*: NumPy array representing the damage factors of the curve for the asset type.\n",
    "        *maxdam_asset*: Maximum damage value for asset.\n",
    "        *unit_maxdam*: The unit of maximum damage value for asset.\n",
    "    Returns:\n",
    "        *float*: The calculated damage for the specific asset.\n",
    "    \"\"\"\n",
    "     \n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = hazard_numpified[asset[1]['hazard_point'].values] \n",
    "    get_hazard_points[shapely.intersects(get_hazard_points[:,1],asset_geom)]\n",
    "    if type(maxdam_asset) == str: maxdam_asset = float(maxdam_asset)\n",
    "\n",
    "    # estimate damage\n",
    "    if len(get_hazard_points) == 0: # no overlay of asset with hazard\n",
    "        return 0\n",
    "    else:\n",
    "        if asset_geom.geom_type == 'LineString':\n",
    "            overlay_meters = shapely.length(shapely.intersection(get_hazard_points[:,1],asset_geom)) # get the length of exposed meters per hazard cell\n",
    "            return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_meters*maxdam_asset) #return asset number, total damage for asset number (damage factor * meters * max. damage)\n",
    "        elif asset_geom.geom_type in ['MultiPolygon','Polygon']:\n",
    "            overlay_m2 = shapely.area(shapely.intersection(get_hazard_points[:,1],asset_geom))\n",
    "            if '/unit' in unit_maxdam:\n",
    "                converted_maxdam = maxdam_asset / shapely.area(asset_geom) #convert to maxdam/m2\n",
    "                return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_m2*converted_maxdam)\n",
    "            else:\n",
    "                return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_m2*maxdam_asset) #return asset number, total damage for asset number (damage factor * meters * max. damage)\n",
    "        elif asset_geom.geom_type == 'Point':\n",
    "            return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*maxdam_asset)\n",
    "\n",
    "def get_damage_per_asset_and_overlay(asset,hazard_numpified,asset_geom,hazard_intensity,fragility_values,maxdam_asset,unit_maxdam):\n",
    "    \"\"\"\n",
    "    Calculate damage for a given asset based on hazard information.\n",
    "    Arguments:\n",
    "        *asset*: Tuple containing information about the asset. It includes:\n",
    "            - Index or identifier of the asset (asset[0]).\n",
    "            - The specific hazard points in which asset is exposed (asset[1]['hazard_point']).\n",
    "        *hazard_numpified*: NumPy array representing hazard information.\n",
    "        *asset_geom*: Shapely geometry representing the spatial coordinates of the asset.\n",
    "        *hazard_intensity*: NumPy array representing the hazard intensities of the curve for the asset type.\n",
    "        *fragility_values*: NumPy array representing the damage factors of the curve for the asset type.\n",
    "        *maxdam_asset*: Maximum damage value for asset.\n",
    "        *unit_maxdam*: The unit of maximum damage value for asset.\n",
    "    Returns:\n",
    "        *float*: The calculated damage for the specific asset.\n",
    "    \"\"\"\n",
    "     \n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = hazard_numpified[asset[1]['hazard_point'].values] \n",
    "    get_hazard_points[shapely.intersects(get_hazard_points[:,1],asset_geom)]\n",
    "    if type(maxdam_asset) == str: maxdam_asset = float(maxdam_asset)\n",
    "\n",
    "    # estimate damage\n",
    "    if len(get_hazard_points) == 0: # no overlay of asset with hazard\n",
    "        return 0\n",
    "    else:\n",
    "        if asset_geom.geom_type == 'LineString':\n",
    "            overlay_meters = shapely.length(shapely.intersection(get_hazard_points[:,1],asset_geom)) # get the length of exposed meters per hazard cell\n",
    "            return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_meters*maxdam_asset), np.sum(overlay_meters) #return asset number, total damage for asset number (damage factor * meters * max. damage)\n",
    "        elif asset_geom.geom_type in ['MultiPolygon','Polygon']:\n",
    "            overlay_m2 = shapely.area(shapely.intersection(get_hazard_points[:,1],asset_geom))\n",
    "            if '/unit' in unit_maxdam:\n",
    "                converted_maxdam = maxdam_asset / shapely.area(asset_geom) #convert to maxdam/m2\n",
    "                return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_m2*converted_maxdam)\n",
    "            else:\n",
    "                return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_m2*maxdam_asset) #return asset number, total damage for asset number (damage factor * meters * max. damage)\n",
    "        elif asset_geom.geom_type == 'Point':\n",
    "            return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*maxdam_asset)\n",
    "\n",
    "def create_pathway_dict(data_path, flood_data_path, eq_data_path, landslide_data_path, cyclone_data_path): \n",
    "\n",
    "    \"\"\"\n",
    "    Create a dictionary containing paths to various hazard datasets.\n",
    "    Arguments:\n",
    "        *data_path* (Path): Base directory path for general data.\n",
    "        *flood_data_path* (Path): Path to flood hazard data.\n",
    "        *eq_data_path* (Path): Path to earthquake hazard data.\n",
    "        *landslide_data_path* (Path): Path to landslide hazard data.\n",
    "        *cyclone_data_path* (Path): Path to tropical cyclone hazard data.\n",
    "    Returns:\n",
    "        *dict*: A dictionary where keys represent a general pathway and different hazard types and values are corresponding paths.\n",
    "    \"\"\"\n",
    "\n",
    "    #create a dictionary\n",
    "    pathway_dict = {'data_path': data_path, \n",
    "                    'fluvial': flood_data_path, \n",
    "                    'pluvial': flood_data_path, \n",
    "                    'windstorm': cyclone_data_path, \n",
    "                    'earthquake': eq_data_path, \n",
    "                    'landslide_rf': landslide_data_path,\n",
    "                    'landslide_eq': landslide_data_path,}\n",
    "\n",
    "    return pathway_dict\n",
    "\n",
    "def read_hazard_data(hazard_data_path,data_path,hazard_type,ISO3):\n",
    "    \"\"\"\n",
    "    Read hazard data files for a specific hazard type.\n",
    "    Arguments:\n",
    "        *hazard_data_path* (Path): Base directory path where hazard data is stored.\n",
    "        *hazard_type* (str): Type of hazard for which data needs to be read ('fluvial', 'pluvial', 'windstorm', 'earthquake', 'landslide').\n",
    "    \n",
    "    Returns:\n",
    "        *list*: A list of Path objects representing individual hazard data files for the specified hazard type.\n",
    "    \"\"\"  \n",
    "\n",
    "    country_df = pd.read_excel(data_path / 'global_information_advanced_fathom_check.xlsx',sheet_name = 'Sheet1') # finalize this file and adjust name\n",
    "    fathom_code = country_df.loc[country_df['ISO_3digit'] == country_code, 'Fathom_countries'].item()\n",
    "\n",
    "    if hazard_type == 'fluvial':\n",
    "        hazard_data = hazard_data_path / fathom_code / 'fluvial_undefended' \n",
    "        return list(hazard_data.iterdir())\n",
    "\n",
    "    elif hazard_type == 'pluvial':\n",
    "        hazard_data = hazard_data_path / fathom_code / 'pluvial' \n",
    "        return list(hazard_data.iterdir())\n",
    "    \n",
    "    elif hazard_type == 'windstorm':\n",
    "        hazard_data = hazard_data_path \n",
    "        return list(hazard_data.iterdir())\n",
    "\n",
    "    elif hazard_type == 'earthquake':\n",
    "        if 'GAR' in str(hazard_data_path):\n",
    "            data_lst = []\n",
    "            hazard_data = hazard_data_path\n",
    "            rp_lst = list(hazard_data.iterdir())\n",
    "            for rp_folder in rp_lst:\n",
    "                temp_lst = [file for file in rp_folder.iterdir() if file.suffix == '.tif']\n",
    "                data_lst.extend(temp_lst)  # Use extend instead of append to flatten the list\n",
    "            return data_lst\n",
    "        \n",
    "        elif 'ADB' in str(eq_data_path):\n",
    "            hazard_data = eq_data_path \n",
    "            return [file for file in hazard_data.iterdir() if file.suffix == '.tif']\n",
    "            \n",
    "        elif 'GEM' in str(hazard_data_path):\n",
    "            hazard_data = hazard_data_path\n",
    "            data_lst = list(hazard_data.iterdir())\n",
    "            data_lst = [file for file in data_lst if file.suffix == '.csv']\n",
    "            return data_lst\n",
    "\n",
    "    #elif hazard_type == 'earthquake':\n",
    "    #    hazard_data = hazard_data_path\n",
    "    #    return list(hazard_data.iterdir())\n",
    "\n",
    "    elif hazard_type == 'landslide_rf':\n",
    "        hazard_data = hazard_data_path / 'rainfall' / '{}_l24-norm-hist.tif'.format(ISO3)\n",
    "        return [hazard_data]\n",
    "\n",
    "    elif hazard_type == 'landslide_eq':\n",
    "        hazard_data = hazard_data_path.parent / 'earthquakes' / 'GEM' / 'GEM-GSHM_PGA-475y-rock_v2023' / 'v2023_1_pga_475_rock_3min.tif' #use only one rp for the triggering conditions\n",
    "        #hazard_data = hazard_data_path.parent / 'earthquakes' / 'GAR' / 'raw' / 'rp_475'/ 'gar17pga475.tif' #use only one rp for the triggering conditions\n",
    "        return [hazard_data]\n",
    "\n",
    "def read_vul_maxdam_orginal(data_path,hazard_type,infra_type):\n",
    "    \"\"\"\n",
    "    Read vulnerability curves and maximum damage data for a specific hazard and infrastructure type.\n",
    "    Arguments:\n",
    "        *data_path*: The base directory path where vulnerability and maximum damage data files are stored.\n",
    "        *hazard_type*: The type of hazard in string format, such as 'pluvial', 'fluvial', or 'windstorm'.\n",
    "        *infra_type*: The type of infrastructure in string format for which vulnerability curves and maximum damage data are needed.\n",
    "    \n",
    "    Returns:\n",
    "        *tuple*: A tuple containing two DataFrames:\n",
    "            - The first DataFrame contains vulnerability curves specific to the given hazard and infrastructure type.\n",
    "            - The second DataFrame contains maximum damage data for the specified infrastructure type.\n",
    "    \"\"\"\n",
    "\n",
    "    vul_data = data_path / 'Vulnerability'\n",
    "    \n",
    "    # Load assumptions file containing curve - maxdam combinations per infrastructure type\n",
    "    assumptions = pd.read_excel(vul_data / 'S1_Assumptions_Test.xlsx',sheet_name = 'Flooding assumptions',header=[1])\n",
    "    assumptions['Infrastructure type'] = assumptions['Infrastructure type'].str.lower()\n",
    "    if \"_\" in infra_type: infra_type = infra_type.replace('_', ' ')\n",
    "    assump_infra_type = assumptions[assumptions['Infrastructure type'] == infra_type]\n",
    "    assump_curves = ast.literal_eval(assump_infra_type['Vulnerability ID number'].item())\n",
    "    assump_maxdams = ast.literal_eval(assump_infra_type['Maximum damage ID number'].item())\n",
    "    \n",
    "    # Get curves\n",
    "    if hazard_type in ['pluvial','fluvial']:  \n",
    "        curves = pd.read_excel(vul_data / 'Table_D2_Hazard_Fragility_and_Vulnerability_Curves_V1.1.0_converted.xlsx',sheet_name = 'F_Vuln_Depth',index_col=[0],header=[0,1,2,3,4])\n",
    "    elif hazard_type == 'windstorm':\n",
    "        curves = pd.read_excel(vul_data / 'Table_D2_Hazard_Fragility_and_Vulnerability_Curves_V1.1.0_converted.xlsx',sheet_name = 'W_Vuln_V10m',index_col=[0],header=[0,1,2,3,4])\n",
    "    \n",
    "    infra_curves =  curves[assump_curves]\n",
    "    \n",
    "    # get maxdam\n",
    "    maxdam = pd.read_excel(vul_data / 'Table_D3_Costs_V1.1.0_converted.xlsx', sheet_name='Cost_Database',index_col=[0])\n",
    "    infra_maxdam = maxdam[maxdam.index.isin(assump_maxdams)]['Amount'].dropna()\n",
    "    infra_maxdam = infra_maxdam[pd.to_numeric(infra_maxdam, errors='coerce').notnull()]\n",
    "\n",
    "    return infra_curves,infra_maxdam\n",
    "\n",
    "def read_vul_maxdam_old(data_path,hazard_type,infra_type):\n",
    "    \"\"\"\n",
    "    Read vulnerability curves and maximum damage data for a specific hazard and infrastructure type.\n",
    "    Arguments:\n",
    "        *data_path*: The base directory path where vulnerability and maximum damage data files are stored.\n",
    "        *hazard_type*: The type of hazard in string format, such as 'pluvial', 'fluvial', or 'windstorm'.\n",
    "        *infra_type*: The type of infrastructure in string format for which vulnerability curves and maximum damage data are needed.\n",
    "    \n",
    "    Returns:\n",
    "        *tuple*: A tuple containing two DataFrames:\n",
    "            - The first DataFrame contains vulnerability curves specific to the given hazard and infrastructure type.\n",
    "            - The second DataFrame contains maximum damage data for the specified infrastructure type.\n",
    "    \"\"\"\n",
    "\n",
    "    vul_data = data_path / 'Vulnerability'\n",
    "    \n",
    "    # Load assumptions file containing curve - maxdam combinations per infrastructure type\n",
    "    assumptions = pd.read_excel(vul_data / 'S1_Assumptions_Test.xlsx',sheet_name = 'Flooding assumptions',header=[1])\n",
    "    assumptions['Infrastructure type'] = assumptions['Infrastructure type'].str.lower()\n",
    "    if \"_\" in infra_type: infra_type = infra_type.replace('_', ' ')\n",
    "    assump_infra_type = assumptions[assumptions['Infrastructure type'] == infra_type]\n",
    "    assump_curves = ast.literal_eval(assump_infra_type['Vulnerability ID number'].item())\n",
    "    assump_maxdams = ast.literal_eval(assump_infra_type['Maximum damage ID number'].item())\n",
    "    \n",
    "    # Get curves\n",
    "    if hazard_type in ['pluvial','fluvial']:  \n",
    "        curves = pd.read_excel(vul_data / 'Table_D2_Hazard_Fragility_and_Vulnerability_Curves_V1.1.0.xlsx',sheet_name = 'F_Vuln_Depth',index_col=[0],header=[0,1,2,3,4])\n",
    "    elif hazard_type == 'windstorm':\n",
    "        curves = pd.read_excel(vul_data / 'Table_D2_Hazard_Fragility_and_Vulnerability_Curves_V1.1.0.xlsx',sheet_name = 'W_Vuln_V10m',index_col=[0],header=[0,1,2,3,4])\n",
    "    \n",
    "    infra_curves =  curves[assump_curves]\n",
    "    \n",
    "    # get maxdam\n",
    "    maxdam = pd.read_excel(vul_data / 'Table_D3_Costs_V1.1.0_converted.xlsx', sheet_name='Cost_Database',index_col=[0])\n",
    "    infra_costs = maxdam[maxdam.index.isin(assump_maxdams)][['Amount', 'Unit']].dropna(subset=['Amount'])\n",
    "    infra_maxdam = infra_costs['Amount'][pd.to_numeric(infra_costs['Amount'], errors='coerce').notnull()]\n",
    "    infra_units = infra_costs['Unit'].filter(items=list(infra_maxdam.index), axis=0)\n",
    "\n",
    "    return infra_curves,infra_maxdam,infra_units\n",
    "\n",
    "def read_flood_map(flood_map_path,diameter_distance=0.00083/2):\n",
    "    \"\"\"\n",
    "    Read flood map data from a NetCDF file and process it into a GeoDataFrame.\n",
    "    Arguments:\n",
    "        *flood_map_path* (Path): Path to the NetCDF file containing flood map data.\n",
    "        *diameter_distance* (float, optional): The diameter distance used for creating square geometries around data points. Default is 0.00083/2.\n",
    "    \n",
    "    Returns:\n",
    "        *geopandas.GeoDataFrame*: A GeoDataFrame representing the processed flood map data.\n",
    "    \"\"\"\n",
    "    \n",
    "    flood_map = xr.open_dataset(flood_map_path, engine=\"rasterio\")\n",
    "\n",
    "    flood_map_vector = flood_map['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "    \n",
    "    #remove data that will not be used\n",
    "    flood_map_vector = flood_map_vector.loc[(flood_map_vector.band_data > 0) & (flood_map_vector.band_data < 100)]\n",
    "    \n",
    "    # create geometry values and drop lat lon columns\n",
    "    flood_map_vector['geometry'] = [shapely.points(x) for x in list(zip(flood_map_vector['x'],flood_map_vector['y']))]\n",
    "    flood_map_vector = flood_map_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "    \n",
    "    # drop all non values to reduce size\n",
    "    flood_map_vector = flood_map_vector.loc[~flood_map_vector['band_data'].isna()].reset_index(drop=True)\n",
    "    \n",
    "    # and turn them into squares again:\n",
    "    flood_map_vector.geometry= shapely.buffer(flood_map_vector.geometry,distance=diameter_distance,cap_style='square').values \n",
    "\n",
    "    return flood_map_vector\n",
    "\n",
    "def read_windstorm_map(windstorm_map_path,bbox):\n",
    "     \n",
    "    # load data from NetCDF file\n",
    "    with xr.open_dataset(flood_map_path) as ds:\n",
    "        \n",
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        ds = ds.rio.clip_box(minx=bbox[0], miny=bbox[1], maxx=bbox[2], maxy=bbox[3])\n",
    "        #ds['band_data'] = ds['band_data']/0.88*1.11 #convert 10-min sustained wind speed to 3-s gust wind speed\n",
    "    \n",
    "        ds_vector = ds['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "        \n",
    "        #remove data that will not be used\n",
    "        ds_vector = ds_vector.loc[(ds_vector.band_data > 0) & (ds_vector.band_data < 100)]\n",
    "        \n",
    "        # create geometry values and drop lat lon columns\n",
    "        ds_vector['geometry'] = [shapely.points(x) for x in list(zip(ds_vector['x'],ds_vector['y']))]\n",
    "        ds_vector = ds_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "        ds_vector['geometry'] = shapely.buffer(ds_vector.geometry, distance=0.1/2, cap_style='square').values\n",
    "    \n",
    "        return ds_vector\n",
    "\n",
    "def read_gar_earthquake_map(earthquake_map_path,bbox,diameter_distance=0.004999972912597225316/2): #0.07201440288057610328 for own download\n",
    "     \n",
    "    # load data from NetCDF file\n",
    "    with xr.open_dataset(earthquake_map_path) as ds:\n",
    "        \n",
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        ds = ds.rio.clip_box(minx=bbox[0], miny=bbox[1], maxx=bbox[2], maxy=bbox[3])\n",
    "\n",
    "        ds_vector = ds['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "\n",
    "        #remove data that will not be used\n",
    "        ds_vector['band_data'] = ds_vector['band_data']/980\n",
    "        ds_vector = ds_vector.loc[(ds_vector.band_data > 0) & (ds_vector.band_data <= 10)]\n",
    "\n",
    "        # create geometry values and drop lat lon columns\n",
    "        ds_vector['geometry'] = [shapely.points(x) for x in list(zip(ds_vector['x'],ds_vector['y']))]\n",
    "        ds_vector = ds_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "\n",
    "        ds_vector['geometry'] = shapely.buffer(ds_vector.geometry, distance=diameter_distance, cap_style='square').values\n",
    "\n",
    "        return ds_vector\n",
    "\n",
    "def read_earthquake_map(earthquake_map_path,bbox,diameter_distance=0.05000000000000000278/2):\n",
    "     \n",
    "    # load data from NetCDF file\n",
    "    with xr.open_dataset(earthquake_map_path) as ds:\n",
    "        \n",
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        ds = ds.rio.clip_box(minx=bbox[0], miny=bbox[1], maxx=bbox[2], maxy=bbox[3])\n",
    "        \n",
    "        ds_vector = ds['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "        \n",
    "        #remove data that will not be used\n",
    "        ds_vector = ds_vector.loc[(ds_vector.band_data > 0) & (ds_vector.band_data <= 10)]\n",
    "        \n",
    "        # create geometry values and drop lat lon columns\n",
    "        ds_vector['geometry'] = [shapely.points(x) for x in list(zip(ds_vector['x'],ds_vector['y']))]\n",
    "        ds_vector = ds_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "\n",
    "        ds_vector['geometry'] = shapely.buffer(ds_vector.geometry, distance=diameter_distance, cap_style='square').values\n",
    "\n",
    "        return ds_vector\n",
    "\n",
    "def h3_to_polygon(h3_index):\n",
    "    # Get the boundary of the hexagon in (lat, lon) pairs\n",
    "    boundary = h3.h3_to_geo_boundary(h3_index)\n",
    "    # Convert to (lon, lat) pairs and create a Polygon\n",
    "    return shapely.Polygon([(lon, lat) for lat, lon in boundary])\n",
    "\n",
    "def overlay_hazard_bbox(df_ds,bbox_geometries):\n",
    "    \"\"\"\n",
    "    Overlay hazard assets on a dataframe of spatial geometries.\n",
    "    Arguments:\n",
    "        *df_ds*: GeoDataFrame containing the spatial geometries of the hazard data. \n",
    "        *boundary*: GeoDataFrame containing the infrastructure assets.\n",
    "    Returns:\n",
    "        *geopandas.GeoSeries*: A GeoSeries containing the spatial geometries of df_ds that intersect with the administrative boundary.\n",
    "    \"\"\"\n",
    "    bbox_polygon = shapely.box(*bbox) #create polygon using bbox coordinates\n",
    "    \n",
    "    #overlay \n",
    "    hazard_tree = shapely.STRtree(df_ds.geometry.values)\n",
    "    intersect_index = hazard_tree.query(bbox_polygon,predicate='intersects')\n",
    "    \n",
    "    return df_ds.iloc[intersect_index].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def read_earthquake_map_csv(earthquake_map_path,bbox):\n",
    "    #using h3 geometries: https://pypi.org/project/h3/\n",
    "    #example Notebooks: https://github.com/uber/h3-py-notebooks\n",
    "    #more info: https://h3geo.org/docs/quickstart\n",
    "     \n",
    "    ds_vector = pd.read_csv(earthquake_map_path)\n",
    "    for col in ds_vector.columns: \n",
    "        if col not in ['lon', 'lat']: ds_vector = ds_vector.rename(columns={col:'band_data'})\n",
    "    \n",
    "    #remove data that will not be used\n",
    "    ds_vector = ds_vector.loc[(ds_vector.band_data > 0) & (ds_vector.band_data <= 10)]\n",
    "\n",
    "    #create geometry values and drop lat lon columns\n",
    "    ds_vector['geometry'] = [shapely.points(x) for x in list(zip(ds_vector['lon'],ds_vector['lat']))]\n",
    "    \n",
    "    #overlay with bbox\n",
    "    ds_vector = overlay_hazard_bbox(ds_vector,bbox)\n",
    "    ds_vector = ds_vector.drop(['geometry'],axis=1)\n",
    "\n",
    "    #transform to h3 hexagons\n",
    "    ds_vector['h3_codes'] = ds_vector.apply(lambda row: h3.geo_to_h3(row['lat'], row['lon'], 6), axis=1) #get h3 code\n",
    "    ds_vector['geometry'] = ds_vector.apply(lambda row: h3_to_polygon(row['h3_codes']), axis=1) #get hexagon geometries\n",
    "\n",
    "    #drop columns\n",
    "    ds_vector = ds_vector.drop(['lon','lat','h3_codes'],axis=1)\n",
    "\n",
    "    return ds_vector\n",
    "\n",
    "def read_landslide_map(landslide_map_path,bbox,diameter_distance=0.008333333333325620637/2):\n",
    "     \n",
    "    # load data from NetCDF file\n",
    "    with xr.open_dataset(landslide_map_path) as ds:\n",
    "        \n",
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        ds = ds.rio.clip_box(minx=bbox[0], miny=bbox[1], maxx=bbox[2], maxy=bbox[3])\n",
    "        \n",
    "        ds_vector = ds['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "        \n",
    "        #remove data that will not be used\n",
    "        ds_vector = ds_vector.loc[(ds_vector.band_data > 0) & (ds_vector.band_data <= 1)]\n",
    "        \n",
    "        # create geometry values and drop lat lon columns\n",
    "        ds_vector['geometry'] = [shapely.points(x) for x in list(zip(ds_vector['x'],ds_vector['y']))]\n",
    "        ds_vector = ds_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "\n",
    "        ds_vector['geometry'] = shapely.buffer(ds_vector.geometry, distance=diameter_distance, cap_style='square').values\n",
    "\n",
    "        return ds_vector\n",
    "\n",
    "def read_susceptibility_map(landslide_map_path, hazard_type, bbox,diameter_distance=0.0008333333333333522519/2):\n",
    "\n",
    "    if hazard_type in ['landslide_eq']:\n",
    "         susc_footprint = pathlib.Path(landslide_map_path).parent.parent / 'susceptibility_giri' / 'susc_earthquake_trig_cdri.tif'\n",
    "    elif hazard_type in ['landslide_rf']:\n",
    "         susc_footprint = pathlib.Path(landslide_map_path).parent.parent / 'susceptibility_giri' / 'susc_prec_trig_cdri.tif'\n",
    "     \n",
    "    # load data from NetCDF file\n",
    "    with xr.open_dataset(susc_footprint) as ds:\n",
    "        \n",
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        ds = ds.rio.clip_box(minx=bbox[0], miny=bbox[1], maxx=bbox[2], maxy=bbox[3])\n",
    "        \n",
    "        ds_vector = ds['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "        \n",
    "        #remove data that will not be used\n",
    "        ds_vector = ds_vector.loc[(ds_vector.band_data > 0) & (ds_vector.band_data <= 5)]\n",
    "        \n",
    "        # create geometry values and drop lat lon columns\n",
    "        ds_vector['geometry'] = [shapely.points(x) for x in list(zip(ds_vector['x'],ds_vector['y']))]\n",
    "        ds_vector = ds_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "\n",
    "        ds_vector['geometry'] = shapely.buffer(ds_vector.geometry, distance=diameter_distance, cap_style='square').values\n",
    "\n",
    "        return ds_vector\n",
    "\n",
    "def read_susceptibility_map_cropped(susc_path, diameter_distance=0.0008333333333333522519/2):\n",
    "     \n",
    "    # load data from NetCDF file\n",
    "    with xr.open_dataset(susc_path) as ds:\n",
    "        \n",
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        \n",
    "        ds_vector = ds['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "        \n",
    "        #remove data that will not be used\n",
    "        ds_vector = ds_vector.loc[(ds_vector.band_data > 1) & (ds_vector.band_data <= 5)] #also omit class 1 in this early phase, because won't be needed anyway following table in GIRI report\n",
    "        \n",
    "        # create geometry values and drop lat lon columns\n",
    "        ds_vector['geometry'] = [shapely.points(x) for x in list(zip(ds_vector['x'],ds_vector['y']))]\n",
    "        ds_vector = ds_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "\n",
    "        #ds_vector['geometry'] = shapely.buffer(ds_vector.geometry, distance=diameter_distance, cap_style='square').values\n",
    "\n",
    "        return ds_vector\n",
    "\n",
    "def combine_columns(a, b):\n",
    "    \"\"\"\n",
    "    Combine values from two input arguments 'a' and 'b' into a single string.\n",
    "    Arguments:\n",
    "    - a (str or None): Value from column 'A'.\n",
    "    - b (str or None): Value from column 'B'.\n",
    "\n",
    "    Returns:\n",
    "    - str or None: A string of 'a', 'b' or combination. If both 'a' and 'b' are None, return None.\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.notna(a) and pd.notna(b) == False: #if only a contains a string\n",
    "        return f\"{a}\" \n",
    "    elif pd.notna(b) and pd.notna(a) == False: #if only b contains a string\n",
    "        return f\"{b}\"\n",
    "    elif pd.notna(a) and pd.notna(b):  #if both values contain a string\n",
    "        if a == b: \n",
    "            return f\"{a}\"\n",
    "        elif a == 'yes' or b == 'yes':\n",
    "            if a == 'yes':\n",
    "                return  f\"{b}\"\n",
    "            elif b == 'yes':\n",
    "                return  f\"{a}\"\n",
    "        else: \n",
    "            return f\"{a}\" #f\"{a}_{b}\" # assuming that value from column A contains the more detailed information\n",
    "    else: \n",
    "        None # Decision point: If nones are existent, decide on what to do with Nones. Are we sure that these are education facilities? Delete them? Provide another tag to them?\n",
    "\n",
    "def filter_dataframe(assets, column_names_lst):\n",
    "    \"\"\"\n",
    "    Filter a GeoDataFrame by combining information from two specified columns and removing selected columns.\n",
    "    Args:\n",
    "        assets (geopandas.GeoDataFrame): The input GeoDataFrame containing spatial geometries and columns to filter.\n",
    "        column_names_lst (list): A list of two column names whose information needs to be combined to create a new 'asset' column.\n",
    "\n",
    "    Returns:\n",
    "        geopandas.GeoDataFrame: A filtered GeoDataFrame with a new 'asset' column and selected columns dropped, and points converted to polygons.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(column_names_lst) == 2:        \n",
    "        assets['asset'] = assets.apply(lambda row: combine_columns(row[column_names_lst[0]], row[column_names_lst[1]]), axis=1) # create new column based on tag information provided in two columns\n",
    "    elif len(column_names_lst) == 3:\n",
    "        assets['asset_temp'] = assets.apply(lambda row: combine_columns(row[column_names_lst[0]], row[column_names_lst[1]]), axis=1) # create temp column based on tag information provided in two columns\n",
    "        assets['asset'] = assets.apply(lambda row: combine_columns(row['asset_temp'], row[column_names_lst[2]]), axis=1) # create new column based on tag information provided in two columns\n",
    "        column_names_lst.append('asset_temp')        \n",
    "    else:\n",
    "        print(\"Warning: column_names_lst should contain 2 or 3 items\")\n",
    "\n",
    "    assets = assets.drop(columns=column_names_lst, axis=1) # drop columns\n",
    "    assets = remove_contained_assets_and_convert(assets)\n",
    "    \n",
    "    return assets\n",
    "\n",
    "def delete_linestring_data(assets, infra_lst):\n",
    "    \"\"\"\n",
    "    Filter and update a GeoDataFrame by excluding rows with LineString geometries.\n",
    "\n",
    "    Parameters:\n",
    "    - assets (geopandas.GeoDataFrame): The original GeoDataFrame.\n",
    "    - infra_lst (lst): A list with the infrastructure typs to filter.\n",
    "\n",
    "    Returns:\n",
    "    - geopandas.GeoDataFrame: The updated GeoDataFrame with excluded LineString rows.\n",
    "    \"\"\"\n",
    "\n",
    "    for infra_type in infra_lst:\n",
    "        #create subset of data\n",
    "        condition = assets['asset'] == infra_type\n",
    "        subset = assets[condition]\n",
    "        \n",
    "        #delete line data if there is line data (assuming that this function is only for point and polygon data)\n",
    "        subset = subset[subset['geometry'].geom_type.isin(['Point', 'MultiPoint', 'Polygon', 'MultiPolygon'])]  # Keep (multi-) points and polygon geometries\n",
    "    \n",
    "        #update the original Dataframe by excluding rows in the subset\n",
    "        assets = assets[~condition | condition & subset['geometry'].notna()]\n",
    "\n",
    "    return assets\n",
    "\n",
    "def delete_point_and_polygons(assets, infra_lst):\n",
    "    \"\"\"\n",
    "    Filter and update a GeoDataFrame by excluding rows with points and (multi-)polygon geometries.\n",
    "\n",
    "    Parameters:\n",
    "    - assets (geopandas.GeoDataFrame): The original GeoDataFrame.\n",
    "    - infra_lst (lst): A list with the infrastructure typs to filter.\n",
    "\n",
    "    Returns:\n",
    "    - geopandas.GeoDataFrame: The updated GeoDataFrame with excluded points and (multi-)polygon rows.\n",
    "    \"\"\"\n",
    "\n",
    "    for infra_type in infra_lst:\n",
    "        #create subset of data\n",
    "        condition = assets['asset'] == infra_type\n",
    "        subset = assets[condition]\n",
    "        \n",
    "        #delete points and (multi-)polygon data if available\n",
    "        subset = subset[subset['geometry'].geom_type.isin(['LineString', 'MultiLineString'])]  # Keep only LineString geometries\n",
    "\n",
    "\n",
    "        #update the original Dataframe by excluding rows in the subset\n",
    "        assets = assets[~condition | condition & subset['geometry'].notna()]\n",
    "\n",
    "    return assets\n",
    "\n",
    "def remove_polygons_with_contained_points(gdf):\n",
    "    \"\"\"\n",
    "    Remove polygons in a GeoDataFrame if there is a point falling within them.\n",
    "    Arguments:\n",
    "        gdf : GeoDataFrame containing entries with point and (multi-)polygon geometry\n",
    "    Returns:\n",
    "    - geopandas.GeoDataFrame: GeoDataFrame containing entries with point and (multi-)polygon geometry, but without duplicates\n",
    "    \"\"\"\n",
    "    gdf = gdf.reset_index(drop=True)\n",
    "    \n",
    "    ind_poly_with_points = np.unique(gpd.sjoin(gdf[gdf.geometry.type == 'Point'],\n",
    "                                              gdf[gdf.geometry.type.isin(['MultiPolygon', 'Polygon'])],\n",
    "                                              predicate='within').index_right)\n",
    "    \n",
    "    return gdf.drop(index=ind_poly_with_points).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def remove_contained_assets_and_convert(assets):\n",
    "    \"\"\"\n",
    "    Process the geometry of assets, removing contained points and polygons, and converting points to polygons.\n",
    "    Args:\n",
    "        assets (geopandas.GeoDataFrame): Input GeoDataFrame containing asset geometries.\n",
    "\n",
    "    Returns:\n",
    "        geopandas.GeoDataFrame: Processed GeoDataFrame with updated asset geometries.\n",
    "    \"\"\"\n",
    "    \n",
    "    assets =  remove_contained_polys(remove_contained_points(assets)) #remove points and polygons within a (larger) polygon\n",
    "    \n",
    "    #convert points to polygons\n",
    "    if (assets.loc[assets.geom_type == 'MultiPolygon']).empty:\n",
    "        default_distance = 58.776\n",
    "        assets.loc[assets.geom_type == 'Point','geometry'] = assets.loc[assets.geom_type == 'Point'].buffer(distance=default_distance, cap_style='square')\n",
    "    else:    \n",
    "        assets.loc[assets.geom_type == 'Point','geometry'] = assets.loc[assets.geom_type == 'Point'].buffer(\n",
    "                                                                        distance=np.sqrt(assets.loc[assets.geom_type == 'MultiPolygon'].area.median())/2, cap_style='square')\n",
    "\n",
    "    return assets\n",
    "\n",
    "def create_point_from_polygon(gdf):\n",
    "    \"\"\"\n",
    "    Transforms polygons into points\n",
    "    Arguments:\n",
    "        gdf: A geodataframe containing a column geometry\n",
    "    Returns:\n",
    "    - geopandas.GeoDataFrame: The updated GeoDataFrame without polygons but with only point geometries\n",
    "    \"\"\"\n",
    "    gdf['geometry'] = gdf['geometry'].apply(lambda geom: MultiPolygon([geom]) if geom.geom_type == 'Polygon' else geom) #convert to multipolygons in case polygons are in the df\n",
    "    #gdf.loc[gdf.geom_type == 'MultiPolygon','geometry'] = gdf.loc[assets.geom_type == 'MultiPolygon'].centroid #convert polygon to point\n",
    "    gdf.loc[gdf.geom_type == 'MultiPolygon','geometry'] = gdf.loc[gdf.geom_type == 'MultiPolygon'].centroid #convert polygon to point\n",
    "    return gdf\n",
    "    \n",
    "def process_selected_assets(gdf, polygon_types, point_types):\n",
    "    \"\"\"\n",
    "    Process the geometry of selected assets, removing contained points and polygons, and converting non-contained points to polygons.\n",
    "    Args:\n",
    "        gdf (geopandas.GeoDataFrame): Input GeoDataFrame containing asset geometries.\n",
    "        selected_types (list): List of asset types to process.\n",
    "\n",
    "    Returns:\n",
    "        geopandas.GeoDataFrame: Processed GeoDataFrame with updated asset geometries.\n",
    "    \"\"\"\n",
    "    asset_temp = gdf['asset'].tolist()\n",
    "    gdf.insert(1, 'asset_temp', asset_temp) \n",
    "    \n",
    "    # For assets that we need as (multi-)polygons: group by asset type and apply the processing function\n",
    "    filtered_assets = gdf[gdf['asset'].isin(polygon_types)] # Filter only selected asset types\n",
    "    polygon_gdf = (filtered_assets.groupby('asset_temp').apply(remove_contained_assets_and_convert, include_groups=False)).reset_index(drop=True)\n",
    "\n",
    "    # For assets that we need as (multi-)points: group by asset type and apply the processing function\n",
    "    filtered_assets = gdf[gdf['asset'].isin(point_types)] # Filter only selected asset types\n",
    "    #point_gdf = (filtered_assets.groupby('asset').apply(create_point_from_polygon)).reset_index(drop=True)\n",
    "    point_gdf = (filtered_assets.groupby('asset_temp').apply(lambda group: create_point_from_polygon(remove_polygons_with_contained_points(group)), include_groups=False)).reset_index(drop=True)\n",
    "    \n",
    "    # Concatenate the two dataframes along rows\n",
    "    merged_gdf = pd.concat([polygon_gdf, point_gdf], ignore_index=True)\n",
    "    \n",
    "    return merged_gdf\n",
    "\n",
    "def create_damage_csv(damage_output, hazard_type, pathway_dict, country_code, sub_system):\n",
    "    \"\"\"\n",
    "    Create a CSV file containing damage information.\n",
    "    Arguments:\n",
    "        damage_output: A dictionary containing damage information.\n",
    "        hazard_type: The type of hazard (e.g., 'earthquake', 'flood').\n",
    "        pathway_dict: A dictionary containing file paths for different data.\n",
    "        country_code: A string containing information about the country code\n",
    "        sub_system: A string containing information about the subsystem considered\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "  \n",
    "    hazard_output_path = pathway_dict['data_path'] / 'damage' / country_code\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not hazard_output_path.exists():\n",
    "        # Create the directory\n",
    "        hazard_output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    csv_file_path = hazard_output_path / '{}_{}_{}.csv'.format(country_code, hazard_type, sub_system)\n",
    "    \n",
    "    with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        \n",
    "        # Write header\n",
    "        csv_writer.writerow(['Country', 'Return period', 'Subsystem', 'Infrastructure type', 'Curve ID number', 'Damage ID number', 'Damage', 'Exposed assets'])\n",
    "        \n",
    "        # Write data\n",
    "        for key, value in damage_output.items():\n",
    "            csv_writer.writerow(list(key) + list(value))\n",
    "    \n",
    "    print(f\"CSV file created at: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d7a95c-625d-4f15-b081-82ce1967e557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_damage_csv_without_exposure(damage_output, hazard_type, pathway_dict, country_code, sub_system):\n",
    "    \"\"\"\n",
    "    Create a CSV file containing damage information.\n",
    "    Arguments:\n",
    "        damage_output: A dictionary containing damage information.\n",
    "        hazard_type: The type of hazard (e.g., 'earthquake', 'flood').\n",
    "        pathway_dict: A dictionary containing file paths for different data.\n",
    "        country_code: A string containing information about the country code\n",
    "        sub_system: A string containing information about the subsystem considered\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if hazard_type in ['landslide_eq', 'landslide_rf']:\n",
    "\n",
    "        hazard_output_path = pathway_dict['data_path'] / 'damage' / country_code\n",
    "        \n",
    "        # Check if the directory exists\n",
    "        if not hazard_output_path.exists():\n",
    "            # Create the directory\n",
    "            hazard_output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        csv_file_path = hazard_output_path / '{}_{}_{}.csv'.format(country_code, hazard_type, sub_system)\n",
    "        \n",
    "        with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            \n",
    "            # Write header\n",
    "            csv_writer.writerow(['Country', 'Return period landslide', 'Return period hazard trigger', 'Subsystem', 'Infrastructure type', 'Curve ID number or assumption', 'Damage ID number', 'Damage'])\n",
    "            \n",
    "            # Write data\n",
    "            for key, value in damage_output.items():\n",
    "                # Extract values from key dictionary\n",
    "                country = key[0]\n",
    "                return_period = key[1]\n",
    "                return_period_tr = key[2]\n",
    "                subsystem = key[3]\n",
    "                infrastructure_type = key[4]\n",
    "                curve_id_number = key[5]\n",
    "                damage_id_number = key[6]\n",
    "                damage = value\n",
    "    \n",
    "                # Write row to CSV\n",
    "                csv_writer.writerow([country, return_period, return_period_tr, subsystem, infrastructure_type, curve_id_number, damage_id_number, damage])\n",
    "        \n",
    "        print(f\"CSV file created at: {csv_file_path}\")\n",
    "\n",
    "    else:\n",
    "        hazard_output_path = pathway_dict['data_path'] / 'damage' / country_code\n",
    "        \n",
    "        # Check if the directory exists\n",
    "        if not hazard_output_path.exists():\n",
    "            # Create the directory\n",
    "            hazard_output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        csv_file_path = hazard_output_path / '{}_{}_{}.csv'.format(country_code, hazard_type, sub_system)\n",
    "        \n",
    "        with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            \n",
    "            # Write header\n",
    "            csv_writer.writerow(['Country', 'Return period', 'Subsystem', 'Infrastructure type', 'Curve ID number or assumption', 'Damage ID number', 'Damage'])\n",
    "            \n",
    "            # Write data\n",
    "            for key, value in damage_output.items():\n",
    "                # Extract values from key dictionary\n",
    "                country = key[0]\n",
    "                return_period = key[1]\n",
    "                subsystem = key[2]\n",
    "                infrastructure_type = key[3]\n",
    "                curve_id_number = key[4]\n",
    "                damage_id_number = key[5]\n",
    "                damage = value\n",
    "    \n",
    "                # Write row to CSV\n",
    "                csv_writer.writerow([country, return_period, subsystem, infrastructure_type, curve_id_number, damage_id_number, damage])\n",
    "        \n",
    "        print(f\"CSV file created at: {csv_file_path}\")\n",
    "\n",
    "def get_damage_per_asset_rp(asset,hazard_numpified,asset_geom,hazard_intensity,fragility_values,maxdam_asset,unit_maxdam):\n",
    "    \"\"\"\n",
    "    Calculate damage for a given asset based on hazard information.\n",
    "    Arguments:\n",
    "        *asset*: Tuple containing information about the asset. It includes:\n",
    "            - Index or identifier of the asset (asset[0]).\n",
    "            - The specific hazard points in which asset is exposed (asset[1]['hazard_point']).\n",
    "        *hazard_numpified*: NumPy array representing hazard information.\n",
    "        *asset_geom*: Shapely geometry representing the spatial coordinates of the asset.\n",
    "        *hazard_intensity*: NumPy array representing the hazard intensities of the curve for the asset type.\n",
    "        *fragility_values*: NumPy array representing the damage factors of the curve for the asset type.\n",
    "        *maxdam_asset*: Maximum damage value for asset.\n",
    "        *unit_maxdam*: The unit of maximum damage value for asset.\n",
    "    Returns:\n",
    "        *float*: The calculated damage for the specific asset.\n",
    "    \"\"\"\n",
    "     \n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = hazard_numpified[asset[1]['hazard_point'].values] \n",
    "    get_hazard_points = get_hazard_points[shapely.intersects(get_hazard_points[:,1],asset_geom)]\n",
    "    return_periods = asset[1]['return_period'].values\n",
    "    for i, (point, polygon) in enumerate(get_hazard_points):\n",
    "        get_hazard_points[i][0] = return_periods[i]\n",
    "    if type(maxdam_asset) == str: maxdam_asset = float(maxdam_asset)\n",
    "\n",
    "    # estimate damage\n",
    "    if len(get_hazard_points) == 0: # no overlay of asset with hazard\n",
    "        return np.empty(0)\n",
    "    else:\n",
    "        if asset_geom.geom_type == 'LineString':\n",
    "            overlay_meters = shapely.length(shapely.intersection(get_hazard_points[:,1],asset_geom)) # get the length of exposed meters per hazard cell\n",
    "            damage = np.float16(np.full(len(get_hazard_points[:,0]), fragility_values[0])) * overlay_meters * maxdam_asset\n",
    "            return np.vstack([damage, get_hazard_points[:,0]])\n",
    "\n",
    "        elif asset_geom.geom_type in ['MultiPolygon','Polygon']:\n",
    "            overlay_m2 = shapely.area(shapely.intersection(get_hazard_points[:,1],asset_geom))\n",
    "            if '/unit' in unit_maxdam:\n",
    "                converted_maxdam = maxdam_asset / shapely.area(asset_geom) #convert to maxdam/m2\n",
    "                damage = (np.float16(np.full(len(get_hazard_points[:,0]), fragility_values[0])) * overlay_m2 * converted_maxdam)\n",
    "                return np.vstack([damage, get_hazard_points[:,0]])\n",
    "            else:\n",
    "                damage = (np.float16(np.full(len(get_hazard_points[:,0]), fragility_values[0])) * overlay_m2 * maxdam_asset)\n",
    "                return np.vstack([damage, get_hazard_points[:,0]])\n",
    "                    \n",
    "        elif asset_geom.geom_type == 'Point':\n",
    "            damage = (np.float16(np.full(len(get_hazard_points[:,0]), fragility_values[0])) * maxdam_asset)\n",
    "            return np.vstack([damage, get_hazard_points[:,0]])\n",
    "\n",
    "def get_damage_per_asset_all_hazards(asset,hazard_numpified,asset_geom,hazard_intensity,fragility_values,maxdam_asset,unit_maxdam):\n",
    "    \"\"\"\n",
    "    Calculate damage for a given asset based on hazard information.\n",
    "    Arguments:\n",
    "        *asset*: Tuple containing information about the asset. It includes:\n",
    "            - Index or identifier of the asset (asset[0]).\n",
    "            - The specific hazard points in which asset is exposed (asset[1]['hazard_point']).\n",
    "        *hazard_numpified*: NumPy array representing hazard information.\n",
    "        *asset_geom*: Shapely geometry representing the spatial coordinates of the asset.\n",
    "        *hazard_intensity*: NumPy array representing the hazard intensities of the curve for the asset type.\n",
    "        *fragility_values*: NumPy array representing the damage factors of the curve for the asset type.\n",
    "        *maxdam_asset*: Maximum damage value for asset.\n",
    "        *unit_maxdam*: The unit of maximum damage value for asset.\n",
    "    Returns:\n",
    "        *float*: The calculated damage for the specific asset.\n",
    "    \"\"\"\n",
    "     \n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = hazard_numpified[asset[1]['hazard_point'].values] \n",
    "    get_hazard_points = get_hazard_points[shapely.intersects(get_hazard_points[:,1],asset_geom)]\n",
    "\n",
    "    if 'return_period' in asset[1].columns: #if there is no intensity value in the map, but the return periods are provided \n",
    "        return_periods = asset[1]['return_period'].values\n",
    "        for i, (point, polygon) in enumerate(get_hazard_points):\n",
    "            get_hazard_points[i][0] = return_periods[i]\n",
    "\n",
    "    # estimate damage\n",
    "    if len(get_hazard_points) == 0: # no overlay of asset with hazard\n",
    "        print(0)\n",
    "        \n",
    "    else:\n",
    "        if asset_geom.geom_type == 'LineString':\n",
    "            overlay_meters = shapely.length(shapely.intersection(get_hazard_points[:,1],asset_geom)) # get the length of exposed meters per hazard cell\n",
    "            if hazard_intensity[0] != 'Exposure to hazard':\n",
    "                return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_meters*maxdam_asset) #return asset number, total damage for asset number (damage factor * meters * max. damage)\n",
    "            elif hazard_intensity[0] == 'Exposure to hazard':\n",
    "                damage = np.sum(np.float16(np.full(len(get_hazard_points[:,0]), fragility_values[0])) * overlay_meters * maxdam_asset)\n",
    "                return np.vstack([damage, get_hazard_points[:,0]])\n",
    "\n",
    "        elif asset_geom.geom_type in ['MultiPolygon','Polygon']:\n",
    "            overlay_m2 = shapely.area(shapely.intersection(get_hazard_points[:,1],asset_geom))\n",
    "            if '/unit' in unit_maxdam:\n",
    "                converted_maxdam = maxdam_asset / shapely.area(asset_geom) #convert to maxdam/m2\n",
    "                if hazard_intensity[0] != 'Exposure to hazard':\n",
    "                    return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_m2*converted_maxdam)\n",
    "                elif hazard_intensity[0] == 'Exposure to hazard':\n",
    "                    damage = np.sum(np.float16(np.full(len(get_hazard_points[:,0]), fragility_values[0])) * overlay_m2 * converted_maxdam)\n",
    "                    return np.vstack([damage, get_hazard_points[:,0]])\n",
    "            else:\n",
    "                if hazard_intensity[0] != 'Exposure to hazard':\n",
    "                    return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_m2*maxdam_asset) #return asset number, total damage for asset number (damage factor * meters * max. damage)\n",
    "                elif hazard_intensity[0] == 'Exposure to hazard':\n",
    "                    damage = np.sum(np.float16(np.full(len(get_hazard_points[:,0]), fragility_values[0])) * overlay_m2 * maxdam_asset)\n",
    "                    return np.vstack([damage, get_hazard_points[:,0]])\n",
    "                    \n",
    "        elif asset_geom.geom_type == 'Point':\n",
    "            if hazard_intensity[0] != 'Exposure to hazard':\n",
    "                return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*maxdam_asset)\n",
    "            elif hazard_intensity[0] == 'Exposure to hazard':\n",
    "                damage = np.sum(np.float16(np.full(len(get_hazard_points[:,0]), fragility_values[0])) * maxdam_asset)\n",
    "                return np.vstack([damage, get_hazard_points[:,0]])\n",
    "\n",
    "def read_vul_maxdam(data_path,hazard_type,infra_type,database_id_curves=False,database_maxdam=False):\n",
    "    \"\"\"\n",
    "    Read vulnerability curves and maximum damage data for a specific hazard and infrastructure type.\n",
    "    Arguments:\n",
    "        *data_path*: The base directory path where vulnerability and maximum damage data files are stored.\n",
    "        *hazard_type*: The type of hazard in string format, such as 'pluvial', 'fluvial', or 'windstorm'.\n",
    "        *infra_type*: The type of infrastructure in string format for which vulnerability curves and maximum damage data are needed.\n",
    "    \n",
    "    Returns:\n",
    "        *tuple*: A tuple containing two DataFrames:\n",
    "            - The first DataFrame contains vulnerability curves specific to the given hazard and infrastructure type.\n",
    "            - The second DataFrame contains maximum damage data for the specified infrastructure type.\n",
    "    \"\"\"\n",
    "\n",
    "    database_id_curves=False\n",
    "    database_maxdam=False\n",
    "\n",
    "    vul_data = data_path / 'Vulnerability'\n",
    "    \n",
    "    # Load assumptions file containing curve - maxdam combinations per infrastructure type\n",
    "    if hazard_type in ['pluvial','fluvial']: \n",
    "        assumptions = pd.read_excel(vul_data / 'S1_Assumptions_Test.xlsx',sheet_name = 'Flooding assumptions',header=[1])\n",
    "    elif hazard_type == 'windstorm':\n",
    "        assumptions = pd.read_excel(vul_data / 'S1_Assumptions_Test.xlsx',sheet_name = 'Windstorm assumptions',header=[1])\n",
    "    elif hazard_type == 'earthquake':\n",
    "        assumptions = pd.read_excel(vul_data / 'S1_Assumptions_Test.xlsx',sheet_name = 'Earthquake assumptions',header=[1])\n",
    "    elif hazard_type in ['landslide_eq', 'landslide_rf']:\n",
    "        assumptions = pd.read_excel(vul_data / 'S1_Assumptions_Test.xlsx',sheet_name = 'Landslide assumptions',header=[1])\n",
    "\n",
    "    if database_id_curves==False:\n",
    "        #get assumptions from dictionary\n",
    "        if hazard_type == 'earthquake':\n",
    "            assump_curves = ['E7.1', 'E7.6', 'E7.7', 'E7.8', 'E7.9', 'E7.10', 'E7.11', 'E7.12', 'E7.13', 'E7.14' ]\n",
    "        elif hazard_type in ['landslide_eq', 'landslide_rf']:\n",
    "            assump_curves = [None]\n",
    "    else:\n",
    "        #get assumptions from database\n",
    "        assumptions['Infrastructure type'] = assumptions['Infrastructure type'].str.lower()\n",
    "        if \"_\" in infra_type: infra_type = infra_type.replace('_', ' ')\n",
    "        assump_infra_type = assumptions[assumptions['Infrastructure type'] == infra_type]\n",
    "        if assump_infra_type['Vulnerability ID number'].item() == 'No ID number, partial destruction is assumed':\n",
    "            assump_curves = [None] #code evt uitbreiden, dat het onderscheid maakt tussen infrastructuur types waar wel/geen id nummer voor is gegeven\n",
    "        else:\n",
    "            assump_curves = ast.literal_eval(assump_infra_type['Vulnerability ID number'].item())\n",
    "\n",
    "    if \" \" in infra_type: infra_type = infra_type.replace(' ', '_')\n",
    "\n",
    "    # Get curves\n",
    "    if hazard_type in ['pluvial','fluvial']:  \n",
    "        curves = pd.read_excel(vul_data / 'Table_D2_Hazard_Fragility_and_Vulnerability_Curves_V1.1.0_conversions.xlsx',sheet_name = 'F_Vuln_Depth',index_col=[0],header=[0,1,2,3,4])\n",
    "        infra_curves =  curves[assump_curves]\n",
    "    elif hazard_type == 'windstorm':\n",
    "        curves = pd.read_excel(vul_data / 'Table_D2_Hazard_Fragility_and_Vulnerability_Curves_V1.1.0_conversions.xlsx',sheet_name = 'W_Vuln_V10m',index_col=[0],header=[0,1,2,3,4])\n",
    "        infra_curves =  curves[assump_curves]\n",
    "    elif hazard_type == 'earthquake':\n",
    "        curves = pd.read_excel(vul_data / 'Table_D2_Hazard_Fragility_and_Vulnerability_Curves_V1.1.0_conversions.xlsx',sheet_name = 'E_Vuln_PGA',index_col=[0],header=[0,1,2,3,4])\n",
    "        infra_curves =  curves[assump_curves]\n",
    "    elif hazard_type in ['landslide_eq', 'landslide_rf']:\n",
    "        if assump_curves == [None]:\n",
    "            #infra_curves = pd.DataFrame([1], columns=['Complete destruction'])\n",
    "            #infra_curves.columns=pd.MultiIndex.from_product([['Damage factor'],infra_curves.columns])\n",
    "            infra_curves = pd.DataFrame([['Exposure to hazard', 0.5]], columns=['Intensity measure', 'Damage factor']).set_index('Intensity measure')\n",
    "            infra_curves.columns=pd.MultiIndex.from_product([['Partial destruction (0.5)'],infra_curves.columns])\n",
    "        else:\n",
    "            curves = pd.read_excel(vul_data / 'Table_D2_Hazard_Fragility_and_Vulnerability_Curves_V1.1.0_conversions.xlsx',sheet_name = 'L_Frag_PGD',index_col=[0],header=[0,1,2,3,4])\n",
    "            infra_curves =  curves[assump_curves]\n",
    "\n",
    "    if database_maxdam==False:\n",
    "        maxdam_dict = {'unclassified':300, \n",
    "                        'primary':2000, \n",
    "                        'secondary':1300, \n",
    "                        'tertiary':700, \n",
    "                        'residential':500,\n",
    "                        'trunk':2000, \n",
    "                        'trunk_link':2000, \n",
    "                        'motorway':2000, \n",
    "                        'motorway_link':2000, \n",
    "                        'primary_link':2000, \n",
    "                        'secondary_link':1300,\n",
    "                        'tertiary_link':700,\n",
    "                        'road':700,\n",
    "                        'track':300, }\n",
    "        infra_maxdam =  pd.Series([str(maxdam_dict[infra_type])], index=['default'])\n",
    "        infra_maxdam.name = 'Amount'   \n",
    "        infra_units =  pd.Series(['euro/m'], index=['default'])\n",
    "        infra_units.name = 'unit'\n",
    "    else:\n",
    "        # get maxdam from database\n",
    "        assump_maxdams = ast.literal_eval(assump_infra_type['Maximum damage ID number'].item())\n",
    "        maxdam = pd.read_excel(vul_data / 'Table_D3_Costs_V1.1.0_converted.xlsx', sheet_name='Cost_Database',index_col=[0])\n",
    "        infra_costs = maxdam[maxdam.index.isin(assump_maxdams)][['Amount', 'Unit']].dropna(subset=['Amount'])\n",
    "        infra_maxdam = infra_costs['Amount'][pd.to_numeric(infra_costs['Amount'], errors='coerce').notnull()]\n",
    "        infra_units = infra_costs['Unit'].filter(items=list(infra_maxdam.index), axis=0)\n",
    "\n",
    "    return infra_curves,infra_maxdam,infra_units\n",
    "\n",
    "def matrix_landslide_rf_susc(overlay_rf, get_susc_data, overlay_assets, susc_point):\n",
    "    \"\"\"\n",
    "    Calculate damage for a given asset based on hazard information.\n",
    "    Arguments:\n",
    "        *asset*: Tuple containing information about the asset. It includes:\n",
    "            - Index or identifier of the asset (asset[0]).\n",
    "            - The specific hazard points in which asset is exposed (asset[1]['hazard_point']).\n",
    "        *hazard_numpified*: NumPy array representing hazard information.\n",
    "        *asset_geom*: Shapely geometry representing the spatial coordinates of the asset.\n",
    "        *hazard_intensity*: NumPy array representing the hazard intensities of the curve for the asset type.\n",
    "        *fragility_values*: NumPy array representing the damage factors of the curve for the asset type.\n",
    "        *maxdam_asset*: Maximum damage value for asset.\n",
    "        *unit_maxdam*: The unit of maximum damage value for asset.\n",
    "    Returns:\n",
    "        *float*: The calculated damage for the specific asset.\n",
    "    \"\"\"\n",
    "\n",
    "    bool_series = (overlay_assets['hazard_point'] == susc_point[0])\n",
    "\n",
    "    unique_classes = overlay_rf['cond_classes'].unique()\n",
    "    if len(unique_classes) == 1:\n",
    "        rf_class = unique_classes[0]\n",
    "    else:\n",
    "        rf_class = max(unique_classes)\n",
    "    \n",
    "    if rf_class == '1' and get_susc_data[0] == 1:\n",
    "        #print('1x1')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '1' and get_susc_data[0] == 2:\n",
    "        #print('1x2')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '1' and get_susc_data[0] == 3:\n",
    "        #print('1x3')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '1' and get_susc_data[0] == 4:\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '1' and get_susc_data[0] == 5:\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '2' and get_susc_data[0] == 1:\n",
    "        #print('2x1')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '2' and get_susc_data[0] == 2:\n",
    "        #print('2x2', susc_point[0], len((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index))\n",
    "        #print(len((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index))\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 100\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 5\n",
    "    elif rf_class == '2' and get_susc_data[0] == 3:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 50\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 5\n",
    "    elif rf_class == '2' and get_susc_data[0] == 4:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 33\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 5\n",
    "    elif rf_class == '2' and get_susc_data[0] == 5:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 20\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 5\n",
    "    elif rf_class == '3' and get_susc_data[0] == 1:\n",
    "        #print('3x1')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '3' and get_susc_data[0] == 2:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 50\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 25\n",
    "    elif rf_class == '3' and get_susc_data[0] == 3:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 33\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 25\n",
    "    elif rf_class == '3' and get_susc_data[0] == 4:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 20\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 25\n",
    "    elif rf_class == '3' and get_susc_data[0] == 5:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 10\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 25\n",
    "    elif rf_class == '4' and get_susc_data[0] == 1:\n",
    "        #print('4x1')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '4' and get_susc_data[0] == 2:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 33\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 200\n",
    "    elif rf_class == '4' and get_susc_data[0] == 3:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 20\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 200\n",
    "    elif rf_class == '4' and get_susc_data[0] == 4:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 10\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 200\n",
    "    elif rf_class == '4' and get_susc_data[0] == 5:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 7\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 200\n",
    "    elif rf_class == '5' and get_susc_data[0] == 1:\n",
    "        #print('5x1')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '5' and get_susc_data[0] == 2:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 20\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 1000\n",
    "    elif rf_class == '5' and get_susc_data[0] == 3:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 10\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 1000\n",
    "    elif rf_class == '5' and get_susc_data[0] == 4:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 7\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 1000\n",
    "    elif rf_class == '5' and get_susc_data[0] == 5:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 5\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 1000\n",
    "\n",
    "    return overlay_assets\n",
    "\n",
    "def matrix_landslide_eq_susc(overlay_eq, get_susc_data, overlay_assets, susc_point):\n",
    "    \"\"\"\n",
    "    Calculate damage for a given asset based on hazard information.\n",
    "    Arguments:\n",
    "        *asset*: Tuple containing information about the asset. It includes:\n",
    "            - Index or identifier of the asset (asset[0]).\n",
    "            - The specific hazard points in which asset is exposed (asset[1]['hazard_point']).\n",
    "        *hazard_numpified*: NumPy array representing hazard information.\n",
    "        *asset_geom*: Shapely geometry representing the spatial coordinates of the asset.\n",
    "        *hazard_intensity*: NumPy array representing the hazard intensities of the curve for the asset type.\n",
    "        *fragility_values*: NumPy array representing the damage factors of the curve for the asset type.\n",
    "        *maxdam_asset*: Maximum damage value for asset.\n",
    "        *unit_maxdam*: The unit of maximum damage value for asset.\n",
    "    Returns:\n",
    "        *float*: The calculated damage for the specific asset.\n",
    "    \"\"\"\n",
    "\n",
    "    bool_series = (overlay_assets['hazard_point'] == susc_point[0])\n",
    "\n",
    "    unique_classes = overlay_eq['cond_classes'].unique()\n",
    "    if len(unique_classes) == 1:\n",
    "        eq_class = unique_classes[0]\n",
    "    else:\n",
    "        eq_class = max(unique_classes)\n",
    "    \n",
    "    if eq_class == '1' and get_susc_data[0] == 1:\n",
    "        #print('1x1')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif eq_class == '1' and get_susc_data[0] == 2:\n",
    "        #print('1x2')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif eq_class == '1' and get_susc_data[0] == 3:\n",
    "        #print('1x3')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif eq_class == '1' and get_susc_data[0] == 4:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 1000\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '1' and get_susc_data[0] == 5:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 200\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '2' and get_susc_data[0] == 1:\n",
    "        #print('2x1')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif eq_class == '2' and get_susc_data[0] == 2:\n",
    "        #print('2x2', susc_point[0], len((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index))\n",
    "        #print(len((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index))\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif eq_class == '2' and get_susc_data[0] == 3:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 1000\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '2' and get_susc_data[0] == 4:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 200\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '2' and get_susc_data[0] == 5:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 100\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '3' and get_susc_data[0] == 1:\n",
    "        #print('3x1')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif eq_class == '3' and get_susc_data[0] == 2:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 1000\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '3' and get_susc_data[0] == 3:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 200\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '3' and get_susc_data[0] == 4:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 100\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '3' and get_susc_data[0] == 5:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 20\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '4' and get_susc_data[0] == 1:\n",
    "        #print('4x1')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif eq_class == '4' and get_susc_data[0] == 2:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 200\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '4' and get_susc_data[0] == 3:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 100\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '4' and get_susc_data[0] == 4:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 20\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '4' and get_susc_data[0] == 5:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 10\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '5' and get_susc_data[0] == 1:\n",
    "        #print('5x1')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif eq_class == '5' and get_susc_data[0] == 2:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 100\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '5' and get_susc_data[0] == 3:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 20\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '5' and get_susc_data[0] == 4:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 10\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "    elif eq_class == '5' and get_susc_data[0] == 5:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 2.5\n",
    "        overlay_assets.loc[bool_series, 'return_period_trig'] = 475\n",
    "\n",
    "    return overlay_assets\n",
    "\n",
    "def matrix_landslide_rf_susc_old(overlay_rf, get_susc_data, overlay_assets, susc_point):\n",
    "    \"\"\"\n",
    "    Calculate damage for a given asset based on hazard information.\n",
    "    Arguments:\n",
    "        *asset*: Tuple containing information about the asset. It includes:\n",
    "            - Index or identifier of the asset (asset[0]).\n",
    "            - The specific hazard points in which asset is exposed (asset[1]['hazard_point']).\n",
    "        *hazard_numpified*: NumPy array representing hazard information.\n",
    "        *asset_geom*: Shapely geometry representing the spatial coordinates of the asset.\n",
    "        *hazard_intensity*: NumPy array representing the hazard intensities of the curve for the asset type.\n",
    "        *fragility_values*: NumPy array representing the damage factors of the curve for the asset type.\n",
    "        *maxdam_asset*: Maximum damage value for asset.\n",
    "        *unit_maxdam*: The unit of maximum damage value for asset.\n",
    "    Returns:\n",
    "        *float*: The calculated damage for the specific asset.\n",
    "    \"\"\"\n",
    "\n",
    "    bool_series = (overlay_assets['hazard_point'] == susc_point[0])\n",
    "\n",
    "    unique_classes = overlay_rf['cond_classes'].unique()\n",
    "    if len(unique_classes) == 1:\n",
    "        rf_class = unique_classes[0]\n",
    "    else:\n",
    "        rf_class = max(unique_classes)\n",
    "    \n",
    "    if rf_class == '1' and get_susc_data[0] == 1:\n",
    "        #print('1x1')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '1' and get_susc_data[0] == 2:\n",
    "        #print('1x2')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '1' and get_susc_data[0] == 3:\n",
    "        #print('1x3')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '1' and get_susc_data[0] == 4:\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '1' and get_susc_data[0] == 5:\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '2' and get_susc_data[0] == 1:\n",
    "        #print('2x1')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '2' and get_susc_data[0] == 2:\n",
    "        #print('2x2', susc_point[0], len((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index))\n",
    "        #print(len((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index))\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 100\n",
    "    elif rf_class == '2' and get_susc_data[0] == 3:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 50\n",
    "    elif rf_class == '2' and get_susc_data[0] == 4:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 33\n",
    "    elif rf_class == '2' and get_susc_data[0] == 5:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 20\n",
    "    elif rf_class == '3' and get_susc_data[0] == 1:\n",
    "        #print('3x1')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '3' and get_susc_data[0] == 2:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 50\n",
    "    elif rf_class == '3' and get_susc_data[0] == 3:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 33\n",
    "    elif rf_class == '3' and get_susc_data[0] == 4:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 20\n",
    "    elif rf_class == '3' and get_susc_data[0] == 5:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 10\n",
    "    elif rf_class == '4' and get_susc_data[0] == 1:\n",
    "        #print('4x1')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '4' and get_susc_data[0] == 2:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 33\n",
    "    elif rf_class == '4' and get_susc_data[0] == 3:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 20\n",
    "    elif rf_class == '4' and get_susc_data[0] == 4:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 10\n",
    "    elif rf_class == '4' and get_susc_data[0] == 5:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 7\n",
    "    elif rf_class == '5' and get_susc_data[0] == 1:\n",
    "        #print('5x1')\n",
    "        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    elif rf_class == '5' and get_susc_data[0] == 2:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 20\n",
    "    elif rf_class == '5' and get_susc_data[0] == 3:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 10\n",
    "    elif rf_class == '5' and get_susc_data[0] == 4:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 7\n",
    "    elif rf_class == '5' and get_susc_data[0] == 5:\n",
    "        overlay_assets.loc[bool_series, 'return_period'] = 5\n",
    "\n",
    "    return overlay_assets\n",
    "\n",
    "def filter_landslide_rf_rps(trig_rp, overlay_assets_ls_rp):\n",
    "    \"\"\"\n",
    "    Reassign data by setting new landslide return period for certain rainfall-triggering event\n",
    "    Arguments:\n",
    "        **:\n",
    "    Returns:\n",
    "        **:\n",
    "    \"\"\"\n",
    "\n",
    "    if trig_rp == 5:\n",
    "        return overlay_assets_ls_rp\n",
    "\n",
    "    return_period_trig = np.array(overlay_assets_ls_rp['return_period_trig'])\n",
    "    return_period = np.array(overlay_assets_ls_rp['return_period'])\n",
    "    \n",
    "    if trig_rp == 25:\n",
    "        corresponding_rps = np.where((return_period_trig == 5) & (return_period == 100), 50,\n",
    "                                     np.where((return_period_trig == 5) & (return_period == 50), 33,\n",
    "                                              np.where((return_period_trig == 5) & (return_period == 33), 20,\n",
    "                                                       np.where((return_period_trig == 5) & (return_period == 20), 10, \n",
    "                                                                return_period))))\n",
    "\n",
    "    elif trig_rp == 200:\n",
    "        corresponding_rps = np.where((return_period_trig == 5) & (return_period == 100), 33,\n",
    "                                     np.where((return_period_trig == 5) & (return_period == 50), 20,\n",
    "                                              np.where((return_period_trig == 5) & (return_period == 33), 10,\n",
    "                                                       np.where((return_period_trig == 5) & (return_period == 20), 7,\n",
    "                                                                np.where((return_period_trig == 25) & (return_period == 50), 33, \n",
    "                                                                         np.where((return_period_trig == 25) & (return_period == 33), 20, \n",
    "                                                                                  np.where((return_period_trig == 25) & (return_period == 20), 10, \n",
    "                                                                                           np.where((return_period_trig == 25) & (return_period == 10), 7, \n",
    "                                                                                                    return_period))))))))\n",
    "\n",
    "    elif trig_rp == 1000:\n",
    "        corresponding_rps = np.where((return_period_trig == 5) & (return_period == 100), 20,\n",
    "                                     np.where((return_period_trig == 5) & (return_period == 50), 10,\n",
    "                                              np.where((return_period_trig == 5) & (return_period == 33), 7,\n",
    "                                                       np.where((return_period_trig == 5) & (return_period == 20), 5,\n",
    "                                                                np.where((return_period_trig == 25) & (return_period == 50), 20, \n",
    "                                                                         np.where((return_period_trig == 25) & (return_period == 33), 10, \n",
    "                                                                                  np.where((return_period_trig == 25) & (return_period == 20), 7, \n",
    "                                                                                           np.where((return_period_trig == 25) & (return_period == 10), 5, \n",
    "                                                                                                    np.where((return_period_trig == 200) & (return_period == 10), 20, \n",
    "                                                                                                             np.where((return_period_trig == 200) & (return_period == 10), 10, \n",
    "                                                                                                                      np.where((return_period_trig == 200) & (return_period == 10), 7, \n",
    "                                                                                                                               np.where((return_period_trig == 200) & (return_period == 10), 5, \n",
    "                                                                                                    return_period))))))))))))\n",
    "    overlay_assets_ls_rp.loc[:,'return_period_trig'] = trig_rp # adjust column\n",
    "    overlay_assets_ls_rp.loc[:,'return_period'] = corresponding_rps\n",
    "\n",
    "    return overlay_assets_ls_rp\n",
    "\n",
    "def accumulated_damage_rp_inverse(return_periods_dict_for_asset):\n",
    "    \"\"\"\n",
    "    Adjusts the damage values in the return_periods_dict_for_asset dictionary.\n",
    "    The damages of all return periods lower than a certain return period are added to the damages of that return period.\n",
    "    \n",
    "    Parameters:\n",
    "    return_periods_dict_for_asset (dict): A dictionary with return periods as keys and damages as values.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Adjusted dictionary with accumulated damages.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort the return periods in descending order\n",
    "    sorted_return_periods = sorted(return_periods_dict_for_asset.keys(), reverse=True)\n",
    "    \n",
    "    # Initialize the cumulative damage\n",
    "    cumulative_damage = 0\n",
    "    \n",
    "    # Iterate over the sorted return periods\n",
    "    for current_period in sorted_return_periods:\n",
    "        # Add the current period's damage to the cumulative damage\n",
    "        cumulative_damage += return_periods_dict_for_asset[current_period]\n",
    "        \n",
    "        # Store the adjusted damage in the dictionary\n",
    "        return_periods_dict_for_asset[current_period] = cumulative_damage\n",
    "    \n",
    "    return return_periods_dict_for_asset\n",
    "\n",
    "def accumulated_damage_rp(return_periods_dict_for_asset):\n",
    "    \"\"\"\n",
    "    Adjusts the damage values in the return_periods_dict_for_asset dictionary.\n",
    "    The damages of all return periods lower than a certain return period are added to the damages of that return period.\n",
    "    \n",
    "    Parameters:\n",
    "    return_periods_dict_for_asset (dict): A dictionary with return periods as keys and damages as values.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Adjusted dictionary with accumulated damages.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort the return periods\n",
    "    sorted_return_periods = sorted(return_periods_dict_for_asset.keys())\n",
    "    \n",
    "    # Initialize the adjusted dictionary\n",
    "    adjusted_dict = {}\n",
    "    \n",
    "    # Iterate over the sorted return periods\n",
    "    for i, current_period in enumerate(sorted_return_periods):\n",
    "        # Initialize the cumulative damage\n",
    "        cumulative_damage = return_periods_dict_for_asset[current_period]\n",
    "        \n",
    "        # Add the damages of all lower return periods\n",
    "        for lower_period in sorted_return_periods[:i]:\n",
    "            cumulative_damage += return_periods_dict_for_asset[lower_period]\n",
    "        \n",
    "        # Store the adjusted damage in the dictionary\n",
    "        adjusted_dict[current_period] = cumulative_damage\n",
    "    \n",
    "    return adjusted_dict\n",
    "\n",
    "def overlay_hazard_boundary(df_ds,country_border_geometries):\n",
    "    \"\"\"\n",
    "    Overlay hazard assets on a dataframe of spatial geometries.\n",
    "    Arguments:\n",
    "        *df_ds*: GeoDataFrame containing the spatial geometries of the hazard data. \n",
    "        *boundary*: GeoDataFrame containing the infrastructure assets.\n",
    "    Returns:\n",
    "        *geopandas.GeoSeries*: A GeoSeries containing the spatial geometries of df_ds that intersect with the administrative boundary.\n",
    "    \"\"\"\n",
    "    #overlay \n",
    "    hazard_tree = shapely.STRtree(df_ds.geometry.values)\n",
    "    if (shapely.get_type_id(country_border_geometries.iloc[0]) == 3) | (shapely.get_type_id(country_border_geometries.iloc[0]) == 6): # id types 3 and 6 stand for polygon and multipolygon\n",
    "        intersect_index = hazard_tree.query(country_border_geometries.geometry,predicate='intersects')\n",
    "    intersect_index = np.unique(np.concatenate(intersect_index))\n",
    "    \n",
    "    return df_ds.iloc[intersect_index].reset_index(drop=True)\n",
    "\n",
    "def overlay_hazard_boundary_temp(df_ds,country_border_geometries):\n",
    "    \"\"\"\n",
    "    Overlay hazard assets on a dataframe of spatial geometries.\n",
    "    Arguments:\n",
    "        *df_ds*: GeoDataFrame containing the spatial geometries of the hazard data. \n",
    "        *boundary*: GeoDataFrame containing the infrastructure assets.\n",
    "    Returns:\n",
    "        *geopandas.GeoSeries*: A GeoSeries containing the spatial geometries of df_ds that intersect with the administrative boundary.\n",
    "    \"\"\"\n",
    "    #overlay \n",
    "    hazard_tree = shapely.STRtree(df_ds.geometry.values)\n",
    "    #if (shapely.get_type_id(country_border_geometries.iloc[0]) == 3) | (shapely.get_type_id(country_border_geometries.iloc[0]) == 6): # id types 3 and 6 stand for polygon and multipolygon\n",
    "    intersect_index = hazard_tree.query(country_border_geometries.geometry,predicate='intersects')\n",
    "    intersect_index = np.unique(np.concatenate(intersect_index))\n",
    "    \n",
    "    return df_ds.iloc[intersect_index].reset_index(drop=True)\n",
    "\n",
    "def read_rainfall_map(rf_data_path,diameter_distance=0.25/2):\n",
    "     \n",
    "    # load data from NetCDF file\n",
    "    with xr.open_dataset(rf_data_path) as ds:\n",
    "        \n",
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        #ds = ds.rio.clip_box(minx=bbox[0], miny=bbox[1], maxx=bbox[2], maxy=bbox[3])\n",
    "        \n",
    "        ds_vector = ds['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "        \n",
    "        #remove data that will not be used\n",
    "        ds_vector = ds_vector.loc[(ds_vector.band_data > 0) & (ds_vector.band_data <= 10)]\n",
    "        \n",
    "        # create geometry values and drop lat lon columns\n",
    "        ds_vector['geometry'] = [shapely.points(x) for x in list(zip(ds_vector['x'],ds_vector['y']))]\n",
    "        ds_vector = ds_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "\n",
    "        ds_vector['geometry'] = shapely.buffer(ds_vector.geometry, distance=diameter_distance, cap_style='square').values\n",
    "\n",
    "        return ds_vector\n",
    "\n",
    "def landslide_damage(overlay_assets,infra_curves,susc_numpified,assets_infra_type):\n",
    "    \"\"\"\n",
    "    Calculate and output exposure and damage for landslides.\n",
    "    Arguments:\n",
    "\n",
    "    Returns:\n",
    "        *float*: The calculated damage for the specific asset.\n",
    "    \"\"\"  \n",
    "\n",
    "    trig_rp_lst = sorted(overlay_assets['return_period_trig'].unique()) #get list of unique RPs for landslide trigger\n",
    "    for trig_rp in trig_rp_lst:\n",
    "        #overlay_assets_ls_rp = overlay_assets[overlay_assets['return_period_trig'] == trig_rp]\n",
    "        overlay_assets_ls_rp = overlay_assets[overlay_assets['return_period_trig'] <= trig_rp] #get all locations where certain rp may occur\n",
    "        overlay_assets_ls_rp = filter_landslide_rf_rps(trig_rp, overlay_assets_ls_rp)\n",
    "        if hazard_type == 'landslide_eq': collect_asset_damages_per_curve_rp = {key: [] for key in [2.5, 10, 20, 100, 200, 1000]}\n",
    "        if hazard_type == 'landslide_rf': collect_asset_damages_per_curve_rp = {key: [] for key in sorted(overlay_assets_ls_rp['return_period'].unique())}\n",
    "        curve_ids_list = [] # for output at asset level            \n",
    "        for infra_curve in infra_curves:\n",
    "            # get curves\n",
    "            curve = infra_curves[infra_curve[0]]\n",
    "            hazard_intensity = curve.index.values\n",
    "            fragility_values = (np.nan_to_num(curve.values,nan=(np.nanmax(curve.values)))).flatten()\n",
    "\n",
    "            for maxdam in maxdams:\n",
    "                if hazard_type == 'landslide_eq': return_periods_dict_for_infratype = {key: 0 for key in [2.5, 10, 20, 100, 200, 1000]}\n",
    "                if hazard_type == 'landslide_rf': return_periods_dict_for_infratype = {key: 0 for key in sorted(overlay_assets_ls_rp['return_period'].unique())}\n",
    "                collect_geom = []\n",
    "                unit_maxdam = infra_units[maxdams[maxdams == maxdam].index[0]] #get unit maxdam\n",
    "                \n",
    "                collect_damage_asset = {}  # for output at asset level\n",
    "                for asset in tqdm(overlay_assets_ls_rp.groupby('asset'),total=len(overlay_assets_ls_rp.asset.unique())): #group asset items for different hazard points per asset and get total number of unique assets\n",
    "                    if hazard_type == 'landslide_eq': return_periods_dict_for_asset = {key: 0 for key in [2.5, 10, 20, 100, 200, 1000]}\n",
    "                    if hazard_type == 'landslide_rf': return_periods_dict_for_asset = {key: 0 for key in sorted(overlay_assets_ls_rp['return_period'].unique())}\n",
    "                    asset_geom = geom_dict[asset[0]]\n",
    "                    collect_geom.append(asset_geom.wkt)\n",
    "                    if np.max(fragility_values) == 0: #if exposure does not lead to damage\n",
    "                        collect_inb.append(np.empty(0)) #can actually be removed?  \n",
    "                    else:\n",
    "                        collect_inb = (get_damage_per_asset_rp(asset,susc_numpified,asset_geom,hazard_intensity,fragility_values,maxdam,unit_maxdam)) #get list of damages for specific asset\n",
    "                        if not len(collect_inb) == 0:\n",
    "                            for i in range(len(collect_inb[1])):\n",
    "                                return_periods_dict_for_infratype[collect_inb[1][i]] += collect_inb[0][i]\n",
    "                                return_periods_dict_for_asset[collect_inb[1][i]] += collect_inb[0][i] #for output at asset level: get damage per RP asset\n",
    "                            return_periods_dict_for_asset = accumulated_damage_rp(return_periods_dict_for_asset) #for output at asset leve: get accumulated RP damages for asset\n",
    "                    for rp in collect_asset_damages_per_curve_rp:\n",
    "                        asset_damage = pd.Series({asset[0]:return_periods_dict_for_asset[rp]})  # for output at asset level\n",
    "                        asset_damage.columns = [infra_curve[0]]  # for output at asset level\n",
    "                        collect_asset_damages_per_curve_rp[rp].append(asset_damage)  # for output at asset level\n",
    "                curve_ids_list.append(infra_curve[0])  # for output at asset level\n",
    "\n",
    "                #aggegated output\n",
    "                return_periods_dict_for_infratype = accumulated_damage_rp(return_periods_dict_for_infratype) #accumulate damages\n",
    "                for rp in list(return_periods_dict_for_infratype.keys()):\n",
    "                    collect_output[country_code, rp, trig_rp, sub_system, infra_type, infra_curve[0], ((maxdams[maxdams == maxdam]).index)[0]] = return_periods_dict_for_infratype[rp] #collect output for asset, infra_curve and maxdam combination                              \n",
    "    \n",
    "        #asset level output\n",
    "        for rp in collect_asset_damages_per_curve_rp:\n",
    "            if len(collect_asset_damages_per_curve_rp[rp]) != 0: \n",
    "                asset_damages_per_curve_rp = pd.concat(collect_asset_damages_per_curve_rp[rp], ignore_index=False).to_frame(name='Partial destruction (0.5)')\n",
    "                #asset_damages_per_curve_rp.columns = curve_ids_list\n",
    "                damaged_assets = assets_infra_type.merge(asset_damages_per_curve_rp,left_index=True,right_index=True,how='outer')\n",
    "                damaged_assets = damaged_assets.drop(['buffered'],axis=1)\n",
    "                damaged_assets.crs = 3857\n",
    "                damaged_assets = damaged_assets.to_crs(4326)\n",
    "                damaged_assets.damage = damaged_assets[curve_ids_list].fillna(0)\n",
    "                damaged_assets['return_period_trig'] = trig_rp\n",
    "                damaged_assets['return_period_landslide'] = rp\n",
    "                save_path = pathway_dict['data_path'] / 'damage' / country_code / f'{country_code}_{hazard_type}_ls{rp}_trig{trig_rp}_{sub_system}_{infra_type}.parquet'\n",
    "                damaged_assets.to_parquet(save_path)\n",
    "\n",
    "\n",
    "def landslide_damage_and_overlay(overlay_assets,infra_curves,susc_numpified,assets_infra_type):\n",
    "    \"\"\"\n",
    "    Calculate and output exposure and damage for landslides.\n",
    "    Arguments:\n",
    "\n",
    "    Returns:\n",
    "        *float*: The calculated damage for the specific asset.\n",
    "    \"\"\"      \n",
    "\n",
    "    trig_rp_lst = sorted(overlay_assets['return_period_trig'].unique()) #get list of unique RPs for landslide trigger\n",
    "    for trig_rp in trig_rp_lst:\n",
    "        #overlay_assets_ls_rp = overlay_assets[overlay_assets['return_period_trig'] == trig_rp]\n",
    "        overlay_assets_ls_rp = overlay_assets[overlay_assets['return_period_trig'] <= trig_rp] #get all locations where certain rp may occur\n",
    "        if hazard_type == 'landslide_eq': \n",
    "            collect_asset_damages_per_curve_rp = {key: [] for key in [2.5, 10, 20, 100, 200, 1000]}\n",
    "            collect_asset_exposure_per_curve_rp = {key: [] for key in [2.5, 10, 20, 100, 200, 1000]}\n",
    "            collect_asset_landslides_per_curve_rp = {key: [] for key in [2.5, 10, 20, 100, 200, 1000]}\n",
    "        elif hazard_type == 'landslide_rf': \n",
    "            overlay_assets_ls_rp = filter_landslide_rf_rps(trig_rp, overlay_assets_ls_rp)\n",
    "            collect_asset_damages_per_curve_rp = {key: [] for key in sorted(overlay_assets_ls_rp['return_period'].unique())}\n",
    "            collect_asset_exposure_per_curve_rp = {key: [] for key in sorted(overlay_assets_ls_rp['return_period'].unique())}\n",
    "            collect_asset_landslides_per_curve_rp = {key: [] for key in sorted(overlay_assets_ls_rp['return_period'].unique())}\n",
    "        curve_ids_list = [] # for output at asset level            \n",
    "        for infra_curve in infra_curves:\n",
    "            # get curves\n",
    "            curve = infra_curves[infra_curve[0]]\n",
    "            hazard_intensity = curve.index.values\n",
    "            fragility_values = (np.nan_to_num(curve.values,nan=(np.nanmax(curve.values)))).flatten()\n",
    "\n",
    "            for maxdam in maxdams:\n",
    "                if hazard_type == 'landslide_eq': return_periods_dict_for_infratype = {key: 0 for key in [2.5, 10, 20, 100, 200, 1000]}\n",
    "                if hazard_type == 'landslide_rf': return_periods_dict_for_infratype = {key: 0 for key in sorted(overlay_assets_ls_rp['return_period'].unique())}\n",
    "                collect_geom = []\n",
    "                unit_maxdam = infra_units[maxdams[maxdams == maxdam].index[0]] #get unit maxdam\n",
    "                \n",
    "                collect_damage_asset = {}  # for output at asset level\n",
    "                for asset in tqdm(overlay_assets_ls_rp.groupby('asset'),total=len(overlay_assets_ls_rp.asset.unique())): #group asset items for different hazard points per asset and get total number of unique assets\n",
    "                    if hazard_type == 'landslide_eq': \n",
    "                        return_periods_dict_for_asset = {key: 0 for key in [2.5, 10, 20, 100, 200, 1000]}\n",
    "                        return_periods_dict_for_asset_exposure = {key: 0 for key in [2.5, 10, 20, 100, 200, 1000]}\n",
    "                        rp_dict_for_asset_landslide_occur = {key: 0 for key in [2.5, 10, 20, 100, 200, 1000]}\n",
    "                    elif hazard_type == 'landslide_rf': \n",
    "                        return_periods_dict_for_asset = {key: 0 for key in sorted(overlay_assets_ls_rp['return_period'].unique())}\n",
    "                        return_periods_dict_for_asset_exposure = {key: 0 for key in sorted(overlay_assets_ls_rp['return_period'].unique())}\n",
    "                        rp_dict_for_asset_landslide_occur = {key: 0 for key in sorted(overlay_assets_ls_rp['return_period'].unique())}\n",
    "                    asset_geom = geom_dict[asset[0]]\n",
    "                    collect_geom.append(asset_geom.wkt)\n",
    "                    if np.max(fragility_values) == 0: #if exposure does not lead to damage\n",
    "                        collect_inb.append(np.empty(0)) #can actually be removed? \n",
    "                        overlay_inb.append(np.empty(0)) #can actually be removed? \n",
    "                    else:\n",
    "                        #collect_inb = (get_damage_per_asset_rp(asset,susc_numpified,asset_geom,hazard_intensity,fragility_values,maxdam,unit_maxdam)) #get list of damages for specific asset\n",
    "                        collect_inb, overlay_inb = get_damage_and_overlay_per_asset_rp(asset,susc_numpified,asset_geom,hazard_intensity,fragility_values,maxdam,unit_maxdam)\n",
    "                        if not len(collect_inb) == 0:\n",
    "                            for i in range(len(collect_inb[1])):\n",
    "                                return_periods_dict_for_infratype[collect_inb[1][i]] += collect_inb[0][i]\n",
    "                                return_periods_dict_for_asset[collect_inb[1][i]] += collect_inb[0][i] #for output at asset level: get damage per RP asset\n",
    "                                return_periods_dict_for_asset_exposure[overlay_inb[1][i]] += overlay_inb[0][i] #for exposure output at asset level: get exposure per RP asset\n",
    "                                rp_dict_for_asset_landslide_occur[overlay_inb[1][i]] += 1 #for exposure output at asset level: get landslide occurrence per RP asset\n",
    "                            return_periods_dict_for_asset = accumulated_damage_rp(return_periods_dict_for_asset) #for output at asset level: get accumulated RP damages for asset\n",
    "                            return_periods_dict_for_asset_exposure = accumulated_damage_rp(return_periods_dict_for_asset_exposure) #for output at asset level: get accumulated RP exposure for asset\n",
    "                            rp_dict_for_asset_landslide_occur = accumulated_damage_rp(rp_dict_for_asset_landslide_occur) #for output at asset level: get accumulated landslide occurrence for asset\n",
    "                    for rp in collect_asset_damages_per_curve_rp:\n",
    "                        asset_damage = pd.Series({asset[0]:return_periods_dict_for_asset[rp]})  # for output at asset level\n",
    "                        asset_exposure = pd.Series({asset[0]:return_periods_dict_for_asset_exposure[rp]})  # for exposure output at asset level\n",
    "                        asset_damage.columns = [infra_curve[0]]  # for output at asset level\n",
    "                        asset_exposure.columns = 'overlay'  # for exposure output at asset level\n",
    "                        collect_asset_damages_per_curve_rp[rp].append(asset_damage)  # for output at asset level\n",
    "                        collect_asset_exposure_per_curve_rp[rp].append(asset_exposure)  # for exposure output at asset level\n",
    "                        asset_landslide_occ = pd.Series({asset[0]:rp_dict_for_asset_landslide_occur[rp]})  # for # of landslides output at asset level\n",
    "                        asset_landslide_occ.columns = 'number of landslides'  # for # of landslides output at asset level\n",
    "                        collect_asset_landslides_per_curve_rp[rp].append(asset_landslide_occ)  # for # of landslides output at asset level\n",
    "                curve_ids_list.append(infra_curve[0])  # for output at asset level\n",
    "\n",
    "                #aggegated output\n",
    "                return_periods_dict_for_infratype = accumulated_damage_rp(return_periods_dict_for_infratype) #accumulate damages\n",
    "                for rp in list(return_periods_dict_for_infratype.keys()):\n",
    "                    collect_output[country_code, rp, trig_rp, sub_system, infra_type, infra_curve[0], ((maxdams[maxdams == maxdam]).index)[0]] = return_periods_dict_for_infratype[rp] #collect output for asset, infra_curve and maxdam combination                              \n",
    "    \n",
    "        #asset level output\n",
    "        for rp in collect_asset_damages_per_curve_rp:\n",
    "            if len(collect_asset_damages_per_curve_rp[rp]) != 0: \n",
    "                asset_damages_per_curve_rp = pd.concat(collect_asset_damages_per_curve_rp[rp], ignore_index=False).to_frame(name='Partial destruction (0.5)')\n",
    "                asset_exposure_per_curve_rp = pd.concat(collect_asset_exposure_per_curve_rp[rp], ignore_index=False).to_frame(name='Overlay')\n",
    "                asset_damages_per_curve_rp = asset_damages_per_curve_rp.merge(asset_exposure_per_curve_rp, left_index=True, right_index=True) #merge exposure with damages dataframe\n",
    "                asset_landslides_per_curve_rp = pd.concat(collect_asset_landslides_per_curve_rp[rp], ignore_index=False).to_frame(name='number of landslides')\n",
    "                asset_damages_per_curve_rp = asset_damages_per_curve_rp.merge(asset_landslides_per_curve_rp, left_index=True, right_index=True) #merge landslides with damages dataframe\n",
    "                #asset_damages_per_curve_rp.columns = curve_ids_list\n",
    "                damaged_assets = assets_infra_type.merge(asset_damages_per_curve_rp,left_index=True,right_index=True,how='outer')\n",
    "                damaged_assets['Overlay'] = damaged_assets['Overlay'].fillna(0)\n",
    "                damaged_assets['number of landslides'] = damaged_assets['number of landslides'].fillna(0)\n",
    "                damaged_assets = damaged_assets.drop(['buffered'],axis=1)\n",
    "                damaged_assets.crs = 3857\n",
    "                damaged_assets = damaged_assets.to_crs(4326)\n",
    "                damaged_assets[curve_ids_list] = damaged_assets[curve_ids_list].fillna(0)\n",
    "                damaged_assets['return_period_trig'] = trig_rp\n",
    "                damaged_assets['return_period_landslide'] = rp\n",
    "                save_path = pathway_dict['data_path'] / 'damage' / country_code / f'{country_code}_{hazard_type}_ls{rp}_trig{trig_rp}_{sub_system}_{infra_type}.parquet'\n",
    "                damaged_assets.to_parquet(save_path)\n",
    "\n",
    "    return collect_output\n",
    "\n",
    "def get_damage_and_overlay_per_asset_rp(asset,hazard_numpified,asset_geom,hazard_intensity,fragility_values,maxdam_asset,unit_maxdam):\n",
    "    \"\"\"\n",
    "    Calculate damage for a given asset based on hazard information.\n",
    "    Arguments:\n",
    "        *asset*: Tuple containing information about the asset. It includes:\n",
    "            - Index or identifier of the asset (asset[0]).\n",
    "            - The specific hazard points in which asset is exposed (asset[1]['hazard_point']).\n",
    "        *hazard_numpified*: NumPy array representing hazard information.\n",
    "        *asset_geom*: Shapely geometry representing the spatial coordinates of the asset.\n",
    "        *hazard_intensity*: NumPy array representing the hazard intensities of the curve for the asset type.\n",
    "        *fragility_values*: NumPy array representing the damage factors of the curve for the asset type.\n",
    "        *maxdam_asset*: Maximum damage value for asset.\n",
    "        *unit_maxdam*: The unit of maximum damage value for asset.\n",
    "    Returns:\n",
    "        *float*: The calculated damage for the specific asset.\n",
    "    \"\"\"\n",
    "     \n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = hazard_numpified[asset[1]['hazard_point'].values] \n",
    "    get_hazard_points = get_hazard_points[shapely.intersects(get_hazard_points[:,1],asset_geom)]\n",
    "    return_periods = asset[1]['return_period'].values\n",
    "    for i, (point, polygon) in enumerate(get_hazard_points):\n",
    "        get_hazard_points[i][0] = return_periods[i]\n",
    "    if type(maxdam_asset) == str: maxdam_asset = float(maxdam_asset)\n",
    "\n",
    "    # estimate damage\n",
    "    if len(get_hazard_points) == 0: # no overlay of asset with hazard\n",
    "        return np.empty(0), np.empty(0)\n",
    "    else:\n",
    "        if asset_geom.geom_type == 'LineString':\n",
    "            overlay_meters = shapely.length(shapely.intersection(get_hazard_points[:,1],asset_geom)) # get the length of exposed meters per hazard cell\n",
    "            damage = np.float16(np.full(len(get_hazard_points[:,0]), fragility_values[0])) * overlay_meters * maxdam_asset\n",
    "            return np.vstack([damage, get_hazard_points[:,0]]), np.vstack([overlay_meters, get_hazard_points[:,0]])\n",
    "\n",
    "        elif asset_geom.geom_type in ['MultiPolygon','Polygon']:\n",
    "            overlay_m2 = shapely.area(shapely.intersection(get_hazard_points[:,1],asset_geom))\n",
    "            if '/unit' in unit_maxdam:\n",
    "                converted_maxdam = maxdam_asset / shapely.area(asset_geom) #convert to maxdam/m2\n",
    "                damage = (np.float16(np.full(len(get_hazard_points[:,0]), fragility_values[0])) * overlay_m2 * converted_maxdam)\n",
    "                return np.vstack([damage, get_hazard_points[:,0]]), np.vstack([overlay_m2, get_hazard_points[:,0]])\n",
    "            else:\n",
    "                damage = (np.float16(np.full(len(get_hazard_points[:,0]), fragility_values[0])) * overlay_m2 * maxdam_asset)\n",
    "                return np.vstack([damage, get_hazard_points[:,0]]), np.vstack([overlay_m2, get_hazard_points[:,0]])\n",
    "\n",
    "def read_liquefaction_map(liquefaction_map_path,bbox,diameter_distance=0.01051720562427702239/2): #0.01083941445811754771/2):\n",
    "     \n",
    "    # load data from NetCDF file\n",
    "    with xr.open_dataset(liquefaction_map_path) as ds:\n",
    "        \n",
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        ds = ds.rio.clip_box(minx=bbox[0], miny=bbox[1], maxx=bbox[2], maxy=bbox[3])\n",
    "        \n",
    "        ds_vector = ds['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "        \n",
    "        #remove data that will not be used\n",
    "        ds_vector = ds_vector.loc[(ds_vector.band_data > 1)] #all pga's falling in very low category result in no damages\n",
    "        \n",
    "        # create geometry values and drop lat lon columns\n",
    "        ds_vector['geometry'] = [shapely.points(x) for x in list(zip(ds_vector['x'],ds_vector['y']))]\n",
    "        ds_vector = ds_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "\n",
    "        ds_vector['geometry'] = shapely.buffer(ds_vector.geometry, distance=diameter_distance, cap_style='square').values\n",
    "        \n",
    "        return ds_vector.reset_index(drop=True)\n",
    "\n",
    "def overlay_dataframes(df1,df2):\n",
    "    \"\"\"\n",
    "    Overlay a dataframe on another dataframe of spatial geometries.\n",
    "    Arguments:\n",
    "        *df1*: GeoDataFrame containing the spatial geometries. \n",
    "        *df2*: GeoDataFrame containing the spatial geometries.\n",
    "    Returns:\n",
    "        *geopandas.GeoSeries*: A GeoSeries containing the spatial geometries of df1 that intersect with df2.\n",
    "    \"\"\"\n",
    "    \n",
    "    #overlay \n",
    "    hazard_tree = shapely.STRtree(df1.geometry.values)\n",
    "    intersect_index = hazard_tree.query(df2.geometry.values, predicate='intersects')\n",
    "    intersect_index = np.unique(intersect_index[1])\n",
    "    return df1.iloc[intersect_index].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def eq_liquefaction_matrix(hazard_map,cond_map):\n",
    "    \"\"\"\n",
    "    apply earthquake and liquefaction matrix and drop irrelevant hazard cells\n",
    "    Arguments:\n",
    "        *hazard_map*: GeoDataFrame containing earthquake data. \n",
    "        *cond_map*: GeoDataFrame containing liquefaction data.\n",
    "    Returns:\n",
    "        *geopandas.DataFrame*: A dataframe containing relevant earthquake data.\n",
    "    \"\"\"\n",
    "    \n",
    "    bins = [0, 0.092, 0.18, 0.34, 0.65, float('inf')]  # Adjust the thresholds as needed\n",
    "    labels = ['1', '2', '3', '4', '5']\n",
    "    \n",
    "    # Create a new column 'classes' based on the thresholds\n",
    "    hazard_map['classes'] = pd.cut(hazard_map['band_data'], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "    \n",
    "    overlay_hazardpoints = pd.DataFrame(overlay_hazard_assets(cond_map, hazard_map).T, \n",
    "                                        columns=['hazard_point', 'cond_point']) #get df with overlays of liquefaction cells with hazards cells\n",
    "    \n",
    "    # Convert DataFrame to numpy array\n",
    "    hazard_numpified = hazard_map.to_numpy()\n",
    "    cond_numpified = cond_map.to_numpy()\n",
    "    drop_hazard_points = []\n",
    "\n",
    "    hazard_classes = hazard_numpified[:, 2]  # get haz class\n",
    "    cond_classes = cond_numpified[:, 0]      # get con class\n",
    "    \n",
    "    # Get the hazard and condition class pairs\n",
    "    for i, haz_point in tqdm(enumerate(overlay_hazardpoints['hazard_point'].unique())):\n",
    "        # Get earthquake category for hazard point\n",
    "        eq_class = hazard_classes[haz_point]\n",
    "    \n",
    "        # Get all cond_points associated with this hazard point\n",
    "        cond_points_for_hazpoint = overlay_hazardpoints[overlay_hazardpoints['hazard_point'] == haz_point]['cond_point']\n",
    "        \n",
    "        # Get the lowest cond point value for this hazard point\n",
    "        cond_point_value = cond_classes[cond_points_for_hazpoint].min()\n",
    "    \n",
    "        # Build condition to decide whether to drop the hazard point\n",
    "        if (\n",
    "            (eq_class == '1' and cond_point_value in [2, 3, 4, 5]) or\n",
    "            (eq_class == '2' and cond_point_value in [2, 3, 4]) or\n",
    "            (eq_class == '3' and cond_point_value in [2, 3]) or\n",
    "            (eq_class == '4' and cond_point_value == 2)\n",
    "        ):\n",
    "            drop_hazard_points.append(haz_point)\n",
    "    \n",
    "    return hazard_map.drop(index=drop_hazard_points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da673ad9-b62f-4b19-898e-e6428afb89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathway_dict = create_pathway_dict(data_path, flood_data_path, eq_data_path, landslide_data_path, cyclone_data_path)\n",
    "country_code= 'TJK'\n",
    "hazard_types = ['earthquake']#['fluvial'] #['landslide_rf','landslide_eq'] \n",
    "sub_system = 'road'\n",
    "\n",
    "cis_dict = {\n",
    "    \"transportation\": {\"road\": ['unclassified', 'primary', 'secondary', 'tertiary', 'residential', \n",
    "                                'trunk', 'trunk_link',  'motorway','motorway_link',  'primary_link','secondary_link', 'tertiary_link','road', 'track' ]\n",
    "}}\n",
    "\n",
    "#cis_dict = {\n",
    "#    \"transportation\": {\"road\": ['primary']\n",
    "#}}\n",
    "\n",
    "for ci_system in cis_dict: \n",
    "    for sub_system in cis_dict[ci_system]:\n",
    "        infra_type_lst = cis_dict[ci_system][sub_system]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53bf0eb-1da5-47a6-86b7-7dc4539f37a9",
   "metadata": {},
   "source": [
    "# Landslide analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef1b8dd8-5ea0-469b-a60a-68d07596af8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to extract OSM data for road\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract lines: 100%|| 83399/83399 [00:24<00:00, 3361.51it/s]\n",
      "227587it [05:39, 669.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for unclassified for earthquake using the PGA_1500y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1308/1308 [00:01<00:00, 875.30it/s]\n",
      "100%|| 1308/1308 [00:01<00:00, 884.93it/s]\n",
      "100%|| 1308/1308 [00:01<00:00, 892.44it/s]\n",
      "100%|| 1308/1308 [00:01<00:00, 884.73it/s]\n",
      "100%|| 1308/1308 [00:01<00:00, 894.91it/s]\n",
      "100%|| 1308/1308 [00:01<00:00, 875.58it/s]\n",
      "100%|| 1308/1308 [00:01<00:00, 849.88it/s]\n",
      "100%|| 1308/1308 [00:01<00:00, 884.31it/s]\n",
      "100%|| 1308/1308 [00:01<00:00, 876.12it/s]\n",
      "100%|| 1308/1308 [00:01<00:00, 880.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for primary for earthquake using the PGA_1500y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 171/171 [00:00<00:00, 794.39it/s]\n",
      "100%|| 171/171 [00:00<00:00, 724.33it/s]\n",
      "100%|| 171/171 [00:00<00:00, 712.80it/s]\n",
      "100%|| 171/171 [00:00<00:00, 736.25it/s]\n",
      "100%|| 171/171 [00:00<00:00, 725.29it/s]\n",
      "100%|| 171/171 [00:00<00:00, 783.68it/s]\n",
      "100%|| 171/171 [00:00<00:00, 726.03it/s]\n",
      "100%|| 171/171 [00:00<00:00, 723.88it/s]\n",
      "100%|| 171/171 [00:00<00:00, 725.67it/s]\n",
      "100%|| 171/171 [00:00<00:00, 725.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for secondary for earthquake using the PGA_1500y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 202/202 [00:00<00:00, 801.64it/s]\n",
      "100%|| 202/202 [00:00<00:00, 802.98it/s]\n",
      "100%|| 202/202 [00:00<00:00, 829.72it/s]\n",
      "100%|| 202/202 [00:00<00:00, 799.69it/s]\n",
      "100%|| 202/202 [00:00<00:00, 802.86it/s]\n",
      "100%|| 202/202 [00:00<00:00, 802.90it/s]\n",
      "100%|| 202/202 [00:00<00:00, 817.86it/s]\n",
      "100%|| 202/202 [00:00<00:00, 856.86it/s]\n",
      "100%|| 202/202 [00:00<00:00, 755.95it/s]\n",
      "100%|| 202/202 [00:00<00:00, 804.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for tertiary for earthquake using the PGA_1500y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 379/379 [00:00<00:00, 753.59it/s]\n",
      "100%|| 379/379 [00:00<00:00, 728.62it/s]\n",
      "100%|| 379/379 [00:00<00:00, 709.28it/s]\n",
      "100%|| 379/379 [00:00<00:00, 753.14it/s]\n",
      "100%|| 379/379 [00:00<00:00, 754.12it/s]\n",
      "100%|| 379/379 [00:00<00:00, 729.07it/s]\n",
      "100%|| 379/379 [00:00<00:00, 729.04it/s]\n",
      "100%|| 379/379 [00:00<00:00, 732.26it/s]\n",
      "100%|| 379/379 [00:00<00:00, 729.67it/s]\n",
      "100%|| 379/379 [00:00<00:00, 746.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for residential for earthquake using the PGA_1500y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7026/7026 [00:06<00:00, 1095.46it/s]\n",
      "100%|| 7026/7026 [00:06<00:00, 1134.67it/s]\n",
      "100%|| 7026/7026 [00:06<00:00, 1074.28it/s]\n",
      "100%|| 7026/7026 [00:06<00:00, 1128.51it/s]\n",
      "100%|| 7026/7026 [00:06<00:00, 1112.85it/s]\n",
      "100%|| 7026/7026 [00:06<00:00, 1125.15it/s]\n",
      "100%|| 7026/7026 [00:06<00:00, 1121.76it/s]\n",
      "100%|| 7026/7026 [00:06<00:00, 1135.41it/s]\n",
      "100%|| 7026/7026 [00:06<00:00, 1133.71it/s]\n",
      "100%|| 7026/7026 [00:06<00:00, 1105.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for trunk for earthquake using the PGA_1500y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 63/63 [00:00<00:00, 863.18it/s]\n",
      "100%|| 63/63 [00:00<00:00, 801.27it/s]\n",
      "100%|| 63/63 [00:00<00:00, 804.16it/s]\n",
      "100%|| 63/63 [00:00<00:00, 802.68it/s]\n",
      "100%|| 63/63 [00:00<00:00, 1008.46it/s]\n",
      "100%|| 63/63 [00:00<00:00, 854.44it/s]\n",
      "100%|| 63/63 [00:00<00:00, 799.85it/s]\n",
      "100%|| 63/63 [00:00<00:00, 798.93it/s]\n",
      "100%|| 63/63 [00:00<00:00, 801.08it/s]\n",
      "100%|| 63/63 [00:00<00:00, 800.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for trunk_link for earthquake using the PGA_1500y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:00<00:00, 754.54it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for motorway for earthquake using the PGA_1500y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for motorway_link for earthquake using the PGA_1500y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for primary_link for earthquake using the PGA_1500y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 11/11 [00:00<00:00, 702.34it/s]\n",
      "100%|| 11/11 [00:00<?, ?it/s]\n",
      "100%|| 11/11 [00:00<00:00, 704.11it/s]\n",
      "100%|| 11/11 [00:00<00:00, 872.62it/s]\n",
      "100%|| 11/11 [00:00<?, ?it/s]\n",
      "100%|| 11/11 [00:00<00:00, 598.46it/s]\n",
      "100%|| 11/11 [00:00<00:00, 996.38it/s]\n",
      "100%|| 11/11 [00:00<00:00, 677.89it/s]\n",
      "100%|| 11/11 [00:00<00:00, 702.91it/s]\n",
      "100%|| 11/11 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for secondary_link for earthquake using the PGA_1500y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<00:00, 254.67it/s]\n",
      "100%|| 4/4 [00:00<00:00, 257.31it/s]\n",
      "100%|| 4/4 [00:00<00:00, 882.27it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for tertiary_link for earthquake using the PGA_1500y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:00<00:00, 127.09it/s]\n",
      "100%|| 2/2 [00:00<00:00, 128.73it/s]\n",
      "100%|| 2/2 [00:00<00:00, 221.76it/s]\n",
      "100%|| 2/2 [00:00<00:00, 127.79it/s]\n",
      "100%|| 2/2 [00:00<?, ?it/s]\n",
      "100%|| 2/2 [00:00<00:00, 128.22it/s]\n",
      "100%|| 2/2 [00:00<00:00, 190.22it/s]\n",
      "100%|| 2/2 [00:00<00:00, 127.12it/s]\n",
      "100%|| 2/2 [00:00<00:00, 127.80it/s]\n",
      "100%|| 2/2 [00:00<00:00, 128.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for road for earthquake using the PGA_1500y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:00<00:00, 991.72it/s]\n",
      "100%|| 3/3 [00:00<00:00, 191.61it/s]\n",
      "100%|| 3/3 [00:00<?, ?it/s]\n",
      "100%|| 3/3 [00:00<?, ?it/s]\n",
      "100%|| 3/3 [00:00<?, ?it/s]\n",
      "100%|| 3/3 [00:00<?, ?it/s]\n",
      "100%|| 3/3 [00:00<?, ?it/s]\n",
      "100%|| 3/3 [00:00<?, ?it/s]\n",
      "100%|| 3/3 [00:00<?, ?it/s]\n",
      "100%|| 3/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for track for earthquake using the PGA_1500y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2297/2297 [00:02<00:00, 954.86it/s]\n",
      "100%|| 2297/2297 [00:02<00:00, 941.93it/s]\n",
      "100%|| 2297/2297 [00:02<00:00, 955.20it/s]\n",
      "100%|| 2297/2297 [00:02<00:00, 950.81it/s]\n",
      "100%|| 2297/2297 [00:02<00:00, 948.47it/s]\n",
      "100%|| 2297/2297 [00:02<00:00, 950.32it/s]\n",
      "100%|| 2297/2297 [00:02<00:00, 913.48it/s]\n",
      "100%|| 2297/2297 [00:02<00:00, 955.19it/s]\n",
      "100%|| 2297/2297 [00:02<00:00, 949.93it/s]\n",
      "100%|| 2297/2297 [00:02<00:00, 960.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created at: C:\\Projects\\gmhcira\\data\\damage\\TJK\\TJK_earthquake_road.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227587it [06:57, 545.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for unclassified for earthquake using the PGA_2475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2550/2550 [00:03<00:00, 770.49it/s]\n",
      "100%|| 2550/2550 [00:03<00:00, 784.01it/s]\n",
      "100%|| 2550/2550 [00:03<00:00, 767.71it/s]\n",
      "100%|| 2550/2550 [00:03<00:00, 811.75it/s]\n",
      "100%|| 2550/2550 [00:03<00:00, 842.28it/s]\n",
      "100%|| 2550/2550 [00:03<00:00, 832.10it/s]\n",
      "100%|| 2550/2550 [00:03<00:00, 798.61it/s]\n",
      "100%|| 2550/2550 [00:03<00:00, 834.89it/s]\n",
      "100%|| 2550/2550 [00:03<00:00, 838.63it/s]\n",
      "100%|| 2550/2550 [00:03<00:00, 795.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for primary for earthquake using the PGA_2475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 390/390 [00:00<00:00, 767.05it/s]\n",
      "100%|| 390/390 [00:00<00:00, 792.25it/s]\n",
      "100%|| 390/390 [00:00<00:00, 729.57it/s]\n",
      "100%|| 390/390 [00:00<00:00, 799.62it/s]\n",
      "100%|| 390/390 [00:00<00:00, 774.48it/s]\n",
      "100%|| 390/390 [00:00<00:00, 772.41it/s]\n",
      "100%|| 390/390 [00:00<00:00, 748.66it/s]\n",
      "100%|| 390/390 [00:00<00:00, 655.77it/s]\n",
      "100%|| 390/390 [00:00<00:00, 765.95it/s]\n",
      "100%|| 390/390 [00:00<00:00, 773.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for secondary for earthquake using the PGA_2475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 611/611 [00:00<00:00, 776.84it/s]\n",
      "100%|| 611/611 [00:00<00:00, 773.89it/s]\n",
      "100%|| 611/611 [00:00<00:00, 764.03it/s]\n",
      "100%|| 611/611 [00:00<00:00, 809.59it/s]\n",
      "100%|| 611/611 [00:00<00:00, 792.31it/s]\n",
      "100%|| 611/611 [00:01<00:00, 418.74it/s]\n",
      "100%|| 611/611 [00:00<00:00, 653.68it/s]\n",
      "100%|| 611/611 [00:01<00:00, 567.56it/s]\n",
      "100%|| 611/611 [00:01<00:00, 418.16it/s]\n",
      "100%|| 611/611 [00:00<00:00, 657.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for tertiary for earthquake using the PGA_2475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1137/1137 [00:01<00:00, 620.69it/s]\n",
      "100%|| 1137/1137 [00:01<00:00, 644.99it/s]\n",
      "100%|| 1137/1137 [00:01<00:00, 663.21it/s]\n",
      "100%|| 1137/1137 [00:01<00:00, 747.96it/s]\n",
      "100%|| 1137/1137 [00:01<00:00, 730.44it/s]\n",
      "100%|| 1137/1137 [00:01<00:00, 761.70it/s]\n",
      "100%|| 1137/1137 [00:01<00:00, 716.27it/s]\n",
      "100%|| 1137/1137 [00:01<00:00, 711.70it/s]\n",
      "100%|| 1137/1137 [00:01<00:00, 714.37it/s]\n",
      "100%|| 1137/1137 [00:01<00:00, 698.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for residential for earthquake using the PGA_2475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 21740/21740 [00:20<00:00, 1072.46it/s]\n",
      "100%|| 21740/21740 [00:21<00:00, 1026.87it/s]\n",
      "100%|| 21740/21740 [00:20<00:00, 1066.90it/s]\n",
      "100%|| 21740/21740 [00:20<00:00, 1063.42it/s]\n",
      "100%|| 21740/21740 [00:21<00:00, 1034.70it/s]\n",
      "100%|| 21740/21740 [00:21<00:00, 1002.78it/s]\n",
      "100%|| 21740/21740 [00:20<00:00, 1053.85it/s]\n",
      "100%|| 21740/21740 [00:21<00:00, 1021.08it/s]\n",
      "100%|| 21740/21740 [00:20<00:00, 1043.47it/s]\n",
      "100%|| 21740/21740 [00:20<00:00, 1068.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for trunk for earthquake using the PGA_2475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 295/295 [00:00<00:00, 832.30it/s]\n",
      "100%|| 295/295 [00:00<00:00, 892.12it/s]\n",
      "100%|| 295/295 [00:00<00:00, 892.44it/s]\n",
      "100%|| 295/295 [00:00<00:00, 892.17it/s]\n",
      "100%|| 295/295 [00:00<00:00, 830.97it/s]\n",
      "100%|| 295/295 [00:00<00:00, 810.99it/s]\n",
      "100%|| 295/295 [00:00<00:00, 850.50it/s]\n",
      "100%|| 295/295 [00:00<00:00, 852.51it/s]\n",
      "100%|| 295/295 [00:00<00:00, 855.01it/s]\n",
      "100%|| 295/295 [00:00<00:00, 781.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for trunk_link for earthquake using the PGA_2475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 96/96 [00:00<00:00, 1125.85it/s]\n",
      "100%|| 96/96 [00:00<00:00, 1226.39it/s]\n",
      "100%|| 96/96 [00:00<00:00, 1217.37it/s]\n",
      "100%|| 96/96 [00:00<00:00, 1009.28it/s]\n",
      "100%|| 96/96 [00:00<00:00, 1018.45it/s]\n",
      "100%|| 96/96 [00:00<00:00, 1010.78it/s]\n",
      "100%|| 96/96 [00:00<00:00, 1103.77it/s]\n",
      "100%|| 96/96 [00:00<00:00, 1016.97it/s]\n",
      "100%|| 96/96 [00:00<00:00, 1017.21it/s]\n",
      "100%|| 96/96 [00:00<00:00, 1014.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for motorway for earthquake using the PGA_2475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for motorway_link for earthquake using the PGA_2475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for primary_link for earthquake using the PGA_2475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 32/32 [00:00<00:00, 677.45it/s]\n",
      "100%|| 32/32 [00:00<00:00, 1009.83it/s]\n",
      "100%|| 32/32 [00:00<00:00, 2005.52it/s]\n",
      "100%|| 32/32 [00:00<00:00, 1005.02it/s]\n",
      "100%|| 32/32 [00:00<00:00, 2117.10it/s]\n",
      "100%|| 32/32 [00:00<00:00, 1023.36it/s]\n",
      "100%|| 32/32 [00:00<00:00, 841.30it/s]\n",
      "100%|| 32/32 [00:00<00:00, 1023.25it/s]\n",
      "100%|| 32/32 [00:00<00:00, 1023.72it/s]\n",
      "100%|| 32/32 [00:00<00:00, 1001.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for secondary_link for earthquake using the PGA_2475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 110/110 [00:00<00:00, 1165.11it/s]\n",
      "100%|| 110/110 [00:00<00:00, 1395.26it/s]\n",
      "100%|| 110/110 [00:00<00:00, 1180.74it/s]\n",
      "100%|| 110/110 [00:00<00:00, 1158.11it/s]\n",
      "100%|| 110/110 [00:00<00:00, 1168.13it/s]\n",
      "100%|| 110/110 [00:00<00:00, 934.88it/s]\n",
      "100%|| 110/110 [00:00<00:00, 1163.52it/s]\n",
      "100%|| 110/110 [00:00<00:00, 995.53it/s]\n",
      "100%|| 110/110 [00:00<00:00, 1407.55it/s]\n",
      "100%|| 110/110 [00:00<00:00, 1178.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for tertiary_link for earthquake using the PGA_2475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28/28 [00:00<00:00, 877.97it/s]\n",
      "100%|| 28/28 [00:00<00:00, 1782.51it/s]\n",
      "100%|| 28/28 [00:00<00:00, 895.90it/s]\n",
      "100%|| 28/28 [00:00<00:00, 1743.73it/s]\n",
      "100%|| 28/28 [00:00<00:00, 1780.91it/s]\n",
      "100%|| 28/28 [00:00<00:00, 896.00it/s]\n",
      "100%|| 28/28 [00:00<00:00, 1177.16it/s]\n",
      "100%|| 28/28 [00:00<00:00, 896.53it/s]\n",
      "100%|| 28/28 [00:00<00:00, 941.23it/s]\n",
      "100%|| 28/28 [00:00<00:00, 1064.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for road for earthquake using the PGA_2475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:00<00:00, 635.84it/s]\n",
      "100%|| 10/10 [00:00<00:00, 354.52it/s]\n",
      "100%|| 10/10 [00:00<00:00, 635.27it/s]\n",
      "100%|| 10/10 [00:00<00:00, 643.51it/s]\n",
      "100%|| 10/10 [00:00<00:00, 637.02it/s]\n",
      "100%|| 10/10 [00:00<?, ?it/s]\n",
      "100%|| 10/10 [00:00<00:00, 763.04it/s]\n",
      "100%|| 10/10 [00:00<?, ?it/s]\n",
      "100%|| 10/10 [00:00<00:00, 636.59it/s]\n",
      "100%|| 10/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for track for earthquake using the PGA_2475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7121/7121 [00:07<00:00, 930.18it/s]\n",
      "100%|| 7121/7121 [00:08<00:00, 804.41it/s]\n",
      "100%|| 7121/7121 [00:07<00:00, 929.01it/s]\n",
      "100%|| 7121/7121 [00:08<00:00, 886.17it/s]\n",
      "100%|| 7121/7121 [00:08<00:00, 888.54it/s]\n",
      "100%|| 7121/7121 [00:07<00:00, 933.36it/s]\n",
      "100%|| 7121/7121 [00:08<00:00, 867.86it/s]\n",
      "100%|| 7121/7121 [00:07<00:00, 912.08it/s]\n",
      "100%|| 7121/7121 [00:08<00:00, 853.70it/s]\n",
      "100%|| 7121/7121 [00:08<00:00, 884.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created at: C:\\Projects\\gmhcira\\data\\damage\\TJK\\TJK_earthquake_road.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227587it [06:37, 572.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for unclassified for earthquake using the PGA_250y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 154/154 [00:00<00:00, 652.64it/s]\n",
      "100%|| 154/154 [00:00<00:00, 815.36it/s]\n",
      "100%|| 154/154 [00:00<00:00, 753.31it/s]\n",
      "100%|| 154/154 [00:00<00:00, 615.11it/s]\n",
      "100%|| 154/154 [00:00<00:00, 700.23it/s]\n",
      "100%|| 154/154 [00:00<00:00, 609.25it/s]\n",
      "100%|| 154/154 [00:00<00:00, 751.80it/s]\n",
      "100%|| 154/154 [00:00<00:00, 754.57it/s]\n",
      "100%|| 154/154 [00:00<00:00, 753.49it/s]\n",
      "100%|| 154/154 [00:00<00:00, 761.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for primary for earthquake using the PGA_250y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 12/12 [00:00<00:00, 746.90it/s]\n",
      "100%|| 12/12 [00:00<00:00, 768.07it/s]\n",
      "100%|| 12/12 [00:00<00:00, 382.70it/s]\n",
      "100%|| 12/12 [00:00<00:00, 766.75it/s]\n",
      "100%|| 12/12 [00:00<00:00, 383.88it/s]\n",
      "100%|| 12/12 [00:00<00:00, 581.12it/s]\n",
      "100%|| 12/12 [00:00<00:00, 449.66it/s]\n",
      "100%|| 12/12 [00:00<00:00, 768.97it/s]\n",
      "100%|| 12/12 [00:00<00:00, 767.92it/s]\n",
      "100%|| 12/12 [00:00<00:00, 746.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for secondary for earthquake using the PGA_250y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 15/15 [00:00<00:00, 479.59it/s]\n",
      "100%|| 15/15 [00:00<00:00, 479.93it/s]\n",
      "100%|| 15/15 [00:00<00:00, 448.17it/s]\n",
      "100%|| 15/15 [00:00<00:00, 952.66it/s]\n",
      "100%|| 15/15 [00:00<00:00, 958.26it/s]\n",
      "100%|| 15/15 [00:00<00:00, 472.85it/s]\n",
      "100%|| 15/15 [00:00<00:00, 479.61it/s]\n",
      "100%|| 15/15 [00:00<00:00, 479.61it/s]\n",
      "100%|| 15/15 [00:00<00:00, 933.76it/s]\n",
      "100%|| 15/15 [00:00<00:00, 960.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for tertiary for earthquake using the PGA_250y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 34/34 [00:00<00:00, 540.10it/s]\n",
      "100%|| 34/34 [00:00<00:00, 718.77it/s]\n",
      "100%|| 34/34 [00:00<00:00, 543.85it/s]\n",
      "100%|| 34/34 [00:00<00:00, 539.60it/s]\n",
      "100%|| 34/34 [00:00<00:00, 539.18it/s]\n",
      "100%|| 34/34 [00:00<00:00, 450.24it/s]\n",
      "100%|| 34/34 [00:00<00:00, 543.02it/s]\n",
      "100%|| 34/34 [00:00<00:00, 540.29it/s]\n",
      "100%|| 34/34 [00:00<00:00, 540.37it/s]\n",
      "100%|| 34/34 [00:00<00:00, 542.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for residential for earthquake using the PGA_250y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 541/541 [00:00<00:00, 1079.51it/s]\n",
      "100%|| 541/541 [00:00<00:00, 1077.21it/s]\n",
      "100%|| 541/541 [00:00<00:00, 1043.92it/s]\n",
      "100%|| 541/541 [00:00<00:00, 1043.19it/s]\n",
      "100%|| 541/541 [00:00<00:00, 1006.88it/s]\n",
      "100%|| 541/541 [00:00<00:00, 1073.14it/s]\n",
      "100%|| 541/541 [00:00<00:00, 1044.00it/s]\n",
      "100%|| 541/541 [00:00<00:00, 1038.88it/s]\n",
      "100%|| 541/541 [00:00<00:00, 1042.05it/s]\n",
      "100%|| 541/541 [00:00<00:00, 1072.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for trunk for earthquake using the PGA_250y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3/3 [00:00<00:00, 192.47it/s]\n",
      "100%|| 3/3 [00:00<00:00, 353.90it/s]\n",
      "100%|| 3/3 [00:00<00:00, 170.01it/s]\n",
      "100%|| 3/3 [00:00<00:00, 166.85it/s]\n",
      "100%|| 3/3 [00:00<00:00, 1979.69it/s]\n",
      "100%|| 3/3 [00:00<00:00, 380.33it/s]\n",
      "100%|| 3/3 [00:00<00:00, 230.42it/s]\n",
      "100%|| 3/3 [00:00<00:00, 396.03it/s]\n",
      "100%|| 3/3 [00:00<00:00, 191.18it/s]\n",
      "100%|| 3/3 [00:00<00:00, 192.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for trunk_link for earthquake using the PGA_250y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for motorway for earthquake using the PGA_250y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for motorway_link for earthquake using the PGA_250y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for primary_link for earthquake using the PGA_250y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for secondary_link for earthquake using the PGA_250y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for tertiary_link for earthquake using the PGA_250y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for road for earthquake using the PGA_250y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for track for earthquake using the PGA_250y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 69/69 [00:00<00:00, 882.07it/s]\n",
      "100%|| 69/69 [00:00<00:00, 899.07it/s]\n",
      "100%|| 69/69 [00:00<00:00, 732.51it/s]\n",
      "100%|| 69/69 [00:00<00:00, 827.53it/s]\n",
      "100%|| 69/69 [00:00<00:00, 875.69it/s]\n",
      "100%|| 69/69 [00:00<00:00, 687.30it/s]\n",
      "100%|| 69/69 [00:00<00:00, 876.20it/s]\n",
      "100%|| 69/69 [00:00<00:00, 875.15it/s]\n",
      "100%|| 69/69 [00:00<00:00, 896.26it/s]\n",
      "100%|| 69/69 [00:00<00:00, 730.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created at: C:\\Projects\\gmhcira\\data\\damage\\TJK\\TJK_earthquake_road.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227587it [07:10, 529.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for unclassified for earthquake using the PGA_475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 646/646 [00:00<00:00, 841.24it/s]\n",
      "100%|| 646/646 [00:00<00:00, 701.14it/s]\n",
      "100%|| 646/646 [00:00<00:00, 821.58it/s]\n",
      "100%|| 646/646 [00:00<00:00, 875.51it/s]\n",
      "100%|| 646/646 [00:00<00:00, 873.72it/s]\n",
      "100%|| 646/646 [00:00<00:00, 789.87it/s]\n",
      "100%|| 646/646 [00:00<00:00, 850.99it/s]\n",
      "100%|| 646/646 [00:00<00:00, 857.37it/s]\n",
      "100%|| 646/646 [00:00<00:00, 857.63it/s]\n",
      "100%|| 646/646 [00:00<00:00, 771.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for primary for earthquake using the PGA_475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 63/63 [00:00<00:00, 800.01it/s]\n",
      "100%|| 63/63 [00:00<00:00, 797.93it/s]\n",
      "100%|| 63/63 [00:00<00:00, 799.35it/s]\n",
      "100%|| 63/63 [00:00<00:00, 1000.16it/s]\n",
      "100%|| 63/63 [00:00<00:00, 800.63it/s]\n",
      "100%|| 63/63 [00:00<00:00, 800.02it/s]\n",
      "100%|| 63/63 [00:00<00:00, 656.06it/s]\n",
      "100%|| 63/63 [00:00<00:00, 817.39it/s]\n",
      "100%|| 63/63 [00:00<00:00, 890.44it/s]\n",
      "100%|| 63/63 [00:00<00:00, 991.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for secondary for earthquake using the PGA_475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 72/72 [00:00<00:00, 798.01it/s]\n",
      "100%|| 72/72 [00:00<00:00, 920.22it/s]\n",
      "100%|| 72/72 [00:00<00:00, 912.41it/s]\n",
      "100%|| 72/72 [00:00<00:00, 911.58it/s]\n",
      "100%|| 72/72 [00:00<00:00, 757.79it/s]\n",
      "100%|| 72/72 [00:00<00:00, 761.87it/s]\n",
      "100%|| 72/72 [00:00<00:00, 910.94it/s]\n",
      "100%|| 72/72 [00:00<00:00, 908.17it/s]\n",
      "100%|| 72/72 [00:00<00:00, 762.98it/s]\n",
      "100%|| 72/72 [00:00<00:00, 916.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for tertiary for earthquake using the PGA_475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 164/164 [00:00<00:00, 766.96it/s]\n",
      "100%|| 164/164 [00:00<00:00, 745.10it/s]\n",
      "100%|| 164/164 [00:00<00:00, 743.09it/s]\n",
      "100%|| 164/164 [00:00<00:00, 744.64it/s]\n",
      "100%|| 164/164 [00:00<00:00, 744.91it/s]\n",
      "100%|| 164/164 [00:00<00:00, 694.65it/s]\n",
      "100%|| 164/164 [00:00<00:00, 695.39it/s]\n",
      "100%|| 164/164 [00:00<00:00, 758.21it/s]\n",
      "100%|| 164/164 [00:00<00:00, 694.33it/s]\n",
      "100%|| 164/164 [00:00<00:00, 695.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for residential for earthquake using the PGA_475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3659/3659 [00:03<00:00, 1115.90it/s]\n",
      "100%|| 3659/3659 [00:03<00:00, 1090.44it/s]\n",
      "100%|| 3659/3659 [00:03<00:00, 1107.98it/s]\n",
      "100%|| 3659/3659 [00:03<00:00, 1112.67it/s]\n",
      "100%|| 3659/3659 [00:03<00:00, 1092.08it/s]\n",
      "100%|| 3659/3659 [00:03<00:00, 1106.54it/s]\n",
      "100%|| 3659/3659 [00:03<00:00, 1103.24it/s]\n",
      "100%|| 3659/3659 [00:03<00:00, 1068.53it/s]\n",
      "100%|| 3659/3659 [00:03<00:00, 1100.45it/s]\n",
      "100%|| 3659/3659 [00:03<00:00, 1109.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for trunk for earthquake using the PGA_475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30/30 [00:00<00:00, 631.53it/s]\n",
      "100%|| 30/30 [00:00<00:00, 638.47it/s]\n",
      "100%|| 30/30 [00:00<00:00, 634.80it/s]\n",
      "100%|| 30/30 [00:00<00:00, 959.88it/s]\n",
      "100%|| 30/30 [00:00<00:00, 938.74it/s]\n",
      "100%|| 30/30 [00:00<00:00, 641.21it/s]\n",
      "100%|| 30/30 [00:00<00:00, 639.46it/s]\n",
      "100%|| 30/30 [00:00<00:00, 639.12it/s]\n",
      "100%|| 30/30 [00:00<00:00, 796.31it/s]\n",
      "100%|| 30/30 [00:00<00:00, 694.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for trunk_link for earthquake using the PGA_475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:00<00:00, 255.25it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<00:00, 234.05it/s]\n",
      "100%|| 4/4 [00:00<00:00, 275.49it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<00:00, 256.06it/s]\n",
      "100%|| 4/4 [00:00<00:00, 256.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for motorway for earthquake using the PGA_475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for motorway_link for earthquake using the PGA_475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for primary_link for earthquake using the PGA_475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:00<00:00, 257.77it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<00:00, 254.16it/s]\n",
      "100%|| 4/4 [00:00<00:00, 257.42it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<00:00, 419.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for secondary_link for earthquake using the PGA_475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<?, ?it/s]\n",
      "100%|| 1/1 [00:00<00:00, 104.31it/s]\n",
      "100%|| 1/1 [00:00<00:00, 54.49it/s]\n",
      "100%|| 1/1 [00:00<00:00, 73.50it/s]\n",
      "100%|| 1/1 [00:00<00:00, 63.52it/s]\n",
      "100%|| 1/1 [00:00<00:00, 64.33it/s]\n",
      "100%|| 1/1 [00:00<00:00, 64.05it/s]\n",
      "100%|| 1/1 [00:00<00:00, 63.64it/s]\n",
      "100%|| 1/1 [00:00<00:00, 64.39it/s]\n",
      "100%|| 1/1 [00:00<00:00, 117.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for tertiary_link for earthquake using the PGA_475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for road for earthquake using the PGA_475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for track for earthquake using the PGA_475y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1307/1307 [00:01<00:00, 737.22it/s]\n",
      "100%|| 1307/1307 [00:01<00:00, 833.85it/s]\n",
      "100%|| 1307/1307 [00:01<00:00, 880.35it/s]\n",
      "100%|| 1307/1307 [00:01<00:00, 1012.70it/s]\n",
      "100%|| 1307/1307 [00:01<00:00, 999.95it/s]\n",
      "100%|| 1307/1307 [00:01<00:00, 1013.40it/s]\n",
      "100%|| 1307/1307 [00:01<00:00, 1010.84it/s]\n",
      "100%|| 1307/1307 [00:01<00:00, 952.59it/s]\n",
      "100%|| 1307/1307 [00:01<00:00, 843.76it/s]\n",
      "100%|| 1307/1307 [00:01<00:00, 983.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created at: C:\\Projects\\gmhcira\\data\\damage\\TJK\\TJK_earthquake_road.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "227587it [07:27, 508.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for unclassified for earthquake using the PGA_975y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 727/727 [00:01<00:00, 483.75it/s]\n",
      "100%|| 727/727 [00:01<00:00, 468.43it/s]\n",
      "100%|| 727/727 [00:01<00:00, 503.87it/s]\n",
      "100%|| 727/727 [00:01<00:00, 425.72it/s]\n",
      "100%|| 727/727 [00:00<00:00, 767.13it/s]\n",
      "100%|| 727/727 [00:00<00:00, 744.86it/s]\n",
      "100%|| 727/727 [00:01<00:00, 722.04it/s]\n",
      "100%|| 727/727 [00:00<00:00, 756.75it/s]\n",
      "100%|| 727/727 [00:00<00:00, 744.55it/s]\n",
      "100%|| 727/727 [00:00<00:00, 733.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for primary for earthquake using the PGA_975y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 97/97 [00:00<00:00, 684.86it/s]\n",
      "100%|| 97/97 [00:00<00:00, 653.23it/s]\n",
      "100%|| 97/97 [00:00<00:00, 618.92it/s]\n",
      "100%|| 97/97 [00:00<00:00, 687.27it/s]\n",
      "100%|| 97/97 [00:00<00:00, 686.14it/s]\n",
      "100%|| 97/97 [00:00<00:00, 695.38it/s]\n",
      "100%|| 97/97 [00:00<00:00, 686.67it/s]\n",
      "100%|| 97/97 [00:00<00:00, 575.37it/s]\n",
      "100%|| 97/97 [00:00<00:00, 616.61it/s]\n",
      "100%|| 97/97 [00:00<00:00, 606.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for secondary for earthquake using the PGA_975y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 72/72 [00:00<00:00, 915.58it/s]\n",
      "100%|| 72/72 [00:00<00:00, 912.32it/s]\n",
      "100%|| 72/72 [00:00<00:00, 763.02it/s]\n",
      "100%|| 72/72 [00:00<00:00, 850.31it/s]\n",
      "100%|| 72/72 [00:00<00:00, 766.90it/s]\n",
      "100%|| 72/72 [00:00<00:00, 891.42it/s]\n",
      "100%|| 72/72 [00:00<00:00, 834.87it/s]\n",
      "100%|| 72/72 [00:00<00:00, 921.20it/s]\n",
      "100%|| 72/72 [00:00<00:00, 654.65it/s]\n",
      "100%|| 72/72 [00:00<00:00, 809.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for tertiary for earthquake using the PGA_975y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 208/208 [00:00<00:00, 695.01it/s]\n",
      "100%|| 208/208 [00:00<00:00, 600.77it/s]\n",
      "100%|| 208/208 [00:00<00:00, 679.66it/s]\n",
      "100%|| 208/208 [00:00<00:00, 697.83it/s]\n",
      "100%|| 208/208 [00:00<00:00, 661.85it/s]\n",
      "100%|| 208/208 [00:00<00:00, 696.67it/s]\n",
      "100%|| 208/208 [00:00<00:00, 575.06it/s]\n",
      "100%|| 208/208 [00:00<00:00, 528.36it/s]\n",
      "100%|| 208/208 [00:00<00:00, 661.62it/s]\n",
      "100%|| 208/208 [00:00<00:00, 661.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for residential for earthquake using the PGA_975y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 3781/3781 [00:03<00:00, 992.69it/s]\n",
      "100%|| 3781/3781 [00:03<00:00, 1019.92it/s]\n",
      "100%|| 3781/3781 [00:03<00:00, 988.99it/s]\n",
      "100%|| 3781/3781 [00:03<00:00, 1051.96it/s]\n",
      "100%|| 3781/3781 [00:03<00:00, 1069.42it/s]\n",
      "100%|| 3781/3781 [00:03<00:00, 1042.93it/s]\n",
      "100%|| 3781/3781 [00:03<00:00, 1023.81it/s]\n",
      "100%|| 3781/3781 [00:03<00:00, 1031.16it/s]\n",
      "100%|| 3781/3781 [00:03<00:00, 1024.39it/s]\n",
      "100%|| 3781/3781 [00:03<00:00, 1025.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for trunk for earthquake using the PGA_975y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 43/43 [00:00<00:00, 917.52it/s]\n",
      "100%|| 43/43 [00:00<00:00, 906.84it/s]\n",
      "100%|| 43/43 [00:00<00:00, 723.34it/s]\n",
      "100%|| 43/43 [00:00<00:00, 689.01it/s]\n",
      "100%|| 43/43 [00:00<00:00, 808.84it/s]\n",
      "100%|| 43/43 [00:00<00:00, 681.99it/s]\n",
      "100%|| 43/43 [00:00<00:00, 903.99it/s]\n",
      "100%|| 43/43 [00:00<00:00, 389.55it/s]\n",
      "100%|| 43/43 [00:00<00:00, 546.69it/s]\n",
      "100%|| 43/43 [00:00<00:00, 543.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for trunk_link for earthquake using the PGA_975y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<00:00, 255.56it/s]\n",
      "100%|| 4/4 [00:00<00:00, 256.34it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<00:00, 256.04it/s]\n",
      "100%|| 4/4 [00:00<00:00, 256.05it/s]\n",
      "100%|| 4/4 [00:00<00:00, 884.50it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for motorway for earthquake using the PGA_975y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for motorway_link for earthquake using the PGA_975y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for primary_link for earthquake using the PGA_975y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<00:00, 981.07it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<00:00, 255.48it/s]\n",
      "100%|| 4/4 [00:00<00:00, 258.00it/s]\n",
      "100%|| 4/4 [00:00<00:00, 664.63it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n",
      "100%|| 4/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for secondary_link for earthquake using the PGA_975y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:00<00:00, 63.87it/s]\n",
      "100%|| 1/1 [00:00<00:00, 64.02it/s]\n",
      "100%|| 1/1 [00:00<?, ?it/s]\n",
      "100%|| 1/1 [00:00<?, ?it/s]\n",
      "100%|| 1/1 [00:00<?, ?it/s]\n",
      "100%|| 1/1 [00:00<?, ?it/s]\n",
      "100%|| 1/1 [00:00<?, ?it/s]\n",
      "100%|| 1/1 [00:00<00:00, 63.81it/s]\n",
      "100%|| 1/1 [00:00<?, ?it/s]\n",
      "100%|| 1/1 [00:00<00:00, 64.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for tertiary_link for earthquake using the PGA_975y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for road for earthquake using the PGA_975y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TJK runs for track for earthquake using the PGA_975y map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1586/1586 [00:01<00:00, 923.31it/s]\n",
      "100%|| 1586/1586 [00:01<00:00, 892.68it/s]\n",
      "100%|| 1586/1586 [00:01<00:00, 873.35it/s]\n",
      "100%|| 1586/1586 [00:01<00:00, 825.84it/s]\n",
      "100%|| 1586/1586 [00:02<00:00, 787.55it/s]\n",
      "100%|| 1586/1586 [00:01<00:00, 892.32it/s]\n",
      "100%|| 1586/1586 [00:02<00:00, 640.58it/s]\n",
      "100%|| 1586/1586 [00:02<00:00, 637.12it/s]\n",
      "100%|| 1586/1586 [00:01<00:00, 861.60it/s]\n",
      "100%|| 1586/1586 [00:01<00:00, 906.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created at: C:\\Projects\\gmhcira\\data\\damage\\TJK\\TJK_earthquake_road.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#database_id_curves=True,database_maxdam=False #should be added as parameters for read_vul_maxdam \n",
    "eq_data = 'GAR' #GEM or GAR data\n",
    "\n",
    "# get country osm data\n",
    "data_loc = country_download(country_code)\n",
    "\n",
    "# get infrastructure data:\n",
    "print(f'Time to extract OSM data for {sub_system}')\n",
    "assets = extract_cis(data_loc, sub_system)\n",
    "\n",
    "# convert assets to epsg3857 (system in meters)\n",
    "assets = gpd.GeoDataFrame(assets).set_crs(4326).to_crs(3857)\n",
    "\n",
    "if sub_system == 'road':\n",
    "    assets = assets.rename(columns={'highway' : 'asset'})\n",
    "\n",
    "for hazard_type in hazard_types:\n",
    "    # read hazard data\n",
    "    hazard_data_path = pathway_dict[hazard_type]\n",
    "    data_path = pathway_dict['data_path']\n",
    "    hazard_data_list = read_hazard_data(hazard_data_path,data_path,hazard_type,country_code)\n",
    "    if hazard_type in ['pluvial','fluvial','windstorm','landslide_eq','landslide_rf']: hazard_data_list = [file for file in hazard_data_list if file.suffix == '.tif'] #put this code in read hazard data\n",
    "    \n",
    "    if hazard_type in ['windstorm','earthquake','landslide_eq','landslide_rf']:\n",
    "        # load country geometry file and create geometry to clip\n",
    "        ne_countries = gpd.read_file(data_path / \"natural_earth\" / \"ne_10m_admin_0_countries.shp\") #https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-0-countries/\n",
    "        bbox = ne_countries.loc[ne_countries['ISO_A3']==country_code].geometry.envelope.values[0].bounds\n",
    "        country_border_geometries = ne_countries.loc[ne_countries['ISO_A3']==country_code].geometry\n",
    "        \n",
    "    collect_output = {}\n",
    "    for single_footprint in hazard_data_list: #tqdm(hazard_data_list,total=len(hazard_data_list)):\n",
    "    \n",
    "        hazard_name = single_footprint.parts[-1].split('.')[0]\n",
    "        \n",
    "        # load hazard map\n",
    "        if hazard_type in ['pluvial','fluvial']:\n",
    "            hazard_map = read_flood_map(single_footprint)\n",
    "        elif hazard_type in ['windstorm']:\n",
    "             hazard_map = read_windstorm_map(single_footprint,bbox)\n",
    "        elif hazard_type == 'earthquake': \n",
    "             if eq_data == 'GAR': \n",
    "                 hazard_map = read_gar_earthquake_map(single_footprint, bbox) #GAR\n",
    "             elif eq_data == 'GEM':\n",
    "                 hazard_map = read_earthquake_map_csv(single_footprint, bbox) #GEM\n",
    "             \n",
    "             liquefaction_map_path = liquefaction_data_path / 'liquefaction_v1_deg.tif'\n",
    "             cond_map = read_liquefaction_map(liquefaction_map_path, bbox) \n",
    "             hazard_map = overlay_dataframes(hazard_map,cond_map) #get hazard polygons that overlay with cond_map\n",
    "             hazard_map = eq_liquefaction_matrix(hazard_map,cond_map) #apply liquefaction earthquake matrix and drop hazard points that are irrelevant\n",
    "        elif hazard_type in ['landslide_eq', 'landslide_rf']:\n",
    "             if hazard_type == 'landslide_eq':\n",
    "                 if eq_data == 'GAR': \n",
    "                     cond_map = read_gar_earthquake_map(single_footprint, bbox) #GAR\n",
    "                 elif eq_data == 'GEM':\n",
    "                     cond_map = read_earthquake_map(single_footprint, bbox) #GEM\n",
    "                 # Define the thresholds for the classes\n",
    "                 bins = [0, 0.05, 0.15, 0.25, 0.35, 0.45, float('inf')]  # Adjust the thresholds as needed\n",
    "                 labels = ['NaN', '1', '2', '3', '4', '5']\n",
    "                \n",
    "                 # Create a new column 'classes' based on the thresholds\n",
    "                 cond_map['cond_classes'] = pd.cut(cond_map['band_data'], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "                 \n",
    "                 #susc_map = read_susceptibility_map(single_footprint, hazard_type, bbox)\n",
    "                 susc_map = read_susceptibility_map_cropped((pathway_dict['landslide_eq'] / 'susceptibility_giri' / '{}_EQ_triggered_LS.tif'.format(country_code)))\n",
    "                 #susc_map = overlay_hazard_boundary(susc_map,country_border_geometries) #overlay with exact administrative border\n",
    "                 susc_map = overlay_hazard_boundary_temp(susc_map,country_border_geometries) #overlay with exact administrative border\n",
    "                 susc_map['geometry'] = shapely.buffer(susc_map.geometry, distance=0.0008333333333333522519/2, cap_style='square').values\n",
    "             elif hazard_type == 'landslide_rf':\n",
    "                 cond_map = read_rainfall_map(single_footprint)\n",
    "                 \n",
    "                 # Define the thresholds for the classes\n",
    "                 bins = [0, 0.3, 2.0, 3.7, 5.0, float('inf')]  # Adjust the thresholds as needed\n",
    "                 labels = ['1', '2', '3', '4', '5']\n",
    "                \n",
    "                 # Create a new column 'classes' based on the thresholds\n",
    "                 cond_map['cond_classes'] = pd.cut(cond_map['band_data'], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "                 \n",
    "                 #susc_map = read_susceptibility_map(single_footprint, hazard_type, bbox)\n",
    "                 susc_map = read_susceptibility_map_cropped((pathway_dict['landslide_rf'] / 'susceptibility_giri' / '{}_RF_triggered_LS_SSP126.tif'.format(country_code)))\n",
    "                 #susc_map = overlay_hazard_boundary(susc_map,country_border_geometries) #overlay with exact administrative border\n",
    "                 susc_map = overlay_hazard_boundary_temp(susc_map,country_border_geometries) #overlay with exact administrative border \n",
    "                 susc_map['geometry'] = shapely.buffer(susc_map.geometry, distance=0.0008333333333333522519/2, cap_style='square').values\n",
    "    \n",
    "        # convert hazard data to epsg 3857\n",
    "        if hazard_type in ['landslide_eq', 'landslide_rf']:\n",
    "            cond_map = gpd.GeoDataFrame(cond_map).set_crs(4326).to_crs(3857)\n",
    "            susc_map = gpd.GeoDataFrame(susc_map).set_crs(4326).to_crs(3857)\n",
    "        else:\n",
    "            hazard_map = gpd.GeoDataFrame(hazard_map).set_crs(4326).to_crs(3857)\n",
    "    \n",
    "        # Loop through unique infrastructure types within the subsystem\n",
    "        for infra_type in infra_type_lst: \n",
    "            assets_infra_type = assets[assets['asset'] == infra_type].copy().reset_index(drop=True)\n",
    "        \n",
    "            # create dicts for quicker lookup\n",
    "            geom_dict = assets_infra_type['geometry'].to_dict()\n",
    "            type_dict = assets_infra_type['asset'].to_dict()\n",
    "    \n",
    "            ## read vulnerability and maxdam data:\n",
    "            infra_curves,maxdams,infra_units = read_vul_maxdam(data_path,hazard_type, infra_type)\n",
    "    \n",
    "            # start analysis \n",
    "            print(f'{country_code} runs for {infra_type} for {hazard_type} using the {hazard_name} map')# for {len(infra_curves.T)*len(maxdams)} combinations')\n",
    "    \n",
    "            if hazard_type in ['landslide_eq', 'landslide_rf']:\n",
    "                if not assets_infra_type.empty:\n",
    "                    # overlay assets\n",
    "                    overlay_assets = pd.DataFrame(overlay_hazard_assets(susc_map,buffer_assets(assets_infra_type)).T,columns=['asset','hazard_point'])\n",
    "                else: \n",
    "                    overlay_assets = pd.DataFrame(columns=['asset','hazard_point']) #empty dataframe\n",
    "                \n",
    "                # convert dataframe to numpy array\n",
    "                susc_numpified = susc_map.to_numpy()\n",
    "    \n",
    "                #apply hazard x susceptibility matrix\n",
    "                overlay_assets['return_period'] = pd.Series(dtype='int')\n",
    "                overlay_assets['return_period_trig'] = pd.Series(dtype='int')\n",
    "                for susc_point in tqdm(overlay_assets.groupby('hazard_point'),total=len(overlay_assets.hazard_point.unique())):\n",
    "                    get_susc_data = susc_numpified[susc_point[0]] # get susc classes and coordinates\n",
    "                    overlay_cond = cond_map[shapely.intersects(cond_map['geometry'],get_susc_data[1])] #overlay earthquake map with single susc geom \n",
    "                    #put return period in overlay_assets\n",
    "                    if not overlay_cond.empty:\n",
    "                        if hazard_type == 'landslide_eq':\n",
    "                            overlay_assets = matrix_landslide_eq_susc(overlay_cond, get_susc_data, overlay_assets, susc_point) \n",
    "                        elif hazard_type == 'landslide_rf':\n",
    "                            overlay_assets = matrix_landslide_rf_susc(overlay_cond, get_susc_data, overlay_assets, susc_point)\n",
    "                    else:\n",
    "                        overlay_assets = overlay_assets.drop((overlay_assets[overlay_assets['hazard_point'] == susc_point[0]]).index) #delete susc from overlay_assets\n",
    "    \n",
    "                #run and output damage calculations for landslides\n",
    "                if not assets_infra_type.empty:\n",
    "                    if assets_infra_type['geometry'][0].geom_type == 'LineString':\n",
    "                        collect_output = landslide_damage_and_overlay(overlay_assets,infra_curves,susc_numpified,assets_infra_type)\n",
    "                    else:\n",
    "                        collect_output = landslide_damage(overlay_assets,infra_curves,susc_numpified,assets_infra_type)\n",
    "            \n",
    "            elif hazard_type in ['earthquake', 'pluvial', 'fluvial']: #other hazard\n",
    "                if not assets_infra_type.empty:\n",
    "                    # overlay assets\n",
    "                    overlay_assets = pd.DataFrame(overlay_hazard_assets(hazard_map,buffer_assets(assets_infra_type)).T,columns=['asset','hazard_point'])\n",
    "                else: \n",
    "                    overlay_assets = pd.DataFrame(columns=['asset','hazard_point']) #empty dataframe\n",
    "        \n",
    "                # convert dataframe to numpy array\n",
    "                hazard_numpified = hazard_map.to_numpy()\n",
    "    \n",
    "                collect_asset_damages_per_curve = [] # for output at asset level\n",
    "                #collect_asset_exposure_per_curve = [] # for exposure output at asset level\n",
    "                curve_ids_list = [] # for output at asset level\n",
    "                for infra_curve in infra_curves:\n",
    "                    # get curves\n",
    "                    curve = infra_curves[infra_curve[0]]\n",
    "                    hazard_intensity = curve.index.values\n",
    "                    fragility_values = (np.nan_to_num(curve.values,nan=(np.nanmax(curve.values)))).flatten()\n",
    "    \n",
    "                    for maxdam in maxdams:\n",
    "                        collect_inb = []\n",
    "                        collect_geom = []\n",
    "                        unit_maxdam = infra_units[maxdams[maxdams == maxdam].index[0]] #get unit maxdam\n",
    "    \n",
    "                        collect_damage_asset = {}  # for output at asset level\n",
    "                        collect_overlay_asset = {}  # for exposure output at asset level\n",
    "                        for asset in tqdm(overlay_assets.groupby('asset'),total=len(overlay_assets.asset.unique())): #group asset items for different hazard points per asset and get total number of unique assets\n",
    "                            asset_geom = geom_dict[asset[0]]\n",
    "                            collect_geom.append(asset_geom.wkt)\n",
    "                            if np.max(fragility_values) == 0: #if exposure does not lead to damage\n",
    "                                collect_inb.append(0)  \n",
    "                            else:\n",
    "                                #collect_inb.append(get_damage_per_asset_og(asset,hazard_numpified,asset_geom,hazard_intensity,fragility_values,maxdam))\n",
    "                                collect_inb.append(get_damage_per_asset(asset,hazard_numpified,asset_geom,hazard_intensity,fragility_values,maxdam,unit_maxdam)) #get list of damages for specific asset\n",
    "                                #collect_damage_asset[asset[0]] = get_damage_per_asset(asset,hazard_numpified,asset_geom,hazard_intensity,fragility_values,maxdam,unit_maxdam) #for output at asset level\n",
    "                                damage_asset, overlay_asset = get_damage_per_asset_and_overlay(asset,hazard_numpified,asset_geom,hazard_intensity,fragility_values,maxdam,unit_maxdam) #for output at asset level\n",
    "                                collect_damage_asset[asset[0]] = damage_asset # for output at asset level\n",
    "                                collect_overlay_asset[asset[0]] = overlay_asset # for exposure output at asset level\n",
    "                        \n",
    "                        collect_output[country_code, hazard_name, sub_system, infra_type, infra_curve[0], ((maxdams[maxdams == maxdam]).index)[0]] = np.sum(collect_inb) #, collect_geom # dictionary to store results for various combinations of hazard maps, infrastructure curves, and maximum damage values.\n",
    "                        asset_damage = pd.Series(collect_damage_asset)  # for output at asset level\n",
    "                        asset_damage.columns = [infra_curve[0]]  # for output at asset level\n",
    "                        collect_asset_damages_per_curve.append(asset_damage)  # for output at asset level\n",
    "                        asset_exposure = pd.Series(collect_overlay_asset)  # for exposure output at asset level\n",
    "                        asset_exposure.columns = 'overlay'  # for exposure output at asset level\n",
    "                        #collect_asset_exposure_per_curve.append(asset_exposure)  # for exposure output at asset level\n",
    "                    curve_ids_list.append(infra_curve[0])  # for output at asset level\n",
    "    \n",
    "                if collect_asset_damages_per_curve[0].empty == False: #collect_asset_damages_per_curve.empty == False\n",
    "                    asset_damages_per_curve = pd.concat(collect_asset_damages_per_curve,axis=1)\n",
    "                    asset_damages_per_curve.columns = curve_ids_list\n",
    "                    asset_damages_per_curve = asset_damages_per_curve.merge(asset_exposure.rename('overlay'), left_index=True, right_index=True) #merge exposure with damages dataframe\n",
    "                    damaged_assets = assets_infra_type.merge(asset_damages_per_curve,left_index=True,right_index=True,how='outer')\n",
    "                    damaged_assets = damaged_assets.drop(['buffered'],axis=1)\n",
    "                    damaged_assets.crs = 3857\n",
    "                    damaged_assets = damaged_assets.to_crs(4326)\n",
    "                    damaged_assets[curve_ids_list] = damaged_assets[curve_ids_list].fillna(0)\n",
    "                    save_path = pathway_dict['data_path'] / 'damage' / country_code / f'{country_code}_{hazard_type}_{hazard_name}_{sub_system}_{infra_type}.parquet'\n",
    "                    damaged_assets.to_parquet(save_path)\n",
    "\n",
    "            #break #delete after testing, otherwise damage will only be assessed for first hazard map\n",
    "    \n",
    "        #create_damage_csv(collect_output, hazard_type, pathway_dict, country_code, sub_system) #with exposure\n",
    "        if hazard_type in ['landslide_eq', 'landslide_rf']:\n",
    "            create_damage_csv_without_exposure(collect_output, hazard_type, pathway_dict, country_code, sub_system) \n",
    "        else:\n",
    "            create_damage_csv_without_exposure(collect_output, hazard_type, pathway_dict, country_code, sub_system) #check whether this line should be moved to the left (i.e., Excel overwriting is the case now??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b566d-6c46-4b4e-9643-7d91292c5fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
