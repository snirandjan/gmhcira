{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2674e289-cc8d-4452-8581-dc1ae337b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np \n",
    "import shapely \n",
    "import csv\n",
    "\n",
    "import osm_flex.download as dl\n",
    "import osm_flex.extract as ex\n",
    "from osm_flex.simplify import remove_contained_points,remove_exact_duplicates\n",
    "from osm_flex.config import OSM_DATA_DIR,DICT_GEOFABRIK\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lonboard import viz\n",
    "from lonboard.colormap import apply_continuous_cmap\n",
    "from palettable.colorbrewer.sequential import Blues_9\n",
    "\n",
    "from pathlib import Path\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a396602-2c4c-40b1-be4d-6fe9e4f3dce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define paths\n",
    "p = Path('..')\n",
    "data_path = Path(pathlib.Path.home().parts[0]) / 'Projects' / 'gmhcira' / 'data' #should contain folder 'Vulnerability' with vulnerability data\n",
    "flood_data_path = Path('//labsdfs.labs.vu.nl/labsdfs/BETA-IVM-BAZIS/eks510/fathom-global') # Flood data\n",
    "eq_data_path = Path('//labsdfs.labs.vu.nl/data_catalogue/open_street_map/global_hazards/earthquakes/GEM')  # Earthquake data\n",
    "landslide_data_path = Path('//labsdfs.labs.vu.nl/data_catalogue/open_street_map/global_hazards/landslides')  # Landslide data\n",
    "cyclone_data_path = Path('//labsdfs.labs.vu.nl/data_catalogue/open_street_map/global_hazards/tropical_cyclones')  # Cyclone data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "794dd7c4-3c68-44eb-b6ab-db5627bab45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_download(iso3):\n",
    "    \"\"\"\n",
    "    Download OpenStreetMap data for a specific country.\n",
    "    Arguments:\n",
    "        *iso3* (str): ISO 3166-1 alpha-3 country code.\n",
    "    Returns:\n",
    "        *Path*: The file path of the downloaded OpenStreetMap data file.\n",
    "    \"\"\"\n",
    "    \n",
    "    dl.get_country_geofabrik(iso3) # Use the download library to get the geofabrik data for the specified country\n",
    "    data_loc = OSM_DATA_DIR.joinpath(f'{DICT_GEOFABRIK[iso3][1]}-latest.osm.pbf') # Specify the location of the OpenStreetMap (OSM) data file\n",
    "    return data_loc\n",
    "\n",
    "def overlay_hazard_assets(df_ds,assets):\n",
    "    \"\"\"\n",
    "    Overlay hazard assets on a dataframe of spatial geometries.\n",
    "    Arguments:\n",
    "        *df_ds*: GeoDataFrame containing the spatial geometries of the hazard data. \n",
    "        *assets*: GeoDataFrame containing the infrastructure assets.\n",
    "    Returns:\n",
    "        *geopandas.GeoSeries*: A GeoSeries containing the spatial geometries of df_ds that intersect with the infrastructure assets.\n",
    "    \"\"\"\n",
    "    \n",
    "    #overlay \n",
    "    hazard_tree = shapely.STRtree(df_ds.geometry.values)\n",
    "    if (shapely.get_type_id(assets.iloc[0].geometry) == 3) | (shapely.get_type_id(assets.iloc[0].geometry) == 6): # id types 3 and 6 stand for polygon and multipolygon\n",
    "        return  hazard_tree.query(assets.geometry,predicate='intersects')    \n",
    "    else:\n",
    "        return  hazard_tree.query(assets.buffered,predicate='intersects')\n",
    "\n",
    "def buffer_assets(assets,buffer_size=0.00083):\n",
    "    \"\"\"\n",
    "    Buffer spatial assets in a GeoDataFrame.\n",
    "    Arguments:\n",
    "        *assets*: GeoDataFrame containing spatial geometries to be buffered.\n",
    "        *buffer_size* (float, optional): The distance by which to buffer the geometries. Default is 0.00083.\n",
    "    Returns:\n",
    "        *GeoDataFrame*: A new GeoDataFrame with an additional 'buffered' column containing the buffered geometries.\n",
    "    \"\"\"\n",
    "    assets['buffered'] = shapely.buffer(assets.geometry.values,distance=buffer_size)\n",
    "    return assets\n",
    "\n",
    "def get_damage_per_asset(asset,hazard_numpified,asset_geom,hazard_intensity,fragility_values,maxdam_asset):\n",
    "    \"\"\"\n",
    "    Calculate damage for a given asset based on hazard information.\n",
    "    Arguments:\n",
    "        *asset*: Tuple containing information about the asset. It includes:\n",
    "            - Index or identifier of the asset (asset[0]).\n",
    "            - The specific hazard points in which asset is exposed (asset[1]['hazard_point']).\n",
    "        *hazard_numpified*: NumPy array representing hazard information.\n",
    "        *asset_geom*: Shapely geometry representing the spatial coordinates of the asset.\n",
    "        *hazard_intensity*: NumPy array representing the hazard intensities of the curve for the asset type.\n",
    "        *fragility_values*: NumPy array representing the damage factors of the curve for the asset type.\n",
    "        *maxdam_asset*: Maximum damage value for asset.\n",
    "    Returns:\n",
    "        *float*: The calculated damage for the specific asset.\n",
    "    \"\"\"\n",
    "     \n",
    "    # find the exact hazard overlays:\n",
    "    get_hazard_points = hazard_numpified[asset[1]['hazard_point'].values] \n",
    "    get_hazard_points[shapely.intersects(get_hazard_points[:,1],asset_geom)]\n",
    "\n",
    "    # estimate damage\n",
    "    if len(get_hazard_points) == 0: # no overlay of asset with hazard\n",
    "        return 0\n",
    "    \n",
    "    else:\n",
    "        if asset_geom.geom_type == 'LineString':\n",
    "            overlay_meters = shapely.length(shapely.intersection(get_hazard_points[:,1],asset_geom)) # get the length of exposed meters per hazard cell\n",
    "            return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_meters*maxdam_asset) #return asset number, total damage for asset number (damage factor * meters * max. damage)\n",
    "        elif asset_geom.geom_type in ['MultiPolygon','Polygon']:\n",
    "            overlay_m2 = shapely.area(shapely.intersection(get_hazard_points[:,1],asset_geom))\n",
    "            return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*overlay_m2*maxdam_asset) #return asset number, total damage for asset number (damage factor * meters * max. damage)\n",
    "        elif asset_geom.geom_type == 'Point':\n",
    "            return np.sum((np.interp(np.float16(get_hazard_points[:,0]),hazard_intensity,fragility_values))*maxdam_asset)\n",
    "\n",
    "def create_pathway_dict(data_path, flood_data_path, eq_data_path, landslide_data_path, cyclone_data_path): \n",
    "\n",
    "    \"\"\"\n",
    "    Create a dictionary containing paths to various hazard datasets.\n",
    "    Arguments:\n",
    "        *data_path* (Path): Base directory path for general data.\n",
    "        *flood_data_path* (Path): Path to flood hazard data.\n",
    "        *eq_data_path* (Path): Path to earthquake hazard data.\n",
    "        *landslide_data_path* (Path): Path to landslide hazard data.\n",
    "        *cyclone_data_path* (Path): Path to tropical cyclone hazard data.\n",
    "    Returns:\n",
    "        *dict*: A dictionary where keys represent a general pathway and different hazard types and values are corresponding paths.\n",
    "    \"\"\"\n",
    "\n",
    "    #create a dictionary\n",
    "    pathway_dict = {'data_path': data_path, \n",
    "                    'fluvial': flood_data_path, \n",
    "                    'pluvial': flood_data_path, \n",
    "                    'windstorm': cyclone_data_path, \n",
    "                    'earthquake': eq_data_path, \n",
    "                    'landslides': landslide_data_path,}\n",
    "\n",
    "    return pathway_dict\n",
    "\n",
    "def read_hazard_data(hazard_data_path,hazard_type):\n",
    "    \"\"\"\n",
    "    Read hazard data files for a specific hazard type.\n",
    "    Arguments:\n",
    "        *hazard_data_path* (Path): Base directory path where hazard data is stored.\n",
    "        *hazard_type* (str): Type of hazard for which data needs to be read ('fluvial', 'pluvial', 'windstorm', 'earthquake', 'landslides').\n",
    "    \n",
    "    Returns:\n",
    "        *list*: A list of Path objects representing individual hazard data files for the specified hazard type.\n",
    "    \"\"\"  \n",
    "\n",
    "    if hazard_type == 'fluvial':\n",
    "        hazard_data = hazard_data_path / 'Jamaica' / 'fluvial_undefended' # need to make country an input\n",
    "        return list(hazard_data.iterdir())\n",
    "\n",
    "    elif hazard_type == 'pluvial':\n",
    "        hazard_data = hazard_data_path / 'Jamaica' / 'pluvial' # need to make country an input\n",
    "        return list(hazard_data.iterdir())\n",
    "\n",
    "    elif hazard_type == 'windstorm':\n",
    "        hazard_data = hazard_data_path \n",
    "        return list(hazard_data.iterdir())\n",
    "\n",
    "    elif hazard_type == 'earthquake':\n",
    "        hazard_data = hazard_data_path\n",
    "        return list(hazard_data.iterdir())\n",
    "\n",
    "    elif hazard_type == 'landslides':\n",
    "        hazard_data = hazard_data_path \n",
    "        return list(hazard_data.iterdir())\n",
    "\n",
    "\n",
    "def read_vul_maxdam(data_path,hazard_type,sub_system):\n",
    "    \"\"\"\n",
    "    Read vulnerability curves and maximum damage data for a specific hazard and infrastructure type.\n",
    "    Arguments:\n",
    "        *data_path*: The base directory path where vulnerability and maximum damage data files are stored.\n",
    "        *hazard_type*: The type of hazard in string format, such as 'pluvial', 'fluvial', or 'windstorm'.\n",
    "        *sub_system*: The type of infrastructure in string format for which vulnerability curves and maximum damage data are needed.\n",
    "    \n",
    "    Returns:\n",
    "        *tuple*: A tuple containing two DataFrames:\n",
    "            - The first DataFrame contains vulnerability curves specific to the given hazard and infrastructure type.\n",
    "            - The second DataFrame contains maximum damage data for the specified infrastructure type.\n",
    "    \"\"\"\n",
    "\n",
    "    vul_data = data_path / 'Vulnerability'\n",
    "\n",
    "    if hazard_type in ['pluvial','fluvial']:  \n",
    "        curves = pd.read_excel(vul_data / 'Table_D2_Multi-Hazard_Fragility_and_Vulnerability_Curves_V1.0.0.xlsx',sheet_name = 'F_Vuln_Depth',index_col=[0],header=[0,1,2,3,4])\n",
    "    elif hazard_type == 'windstorm':\n",
    "        curves = pd.read_excel(vul_data / 'Table_D2_Multi-Hazard_Fragility_and_Vulnerability_Curves_V1.0.0.xlsx',sheet_name = 'W_Vuln_V10m',index_col=[0],header=[0,1,2,3,4])\n",
    "\n",
    "    infra_curves =  curves.loc[:, curves.columns.get_level_values('Infrastructure description').str.lower().str.contains(sub_system)]\n",
    "    \n",
    "    maxdam = pd.read_excel(vul_data / 'Table_D3_Costs_V1.0.0.xlsx',sheet_name='Cost_Database',index_col=[0])\n",
    "    infra_maxdam = maxdam.loc[maxdam.index.get_level_values('Infrastructure description').str.lower().str.contains(sub_system),'Amount'].dropna()\n",
    "    infra_maxdam = infra_maxdam[pd.to_numeric(infra_maxdam, errors='coerce').notnull()]\n",
    "\n",
    "    return infra_curves,infra_maxdam\n",
    "\n",
    "def read_flood_map(flood_map_path,diameter_distance=0.00083/2):\n",
    "    \"\"\"\n",
    "    Read flood map data from a NetCDF file and process it into a GeoDataFrame.\n",
    "    Arguments:\n",
    "        *flood_map_path* (Path): Path to the NetCDF file containing flood map data.\n",
    "        *diameter_distance* (float, optional): The diameter distance used for creating square geometries around data points. Default is 0.00083/2.\n",
    "    \n",
    "    Returns:\n",
    "        *geopandas.GeoDataFrame*: A GeoDataFrame representing the processed flood map data.\n",
    "    \"\"\"\n",
    "    \n",
    "    flood_map = xr.open_dataset(flood_map_path, engine=\"rasterio\")\n",
    "\n",
    "    flood_map_vector = flood_map['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "    \n",
    "    #remove data that will not be used\n",
    "    flood_map_vector = flood_map_vector.loc[(flood_map_vector.band_data > 0) & (flood_map_vector.band_data < 100)]\n",
    "    \n",
    "    # create geometry values and drop lat lon columns\n",
    "    flood_map_vector['geometry'] = [shapely.points(x) for x in list(zip(flood_map_vector['x'],flood_map_vector['y']))]\n",
    "    flood_map_vector = flood_map_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "    \n",
    "    # drop all non values to reduce size\n",
    "    flood_map_vector = flood_map_vector.loc[~flood_map_vector['band_data'].isna()].reset_index(drop=True)\n",
    "    \n",
    "    # and turn them into squares again:\n",
    "    flood_map_vector.geometry= shapely.buffer(flood_map_vector.geometry,distance=diameter_distance,cap_style='square').values \n",
    "\n",
    "    return flood_map_vector\n",
    "\n",
    "def read_windstorm_map(windstorm_map_path,bbox):\n",
    "     \n",
    "    # load data from NetCDF file\n",
    "    with xr.open_dataset(flood_map_path) as ds:\n",
    "        \n",
    "        # convert data to WGS84 CRS\n",
    "        ds.rio.write_crs(4326, inplace=True)\n",
    "        ds = ds.rio.clip_box(minx=bbox[0], miny=bbox[1], maxx=bbox[2], maxy=bbox[3])\n",
    "        #ds['band_data'] = ds['band_data']/0.88*1.11 #convert 10-min sustained wind speed to 3-s gust wind speed\n",
    "    \n",
    "        ds_vector = ds['band_data'].to_dataframe().reset_index() #transform to dataframe\n",
    "        \n",
    "        #remove data that will not be used\n",
    "        ds_vector = ds_vector.loc[(ds_vector.band_data > 0) & (ds_vector.band_data < 100)]\n",
    "        \n",
    "        # create geometry values and drop lat lon columns\n",
    "        ds_vector['geometry'] = [shapely.points(x) for x in list(zip(ds_vector['x'],ds_vector['y']))]\n",
    "        ds_vector = ds_vector.drop(['x','y','band','spatial_ref'],axis=1)\n",
    "        ds_vector['geometry'] = shapely.buffer(ds_vector.geometry, distance=0.1/2, cap_style='square').values\n",
    "    \n",
    "        return ds_vector\n",
    "\n",
    "def create_damage_csv(damage_output, hazard_type, pathway_dict, country_code, sub_system):\n",
    "    \"\"\"\n",
    "    Create a CSV file containing damage information.\n",
    "    Arguments:\n",
    "        damage_output: A dictionary containing damage information.\n",
    "        hazard_type: The type of hazard (e.g., 'earthquake', 'flood').\n",
    "        pathway_dict: A dictionary containing file paths for different data.\n",
    "        country_code: A string containing information about the country code\n",
    "        sub_system: A string containing information about the subsystem considered\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "  \n",
    "    hazard_output_path = pathway_dict['data_path'] / 'damage' / country_code\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not hazard_output_path.exists():\n",
    "        # Create the directory\n",
    "        hazard_output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    csv_file_path = hazard_output_path / '{}_{}_{}.csv'.format(country_code, hazard_type, sub_system)\n",
    "    \n",
    "    with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        \n",
    "        # Write header\n",
    "        csv_writer.writerow(['Country', 'Return period', 'Subsystem', 'Infrastructure type', 'Curve ID number', 'Damage ID number', 'Damage'])\n",
    "        \n",
    "        # Write data\n",
    "        for key, value in damage_output.items():\n",
    "            csv_writer.writerow(list(key) + [value])\n",
    "    \n",
    "    print(f\"CSV file created at: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4807dd0e-9c0a-4651-bff4-efdfc27f639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_infrastructure_hazard(pathway_dict, country_code, sub_system, infra_type_lst, hazard_type):\n",
    "\n",
    "    # get country osm data\n",
    "    data_loc = country_download(country_code)\n",
    "    \n",
    "    # get infrastructure data:\n",
    "    assets = ex.extract_cis(data_loc, sub_system)\n",
    "    \n",
    "    # convert assets to epsg3857 (system in meters)\n",
    "    assets = gpd.GeoDataFrame(assets).set_crs(4326).to_crs(3857)\n",
    "    \n",
    "    if sub_system == 'road':\n",
    "        assets = assets.loc[assets.geometry.geom_type == 'LineString']\n",
    "        assets = assets.rename(columns={'highway' : 'asset'})\n",
    "        list_of_highway_assets_to_keep =[\"living_street\", \"motorway\", \"motorway_link\", \"primary\",\"primary_link\", \"residential\",\"road\", \"secondary\", \"secondary_link\",\"tertiary\",\"tertiary_link\", \"trunk\", \"trunk_link\",\"unclassified\",\"service\"]\n",
    "        #reclassify assets \n",
    "        mapping_dict = {\n",
    "            \"living_street\" : \"tertiary\", \n",
    "            \"motorway\" : \"primary\", \n",
    "            \"motorway_link\" : \"primary\", \n",
    "            \"primary\" : \"primary\", \n",
    "            \"primary_link\" : \"primary\", \n",
    "            \"residential\" : \"tertiary\",\n",
    "            \"road\" : \"secondary\", \n",
    "            \"secondary\" : \"secondary\", \n",
    "            \"secondary_link\" : \"secondary\", \n",
    "            \"tertiary\" : \"tertiary\", \n",
    "            \"tertiary_link\" : \"tertiary\", \n",
    "            \"trunk\" : \"primary\",\n",
    "            \"trunk_link\" : \"primary\",\n",
    "            \"unclassified\" : \"tertiary\", \n",
    "            \"service\" : \"tertiary\"\n",
    "        }\n",
    "        \n",
    "        assets['asset'] = assets.asset.apply(lambda x : mapping_dict[x])  #reclassification\n",
    "    elif sub_system == 'rail':\n",
    "        assets = assets.loc[assets.geometry.geom_type == 'LineString']\n",
    "        assets = assets.rename(columns={'railway' : 'asset'})\n",
    "    elif sub_system == 'education':\n",
    "        assets = assets.rename(columns={'building' : 'asset'})\n",
    "        assets = assets.reset_index(drop=True)\n",
    "        assets = remove_contained_points(assets)\n",
    "\n",
    "        #convert points to polygons\n",
    "        assets.loc[assets.geom_type == 'Point','geometry'] = assets.loc[assets.geom_type == 'Point'].buffer(\n",
    "                                                                        distance=np.sqrt(assets.loc[assets.geom_type == 'MultiPolygon'].area.median())/2, cap_style='square')\n",
    "\n",
    "    elif sub_system == 'air':\n",
    "        assets = assets.rename(columns={'aeroway' : 'asset'})\n",
    "\n",
    "    \n",
    "    # read hazard data\n",
    "    hazard_data_path = pathway_dict[hazard_type]\n",
    "    hazard_data_list = read_hazard_data(hazard_data_path,hazard_type)\n",
    "    \n",
    "    # read vulnerability and maxdam data:\n",
    "    data_path = pathway_dict['data_path']\n",
    "    infra_curves,maxdams = read_vul_maxdam(data_path,hazard_type, sub_system)\n",
    "\n",
    "    # start analysis \n",
    "    print(f'{country_code} runs for {sub_system} for {hazard_type} for {len(hazard_data_list)} maps')\n",
    "\n",
    "\n",
    "    if hazard_type in ['windstorm','earthquake','landslide']:\n",
    "        # load country geometry file and create geometry to clip\n",
    "        ne_countries = gpd.read_file(data_path / \"natural_earth\" / \"ne_10m_admin_0_countries.shp\") #https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-0-countries/\n",
    "        bbox = ne_countries.loc[ne_countries['ISO_A3']==country_code].geometry.envelope.values[0].bounds\n",
    "        \n",
    "    collect_output = {}\n",
    "    for single_footprint in hazard_data_list: #tqdm(hazard_data_list,total=len(hazard_data_list)):\n",
    "    \n",
    "        hazard_name = single_footprint.parts[-1].split('.')[0]\n",
    "        \n",
    "        # load hazard map\n",
    "        if hazard_type in ['pluvial','fluvial']:\n",
    "            hazard_map = read_flood_map(single_footprint)\n",
    "        elif hazard_type in ['windstorm']:\n",
    "             hazard_map = read_windstorm_map(single_footprint,bbox)\n",
    "        elif hazard_type in ['earthquake']:\n",
    "             hazard_map = read_earthquake_map(single_footprint)\n",
    "        elif hazard_type in ['landslide']:\n",
    "             hazard_map = read_landslide_map(single_footprint)\n",
    "         \n",
    "        # convert hazard data to epsg 3857\n",
    "        hazard_map = gpd.GeoDataFrame(hazard_map).set_crs(4326).to_crs(3857)\n",
    "\n",
    "        # Loop through unique infrastructure types within the subsystem\n",
    "        for infra_type in infra_type_lst: \n",
    "            assets_infra_type = assets[assets['asset'] == infra_type].copy().reset_index(drop=True)\n",
    "        \n",
    "            # create dicts for quicker lookup\n",
    "            geom_dict = assets_infra_type['geometry'].to_dict()\n",
    "            type_dict = assets_infra_type['asset'].to_dict()\n",
    "\n",
    "            # start analysis \n",
    "            print(f'{country_code} runs for {infra_type} for {hazard_type} for {hazard_name} map for {len(infra_curves.T)*len(maxdams)} combinations')\n",
    "    \n",
    "            # overlay assets\n",
    "            overlay_assets = pd.DataFrame(overlay_hazard_assets(hazard_map,buffer_assets(assets_infra_type)).T,columns=['asset','hazard_point'])\n",
    "    \n",
    "            # convert dataframe to numpy array\n",
    "            hazard_numpified = hazard_map.to_numpy() \n",
    "    \n",
    "            #specify curves and max. damages that can be used for infrastructure type\n",
    "            curve_types = {'primary': ['F7.1', 'F7.2'], # !complete code!\n",
    "                          'secondary': ['F7.3', 'F7.4']} # !complete code!\n",
    "    \n",
    "            maxdam_types = {} # !complete code!\n",
    "\n",
    "            for infra_curve in infra_curves.iloc[:, 0:1]:\n",
    "                # get curves\n",
    "                curve = infra_curves[infra_curve[0]]\n",
    "                hazard_intensity = curve.index.values\n",
    "                fragility_values = (np.nan_to_num(curve.values,nan=(np.nanmax(curve.values)))).flatten()\n",
    "                        \n",
    "                for maxdam in maxdams.iloc[1:2]:\n",
    "                    \n",
    "                    collect_inb = []\n",
    "                    for asset in tqdm(overlay_assets.groupby('asset'),total=len(overlay_assets.asset.unique())): #group asset items for different hazard points per asset and get total number of unique assets\n",
    "                        if np.max(fragility_values) == 0: #if exposure does not lead to damage\n",
    "                            collect_inb.append(0)  \n",
    "                        else:\n",
    "                            asset_geom = geom_dict[asset[0]]\n",
    "                            collect_inb.append(get_damage_per_asset(asset,hazard_numpified,asset_geom,hazard_intensity,fragility_values,maxdam)) #get list of damages for specific asset\n",
    "                    collect_output[country_code, hazard_name, sub_system, infra_type, infra_curve[0], maxdam] = np.sum(collect_inb) # dictionary to store results for various combinations of hazard maps, infrastructure curves, and maximum damage values.\n",
    "        break #delete after testing\n",
    "    return collect_output\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4a399442-b267-48e2-9f40-437d7a228259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of critical infrastructure systems to process\n",
    "cis_dict = {\n",
    "    \"energy\": {\"power\": [\"line\",\"minor_line\",\"cable\",\"plant\",\"substation\",\n",
    "                        \"power_tower\",\"power_pole\"]},\n",
    "    \"transportation\": {\"road\":  [\"primary\", \"secondary\", \"tertiary\"], \n",
    "                        \"airports\": [\"airports\"],\n",
    "                        \"railways\": [\"railway\"]},\n",
    "    \"water\": {\"water_supply\": [\"water_tower\", \"water_well\", \"reservoir_covered\",\n",
    "                                \"water_works\", \"reservoir\"]},\n",
    "    \"waste\": {\"waste_solid\": [\"landfill\",\"waste_transfer_station\"],\n",
    "            \"waste_water\": [\"wastewater_treatment_plant\"]},\n",
    "    \"telecommunication\": {\"telecom\": [\"communication_tower\", \"mast\"]},\n",
    "    \"healthcare\": {\"health\": [\"clinic\", \"doctors\", \"hospital\", \"dentist\", \"pharmacy\", \n",
    "                        \"physiotherapist\", \"alternative\", \"laboratory\", \"optometrist\", \"rehabilitation\", \n",
    "                        \"blood_donation\", \"birthing_center\"]},\n",
    "    \"education\": {\"education_facilities\": [\"college\", \"kindergarten\", \"library\", \"school\", \"university\"]}\n",
    "}\n",
    "\n",
    "cis_dict = {\n",
    "    \"transportation\": {\"road\":  [\"primary\", \"secondary\", \"tertiary\"]}\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "da673ad9-b62f-4b19-898e-e6428afb89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_type='pluvial'\n",
    "country_codes=['JAM']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15de89-7df7-4861-8315-03b04dd3bc81",
   "metadata": {},
   "source": [
    "# Run analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "160755bb-8afe-40ca-8d4a-1099230fe88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extract points: 0it [00:00, ?it/s]\n",
      "extract multipolygons: 100%|█████████████████████████████████████████████████████████████| 2/2 [00:16<00:00,  8.46s/it]\n",
      "extract lines: 100%|███████████████████████████████████████████████████████████| 39974/39974 [00:11<00:00, 3599.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAM runs for road for pluvial for 10 maps\n",
      "JAM runs for primary for pluvial for P_1in50 map for 255 combinations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 853/853 [00:00<00:00, 1453.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAM runs for secondary for pluvial for P_1in50 map for 255 combinations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 377/377 [00:00<00:00, 908.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAM runs for tertiary for pluvial for P_1in50 map for 255 combinations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 13337/13337 [00:07<00:00, 1741.09it/s]\n"
     ]
    }
   ],
   "source": [
    "pathway_dict = create_pathway_dict(data_path, flood_data_path, eq_data_path, landslide_data_path, cyclone_data_path)\n",
    "for country_code in country_codes: \n",
    "    for ci_system in cis_dict: \n",
    "        for sub_system in cis_dict[ci_system]:\n",
    "            infra_type_lst = cis_dict[ci_system][sub_system]\n",
    "            test = country_infrastructure_hazard(pathway_dict, country_code, sub_system, infra_type_lst, hazard_type)\n",
    "            #create_damage_csv(test, hazard_type, pathway_dict, country_code, sub_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a92be13b-3187-4b79-a446-8a13927fc36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " outcome of total infra road types 581309728.9756943\n"
     ]
    }
   ],
   "source": [
    "print(f' outcome of total infra road types {sum(list(test.values()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5e4d5189-cd58-4440-a130-8c731a185ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('JAM',\n",
       "  'P_1in50',\n",
       "  'road',\n",
       "  'primary',\n",
       "  'F7.1',\n",
       "  909.3454827565133): 44278885.94165449,\n",
       " ('JAM',\n",
       "  'P_1in50',\n",
       "  'road',\n",
       "  'secondary',\n",
       "  'F7.1',\n",
       "  909.3454827565133): 47364030.14625165,\n",
       " ('JAM',\n",
       "  'P_1in50',\n",
       "  'road',\n",
       "  'tertiary',\n",
       "  'F7.1',\n",
       "  909.3454827565133): 489666812.8877882}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa4feb-78d5-46d1-9cfc-52850359187b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
